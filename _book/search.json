[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Data Science (2e)",
    "section": "",
    "text": "This is the website for the work-in-progress 2nd edition of “R for Data Science”. This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it.  In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. If you’d like a physical copy of the book, you can order it from amazon; it was published by O’Reilly in January 2017. If you’d like to give back please make a donation to Kākāpō Recovery: the kākāpō (which appears on the cover of R4DS) is a critically endangered native NZ parrot; there are only 199 left.\nPlease note that R4DS uses a Contributor Code of Conduct. By contributing to this book, you agree to abide by its terms.\n\nR4DS is a collaborative effort and many people have contributed fixes and improvements via pull request: adi pradhan (@adidoit), Andrea Gilardi (@agila5), Ajay Deonarine (@ajay-d), @AlanFeder, pete (@alonzi), Alex (@ALShum), Andrew Landgraf (@andland), @andrewmacfarland, Michael Henry (@aviast), Mara Averick (@batpigandme), Brent Brewington (@bbrewington), Bill Behrman (@behrman), Ben Herbertson (@benherbertson), Ben Marwick (@benmarwick), Ben Steinberg (@bensteinberg), Brandon Greenwell (@bgreenwell), Brett Klamer (@bklamer), Christian Mongeau (@chrMongeau), Cooper Morris (@coopermor), Colin Gillespie (@csgillespie), Rademeyer Vermaak (@csrvermaak), Abhinav Singh (@curious-abhinav), Curtis Alexander (@curtisalexander), Christian G. Warden (@cwarden), Kenny Darrell (@darrkj), David Rubinger (@davidrubinger), David Clark (@DDClark), Derwin McGeary (@derwinmcgeary), Daniel Gromer (@dgromer), @djbirke, Devin Pastoor (@dpastoor), Julian During (@duju211), Dylan Cashman (@dylancashman), Dirk Eddelbuettel (@eddelbuettel), Edwin Thoen (@EdwinTh), Ahmed El-Gabbas (@elgabbas), Eric Watt (@ericwatt), Erik Erhardt (@erikerhardt), Etienne B. Racine (@etiennebr), Everett Robinson (@evjrob), Flemming Villalona (@flemingspace), Floris Vanderhaeghe (@florisvdh), Garrick Aden-Buie (@gadenbuie), Garrett Grolemund (@garrettgman), Josh Goldberg (@GoldbergData), bahadir cankardes (@gridgrad), Gustav W Delius (@gustavdelius), Hadley Wickham (@hadley), Hao Chen (@hao-trivago), Harris McGehee (@harrismcgehee), Hengni Cai (@hengnicai), Ian Sealy (@iansealy), Ian Lyttle (@ijlyttle), Ivan Krukov (@ivan-krukov), Jacob Kaplan (@jacobkap), Jazz Weisman (@jazzlw), John D. Storey (@jdstorey), Jeff Boichuk (@jeffboichuk), Gregory Jefferis (@jefferis), 蒋雨蒙 (@JeldorPKU), Jennifer (Jenny) Bryan (@jennybc), Jen Ren (@jenren), Jeroen Janssens (@jeroenjanssens), Jim Hester (@jimhester), JJ Chen (@jjchern), Joanne Jang (@joannejang), John Sears (@johnsears), @jonathanflint, Jon Calder (@jonmcalder), Jonathan Page (@jonpage), Justinas Petuchovas (@jpetuchovas), Jose Roberto Ayala Solares (@jroberayalas), Julia Stewart Lowndes (@jules32), Sonja (@kaetschap), Kara Woo (@karawoo), Katrin Leinweber (@katrinleinweber), Karandeep Singh (@kdpsingh), Kyle Humphrey (@khumph), Kirill Sevastyanenko (@kirillseva), @koalabearski, Kirill Müller (@krlmlr), Noah Landesberg (@landesbergn), @lindbrook, Mauro Lepore (@maurolepore), Mark Beveridge (@mbeveridge), Matt Herman (@mfherman), Mine Cetinkaya-Rundel (@mine-cetinkaya-rundel), Matthew Hendrickson (@mjhendrickson), @MJMarshall, Mustafa Ascha (@mustafaascha), Nelson Areal (@nareal), Nate Olson (@nate-d-olson), Nathanael (@nateaff), Nick Clark (@nickclark1000), @nickelas, Nirmal Patel (@nirmalpatel), Nina Munkholt Jakobsen (@nmjakobsen), Jakub Nowosad (@Nowosad), Peter Hurford (@peterhurford), Patrick Kennedy (@pkq), Radu Grosu (@radugrosu), Ranae Dietzel (@Ranae), Robin Gertenbach (@rgertenbach), Richard Zijdeman (@rlzijdeman), Robin (@Robinlovelace), Emily Robinson (@robinsones), Rohan Alexander (@RohanAlexander), Romero Morais (@RomeroBarata), Albert Y. Kim (@rudeboybert), Saghir (@saghirb), Jonas (@sauercrowd), Robert Schuessler (@schuess), Seamus McKinsey (@seamus-mckinsey), @seanpwilliams, Luke Smith (@seasmith), Matthew Sedaghatfar (@sedaghatfar), Sebastian Kraus (@sekR4), Sam Firke (@sfirke), Shannon Ellis (@ShanEllis), @shoili, S’busiso Mkhondwane (@sibusiso16), @spirgel, Steven M. Mortimer (@StevenMMortimer), Stéphane Guillou (@stragu), Sergiusz Bleja (@svenski), Tal Galili (@talgalili), Tim Waterhouse (@timwaterhouse), TJ Mahr (@tjmahr), Thomas Klebel (@tklebel), Tom Prior (@tomjamesprior), Terence Teo (@tteo), Will Beasley (@wibeasley), @yahwes, Yihui Xie (@yihui), Yiming (Paul) Li (@yimingli), Hiroaki Yutani (@yutannihilation), @zeal626, Azza Ahmed (@zo0z)\nR4DS is hosted by https://www.netlify.com as part of their support of open source software and communities."
  },
  {
    "objectID": "preface-2e.html",
    "href": "preface-2e.html",
    "title": "Preface to the second edition",
    "section": "",
    "text": "Welcome to the second edition of “R for Data Science”."
  },
  {
    "objectID": "preface-2e.html#major-changes",
    "href": "preface-2e.html#major-changes",
    "title": "Preface to the second edition",
    "section": "Major changes",
    "text": "Major changes\n\nThe first part is renamed to “whole game” to reflect the entire data science cycle. It gains a new chapter that briefly introduces the basics of reading data from csv files.\nThe wrangle part is now transform and gains new chapters on numbers, logical vectors, and missing values. These were previously parts of the data transformation chapter, but needed much more room.\nWe’ve added new chapters on column-wise and row-wise operations.\nWe’ve added a new set of chapters on import that goes beyond importing rectangular data to include chapters on working with spreadsheets, databases, and scraping data from the web.\nThe modeling part has been removed. For modeling, we recommend using packages from tidymodels and reading Tidy Modeling with R by Max Kuhn and Julia Silge to learn more about them.\nWe’ve switched from the magrittr pipe to the base pipe."
  },
  {
    "objectID": "preface-2e.html#acknowledgements",
    "href": "preface-2e.html#acknowledgements",
    "title": "Preface to the second edition",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nTO DO: Add acknowledgements."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Data science is an exciting discipline that allows you to transform raw data into understanding, insight, and knowledge. The goal of “R for Data Science” is to help you learn the most important tools in R that will allow you to do data science efficiently and reproducibly. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of R."
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "1  Introduction",
    "section": "\n1.1 What you will learn",
    "text": "1.1 What you will learn\nData science is a huge field, and there’s no way you can master it all by reading a single book. The goal of this book is to give you a solid foundation in the most important tools, and enough knowledge to find the resources to learn more when necessary. Our model of the tools needed in a typical data science project looks something like this:\n\n\n\n\n\n\n\n\nFirst you must import your data into R. This typically means that you take data stored in a file, database, or web application programming interface (API), and load it into a data frame in R. If you can’t get your data into R, you can’t do data science on it!\nOnce you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your efforts on answering questions about the data, not fighting to get the data into the right form for different functions.\nOnce you have tidy data, a common next step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling, because getting your data in a form that’s natural to work with often feels like a fight!\nOnce you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times.\nVisualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or that you need to collect different data. Visualisations can surprise you and they don’t scale particularly well because they require a human to interpret them.\nThe last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others.\nSurrounding all these tools is programming. Programming is a cross-cutting tool that you use in nearly every part of a data science project. You don’t need to be an expert programmer to be a successful data scientist, but learning more about programming pays off, because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease.\nYou’ll use these tools in every data science project, but for most projects they’re not enough. There’s a rough 80-20 rule at play; you can tackle about 80% of every project using the tools that you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book, we’ll point you to resources where you can learn more."
  },
  {
    "objectID": "intro.html#how-this-book-is-organised",
    "href": "intro.html#how-this-book-is-organised",
    "title": "1  Introduction",
    "section": "\n1.2 How this book is organised",
    "text": "1.2 How this book is organised\nThe previous description of the tools of data science is organised roughly according to the order in which you use them in an analysis (although of course you’ll iterate through them multiple times). In our experience, however, this is not the best way to learn them because tarting with data ingest and tidying is sub-optimal because 80% of the time it’s routine and boring, and the other 20% of the time it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualisation and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth the effort.\nWithin each chapter, we try and adhere to a similar pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. Although it can be tempting to skip the exercises, there’s no better way to learn than practicing on real problems."
  },
  {
    "objectID": "intro.html#what-you-wont-learn",
    "href": "intro.html#what-you-wont-learn",
    "title": "1  Introduction",
    "section": "\n1.3 What you won’t learn",
    "text": "1.3 What you won’t learn\nThere are a number of important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic.\n\n1.3.1 Modelling\n\n\n1.3.2 Big data\nThis book proudly focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care, you can typically use them to work with 1-2 Gb of data. If you’re routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book doesn’t teach data.table because it has a very concise interface that offers fewer linguistic cues, which makes it harder to learn. However, if you’re working with large data, the performance payoff is well worth the effort required to learn it.\nIf your data is bigger than this, carefully consider whether your big data problem is actually a small data problem in disguise. While the complete data set might be big, often the data needed to answer a specific question is small. You might be able to find a subset, subsample, or summary that fits in memory and still allows you to answer the question that you’re interested in. The challenge here is finding the right small data, which often requires a lot of iteration.\nAnother possibility is that your big data problem is actually a large number of small data problems in disguise. Each individual problem might fit in memory, but you have millions of them. For example, you might want to fit a model to each person in your dataset. This would be trivial if you had just 10 or 100 people, but instead you have a million. Fortunately, each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing. Once you’ve figured out how to answer your question for a single subset using the tools described in this book, you can learn new tools like sparklyr to solve it for the full dataset.\n\n1.3.3 Python, Julia, and friends\nIn this book, you won’t learn anything about Python, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python.\nHowever, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing.\nWe think R is a great place to start your data science journey because it is an environment designed from the ground up to support data science. R is not just a programming language, it is also an interactive environment for doing data science. To support interaction, R is a much more flexible language than many of its peers. This flexibility comes with its downsides, but the big upside is how easy it is to evolve tailored grammars for specific parts of the data science process. These mini languages help you think about problems as a data scientist, while supporting fluent interaction between your brain and the computer."
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "\n1.4 Prerequisites",
    "text": "1.4 Prerequisites\nWe’ve made a few assumptions about what you already know in order to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some programming experience already. If you’ve never programmed before, you might find Hands on Programming with R by Garrett to be a useful adjunct to this book.\nThere are four things you need to run the code in this book: R, RStudio, a collection of R packages called the tidyverse, and a handful of other packages. Packages are the fundamental units of reproducible R code. They include reusable functions, the documentation that describes how to use them, and sample data.\n\n1.4.1 R\nTo download R, go to CRAN, the comprehensive R archive network. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you.\nA new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to re-install all your packages, but putting it off only makes it worse. You’ll need at least R 4.1.0 for this book.\n\n1.4.2 RStudio\nRStudio is an integrated development environment, or IDE, for R programming. Download and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features. For this book, make sure you have at least RStudio 2022.02.0.\nWhen you start RStudio, you’ll see two key regions in the interface: the console pane, and the output pane.\n\n\n\n\n\n\n\n\nFor now, all you need to know is that you type R code in the console pane, and press enter to run it. You’ll learn more as we go along!\n\n1.4.3 The tidyverse\nYou’ll also need to install some R packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this book are part of the so-called tidyverse. All packages in the tidyverse share a common philosophy of data and R programming, and are designed to work together naturally.\nYou can install the complete tidyverse with a single line of code:\n\ninstall.packages(\"tidyverse\")\n\nOn your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install them on to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isn’t blocked by your firewall or proxy.\nYou will not be able to use the functions, objects, or help files in a package until you load it with library(). Once you have installed a package, you can load it using the library() function:\n\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n\nThis tells you that tidyverse is loading eight packages: ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr, and forcats packages. These are considered to be the core of the tidyverse because you’ll use them in almost every analysis.\nPackages in the tidyverse change fairly frequently. You can check whether updates are available, and optionally install them, by running tidyverse_update().\n\n1.4.4 Other packages\nThere are many other excellent packages that are not part of the tidyverse, because they solve problems in a different domain, or are designed with a different set of underlying principles. This doesn’t make them better or worse, just different. In other words, the complement to the tidyverse is not the messyverse, but many other universes of interrelated packages. As you tackle more data science projects with R, you’ll learn new packages and new ways of thinking about data.\nIn this book we’ll use three data packages from outside the tidyverse:\n\ninstall.packages(c(\"nycflights13\", \"gapminder\", \"Lahman\"))\n\nThese packages provide data on airline flights, world development, and baseball that we’ll use to illustrate key data science ideas."
  },
  {
    "objectID": "intro.html#running-r-code",
    "href": "intro.html#running-r-code",
    "title": "1  Introduction",
    "section": "\n1.5 Running R code",
    "text": "1.5 Running R code\nThe previous section showed you several examples of running R code. Code in the book looks like this:\n\n1 + 2\n#> [1] 3\n\nIf you run the same code in your local console, it will look like this:\n> 1 + 2\n[1] 3\nThere are two main differences. In your console, you type after the >, called the prompt; we don’t show the prompt in the book. In the book, output is commented out with #>; in your console it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and into the console.\nThroughout the book, we use a consistent set of conventions to refer to code:\n\nFunctions are displayed in a code font and followed by parentheses, like sum(), or mean().\nOther R objects (such as data or function arguments) are in a code font, without parentheses, like flights or x.\nSometimes, to make it clear which package an object comes from, we’ll use we’ll use the package name followed by two colons, like dplyr::mutate(), ornycflights13::flights. This is also valid R code."
  },
  {
    "objectID": "intro.html#acknowledgements",
    "href": "intro.html#acknowledgements",
    "title": "1  Introduction",
    "section": "\n1.6 Acknowledgements",
    "text": "1.6 Acknowledgements\nThis book isn’t just the product of Hadley, Mine, and Garrett, but is the result of many conversations (in person and online) that we’ve had with many people in the R community. There are a few people we’d like to thank in particular, because they have spent many hours answering our questions and helping us to better think about data science:\n\nJenny Bryan and Lionel Henry for many helpful discussions around working with lists and list-columns.\nThe three chapters on workflow were adapted (with permission), from http://stat545.com/block002_hello-r-workspace-wd-project.html by Jenny Bryan.\nYihui Xie for his work on the bookdown package, and for tirelessly responding to my feature requests.\nBill Behrman for his thoughtful reading of the entire book, and for trying it out with his data science class at Stanford.\nThe #rstats Twitter community who reviewed all of the draft chapters and provided tons of useful feedback.\n\nThis book was written in the open, and many people contributed pull requests to fix minor problems. Special thanks goes to everyone who contributed via GitHub:\nThanks go to all contributers in alphabetical order: @a-rosenberg, A. s, Abhinav Singh, adi pradhan, Ahmed ElGabbas, Ajay Deonarine, @AlanFeder, Albert Y. Kim, @Alex, Andrea Gilardi, Andrew Landgraf, @andrewmacfarland, Angela Li, Azza Ahmed, bahadir cankardes, @batpigandme, @behrman, Ben Herbertson, Ben Marwick, Ben Steinberg, Benjamin Yeh, Bianca Peterson, Bill Behrman, @BirgerNi, @boardtc, Brandon Greenwell, Brent Brewington, Brett Klamer, Brian G. Barkley, Charlotte Wickham, Christian G. Warden, Christian Heinrich, Christian Mongeau, Colin Gillespie, Cooper Morris, Curtis Alexander, Daniel Gromer, David Clark, David Rubinger, Derwin McGeary, Devin Pastoor, Dirk Eddelbuettel, @djbirke, @DSGeoff, Dylan Cashman, Earl Brown, Edwin Thoen, Eric Watt, Erik Erhardt, Etienne B. Racine, Everett Robinson, Flemming Villalona, Floris Vanderhaeghe, Garrick Aden-Buie, George Wang, Gregory Jefferis, Gustav W Delius, Hao Chen, @harrismcgehee, Hengni Cai, Hiroaki Yutani, Hojjat Salmasian, Ian Lyttle, Ian Sealy, Ivan Krukov, Jacek Kolacz, Jacob Kaplan, Jakub Nowosad, Jazz Weisman, Jeff Boichuk, Jeffrey Arnold, Jen Ren, Jennifer (Jenny) Bryan, @jennybc, Jeroen Janssens, Jim Hester, @jjchern, Joanne Jang, Johannes Gruber, John Blischak, John D. Storey, John Sears, Jon Calder, @Jonas, Jonathan Page, @jonathanflint, Jose Roberto Ayala Solares, Josh Goldberg, @juandering, Julia Stewart Lowndes, Julian During, Justinas Petuchovas, @kaetschap, Kara de la Marck, Kara Woo, Katrin Leinweber, @kdpsingh, Kenny Darrell, Kirill Müller, Kirill Sevastyanenko, @koalabearski, Kunal Marwaha, @KyleHumphrey, Lawrence Wu, @lindbrook, Luke Smith, Luke W Johnston, Mara Averick, Maria Paula Caldas, Mark Beveridge, Matt Herman, Matthew Hendrickson, Matthew Sedaghatfar, @MattWittbrodt, Mauro Lepore, Michael Henry, @MJMarshall, Mustafa Ascha, @nate-d-olson, @nattalides, Nelson Areal, Nicholas Tierney, Nick Clark, @nickelas, Nina Munkholt Jakobsen, Nirmal Patel, Nischal Shrestha, Noah Landesberg, @nwaff, @OaCantona, Pablo E, Patrick Kennedy, @Paul, @pete, Peter Hurford, Rademeyer Vermaak, Radu Grosu, Ranae Dietzel, Riva Quiroga, @rlzijdeman, Rob Tenorio, Robert Schuessler, @robertchu03, Robin Gertenbach, @robinlovelace, @robinsones, Rohan Alexander, @RomeroBarata, S’busiso Mkhondwane, @Saghir, Sam Firke, @seamus-mckinsey, Seamus McKinsey, @seanpwilliams, Sebastian Kraus, Shannon Ellis, @shoili, @sibusiso16, @Sophiazj, @spirgel, Stéphane Guillou, Steve Mortimer, @svenski, Tal Galili, Terence Teo, Thomas Klebel, Tim Waterhouse, TJ Mahr, Tom Prior, @twgardner2, Ulrik Lyngs, Will Beasley, @yahwes, Yihui Xie, Yiming (Paul) Li, Yu Yu Aung, Zach Bogart, @zeal626, Zhuoer Dong, @蒋雨蒙."
  },
  {
    "objectID": "intro.html#colophon",
    "href": "intro.html#colophon",
    "title": "1  Introduction",
    "section": "\n1.7 Colophon",
    "text": "1.7 Colophon\nAn online version of this book is available at http://r4ds.had.co.nz. It will continue to evolve in between reprints of the physical book. The source of the book is available at https://github.com/hadley/r4ds. The book is powered by https://bookdown.org which makes it easy to turn R Markdown files into HTML, PDF, and EPUB.\nThis book was built with:\n\nsessioninfo::session_info(c(\"tidyverse\"))\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.1 (2022-06-23 ucrt)\n#>  os       Windows 10 x64 (build 19043)\n#>  system   x86_64, mingw32\n#>  ui       RTerm\n#>  language (EN)\n#>  collate  English_United Kingdom.utf8\n#>  ctype    English_United Kingdom.utf8\n#>  tz       Europe/Berlin\n#>  date     2022-08-14\n#>  pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package       * version  date (UTC) lib source\n#>  askpass         1.1      2019-01-13 [1] CRAN (R 4.2.1)\n#>  assertthat      0.2.1    2019-03-21 [1] CRAN (R 4.2.1)\n#>  backports       1.4.1    2021-12-13 [1] CRAN (R 4.2.0)\n#>  base64enc       0.1-3    2015-07-28 [1] CRAN (R 4.2.0)\n#>  bit             4.0.4    2020-08-04 [1] CRAN (R 4.2.1)\n#>  bit64           4.0.5    2020-08-30 [1] CRAN (R 4.2.1)\n#>  blob            1.2.3    2022-04-10 [1] CRAN (R 4.2.1)\n#>  broom           1.0.0    2022-07-01 [1] CRAN (R 4.2.1)\n#>  bslib           0.4.0    2022-07-16 [1] CRAN (R 4.2.1)\n#>  cachem          1.0.6    2021-08-19 [1] CRAN (R 4.2.1)\n#>  callr           3.7.1    2022-07-13 [1] CRAN (R 4.2.1)\n#>  cellranger      1.1.0    2016-07-27 [1] CRAN (R 4.2.1)\n#>  cli             3.3.0    2022-04-25 [1] CRAN (R 4.2.1)\n#>  clipr           0.8.0    2022-02-22 [1] CRAN (R 4.2.1)\n#>  colorspace      2.0-3    2022-02-21 [1] CRAN (R 4.2.1)\n#>  cpp11           0.4.2    2021-11-30 [1] CRAN (R 4.2.1)\n#>  crayon          1.5.1    2022-03-26 [1] CRAN (R 4.2.1)\n#>  curl            4.3.2    2021-06-23 [1] CRAN (R 4.2.1)\n#>  data.table      1.14.2   2021-09-27 [1] CRAN (R 4.2.1)\n#>  DBI             1.1.3    2022-06-18 [1] CRAN (R 4.2.1)\n#>  dbplyr          2.2.1    2022-06-27 [1] CRAN (R 4.2.1)\n#>  digest          0.6.29   2021-12-01 [1] CRAN (R 4.2.1)\n#>  dplyr         * 1.0.9    2022-04-28 [1] CRAN (R 4.2.1)\n#>  dtplyr          1.2.1    2022-01-19 [1] CRAN (R 4.2.1)\n#>  ellipsis        0.3.2    2021-04-29 [1] CRAN (R 4.2.1)\n#>  evaluate        0.16     2022-08-09 [1] CRAN (R 4.2.1)\n#>  fansi           1.0.3    2022-03-24 [1] CRAN (R 4.2.1)\n#>  farver          2.1.1    2022-07-06 [1] CRAN (R 4.2.1)\n#>  fastmap         1.1.0    2021-01-25 [1] CRAN (R 4.2.1)\n#>  forcats       * 0.5.1    2021-01-27 [1] CRAN (R 4.2.1)\n#>  fs              1.5.2    2021-12-08 [1] CRAN (R 4.2.1)\n#>  gargle          1.2.0    2021-07-02 [1] CRAN (R 4.2.1)\n#>  generics        0.1.3    2022-07-05 [1] CRAN (R 4.2.1)\n#>  ggplot2       * 3.3.6    2022-05-03 [1] CRAN (R 4.2.1)\n#>  glue            1.6.2    2022-02-24 [1] CRAN (R 4.2.1)\n#>  googledrive     2.0.0    2021-07-08 [1] CRAN (R 4.2.1)\n#>  googlesheets4   1.0.1    2022-08-13 [1] CRAN (R 4.2.1)\n#>  gtable          0.3.0    2019-03-25 [1] CRAN (R 4.2.1)\n#>  haven           2.5.0    2022-04-15 [1] CRAN (R 4.2.1)\n#>  highr           0.9      2021-04-16 [1] CRAN (R 4.2.1)\n#>  hms             1.1.1    2021-09-26 [1] CRAN (R 4.2.1)\n#>  htmltools       0.5.3    2022-07-18 [1] CRAN (R 4.2.1)\n#>  httr            1.4.3    2022-05-04 [1] CRAN (R 4.2.1)\n#>  ids             1.0.1    2017-05-31 [1] CRAN (R 4.2.1)\n#>  isoband         0.2.5    2021-07-13 [1] CRAN (R 4.2.1)\n#>  jquerylib       0.1.4    2021-04-26 [1] CRAN (R 4.2.1)\n#>  jsonlite        1.8.0    2022-02-22 [1] CRAN (R 4.2.1)\n#>  knitr           1.39     2022-04-26 [1] CRAN (R 4.2.1)\n#>  labeling        0.4.2    2020-10-20 [1] CRAN (R 4.2.0)\n#>  lattice         0.20-45  2021-09-22 [2] CRAN (R 4.2.1)\n#>  lifecycle       1.0.1    2021-09-24 [1] CRAN (R 4.2.1)\n#>  lubridate       1.8.0    2021-10-07 [1] CRAN (R 4.2.1)\n#>  magrittr        2.0.3    2022-03-30 [1] CRAN (R 4.2.1)\n#>  MASS            7.3-58.1 2022-08-03 [1] CRAN (R 4.2.1)\n#>  Matrix          1.4-1    2022-03-23 [1] CRAN (R 4.2.1)\n#>  memoise         2.0.1    2021-11-26 [1] CRAN (R 4.2.1)\n#>  mgcv            1.8-40   2022-03-29 [1] CRAN (R 4.2.1)\n#>  mime            0.12     2021-09-28 [1] CRAN (R 4.2.0)\n#>  modelr          0.1.8    2020-05-19 [1] CRAN (R 4.2.1)\n#>  munsell         0.5.0    2018-06-12 [1] CRAN (R 4.2.1)\n#>  nlme            3.1-159  2022-08-09 [1] CRAN (R 4.2.1)\n#>  openssl         2.0.2    2022-05-24 [1] CRAN (R 4.2.1)\n#>  pillar          1.8.0    2022-07-18 [1] CRAN (R 4.2.1)\n#>  pkgconfig       2.0.3    2019-09-22 [1] CRAN (R 4.2.1)\n#>  prettyunits     1.1.1    2020-01-24 [1] CRAN (R 4.2.1)\n#>  processx        3.7.0    2022-07-07 [1] CRAN (R 4.2.1)\n#>  progress        1.2.2    2019-05-16 [1] CRAN (R 4.2.1)\n#>  ps              1.7.1    2022-06-18 [1] CRAN (R 4.2.1)\n#>  purrr         * 0.3.4    2020-04-17 [1] CRAN (R 4.2.1)\n#>  R6              2.5.1    2021-08-19 [1] CRAN (R 4.2.1)\n#>  rappdirs        0.3.3    2021-01-31 [1] CRAN (R 4.2.1)\n#>  RColorBrewer    1.1-3    2022-04-03 [1] CRAN (R 4.2.0)\n#>  readr         * 2.1.2    2022-01-30 [1] CRAN (R 4.2.1)\n#>  readxl          1.4.0    2022-03-28 [1] CRAN (R 4.2.1)\n#>  rematch         1.0.1    2016-04-21 [1] CRAN (R 4.2.1)\n#>  rematch2        2.1.2    2020-05-01 [1] CRAN (R 4.2.1)\n#>  reprex          2.0.1    2021-08-05 [1] CRAN (R 4.2.1)\n#>  rlang           1.0.4    2022-07-12 [1] CRAN (R 4.2.1)\n#>  rmarkdown       2.14     2022-04-25 [1] CRAN (R 4.2.1)\n#>  rstudioapi      0.13     2020-11-12 [1] CRAN (R 4.2.1)\n#>  rvest           1.0.2    2021-10-16 [1] CRAN (R 4.2.1)\n#>  sass            0.4.2    2022-07-16 [1] CRAN (R 4.2.1)\n#>  scales          1.2.0    2022-04-13 [1] CRAN (R 4.2.1)\n#>  selectr         0.4-2    2019-11-20 [1] CRAN (R 4.2.1)\n#>  stringi         1.7.8    2022-07-11 [1] CRAN (R 4.2.1)\n#>  stringr       * 1.4.0    2019-02-10 [1] CRAN (R 4.2.1)\n#>  sys             3.4      2020-07-23 [1] CRAN (R 4.2.1)\n#>  tibble        * 3.1.8    2022-07-22 [1] CRAN (R 4.2.1)\n#>  tidyr         * 1.2.0    2022-02-01 [1] CRAN (R 4.2.1)\n#>  tidyselect      1.1.2    2022-02-21 [1] CRAN (R 4.2.1)\n#>  tidyverse     * 1.3.2    2022-07-18 [1] CRAN (R 4.2.1)\n#>  tinytex         0.40     2022-06-15 [1] CRAN (R 4.2.1)\n#>  tzdb            0.3.0    2022-03-28 [1] CRAN (R 4.2.1)\n#>  utf8            1.2.2    2021-07-24 [1] CRAN (R 4.2.1)\n#>  uuid            1.1-0    2022-04-19 [1] CRAN (R 4.2.0)\n#>  vctrs           0.4.1    2022-04-13 [1] CRAN (R 4.2.1)\n#>  viridisLite     0.4.0    2021-04-13 [1] CRAN (R 4.2.1)\n#>  vroom           1.5.7    2021-11-30 [1] CRAN (R 4.2.1)\n#>  withr           2.5.0    2022-03-03 [1] CRAN (R 4.2.1)\n#>  xfun            0.31     2022-05-10 [1] CRAN (R 4.2.1)\n#>  xml2            1.3.3    2021-11-30 [1] CRAN (R 4.2.1)\n#>  yaml            2.3.5    2022-02-21 [1] CRAN (R 4.2.1)\n#> \n#>  [1] C:/Users/user1/AppData/Local/R/win-library/4.2\n#>  [2] C:/Program Files/R/R-4.2.1/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "whole-game.html",
    "href": "whole-game.html",
    "title": "Whole game",
    "section": "",
    "text": "In this part of the book, you will learn several useful tools that have an immediate payoff:\n\nVisualisation is a great place to start with R programming, because the payoff is so clear: you get to make elegant and informative plots that help you understand data. In Chapter 2 you’ll dive into visualization, learning the basic structure of a ggplot2 plot, and powerful techniques for turning data into plots.\nVisualisation alone is typically not enough, so in Chapter 4, you’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries.\nIn Chapter 6, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualization, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form.\nBefore you can transform and visualize your data, you need to first get your data into R. In Chapter 8 you’ll learn the basics of getting plain-text, rectangular data into R.\nFinally, in Chapter 10, you’ll combine visualization and transformation with your curiosity and skepticism to ask and answer interesting questions about data.\n\nModelling is an important part of the exploratory process, but you don’t have the skills to effectively learn or apply it yet and details of modeling fall outside the scope of this book.\nNestled among these five chapters that teach you the tools for doing data science are three chapters that focus on your R workflow. In Chapter 3, Chapter 5, Chapter 7, and Chapter 9, you’ll learn good workflow practices for writing and organizing your R code. These will set you up for success in the long run, as they’ll give you the tools to stay organised when you tackle real projects."
  },
  {
    "objectID": "data-visualize.html",
    "href": "data-visualize.html",
    "title": "2  Data visualization",
    "section": "",
    "text": "“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nThis chapter will teach you how to visualize your data using ggplot2. R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places.\nIf you’d like to learn more about the theoretical underpinnings of ggplot2, you might enjoy reading “The Layered Grammar of Graphics”, http://vita.had.co.nz/papers/layered-grammar.pdf, the scientific paper that discusses the theoretical underpinnings..\n\nThis chapter focuses on ggplot2, one of the core packages in the tidyverse. To access the datasets, help pages, and functions used in this chapter, load the tidyverse by running this code:\n\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n\nThat one line of code loads the core tidyverse; packages which you will use in almost every data analysis. It also tells you which functions from the tidyverse conflict with functions in base R (or from other packages you might have loaded).\nIf you run this code and get the error message “there is no package called ‘tidyverse’”, you’ll need to first install it, then run library() once again.\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nYou only need to install a package once, but you need to reload it every time you start a new session."
  },
  {
    "objectID": "data-visualize.html#first-steps",
    "href": "data-visualize.html#first-steps",
    "title": "2  Data visualization",
    "section": "\n2.2 First steps",
    "text": "2.2 First steps\nLet’s use our first graph to answer a question: Do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? Is it positive? Negative? Linear? Nonlinear?\n\n2.2.1 The mpg data frame\nYou can test your answer with the mpg data frame found in ggplot2 (a.k.a. ggplot2::mpg). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). mpg contains observations collected by the US Environmental Protection Agency on 38 car models.\n\nmpg\n#> # A tibble: 234 × 11\n#>   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n#>   <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n#> 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n#> 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n#> 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n#> 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n#> 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n#> 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n#> # … with 228 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAmong the variables in mpg are:\n\ndispl, a car’s engine size, in liters.\nhwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.\n\nTo learn more about mpg, open its help page by running ?mpg.\n\n2.2.2 Creating a ggplot\nTo plot mpg, run this code to put displ on the x-axis and hwy on the y-axis:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\nThe plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with smaller engine sizes have higher fuel efficiency and, in general, as engine size increases, fuel efficiency decreases. Does this confirm or refute your hypothesis about fuel efficiency and engine size?\nWith ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = mpg) creates an empty graph, but it’s not very interesting so we won’t show it here.\nYou complete your graph by adding one or more layers to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter.\nEach geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties of your plot. The mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes. ggplot2 looks for the mapped variables in the data argument, in this case, mpg.\n\n2.2.3 A graphing template\nLet’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings.\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nThe rest of this chapter will show you how to complete and extend this template to make different types of graphs. We will begin with the <MAPPINGS> component.\n\n2.2.4 Exercises\n\nRun ggplot(data = mpg). What do you see?\nHow many rows are in mpg? How many columns?\nWhat does the drv variable describe? Read the help for ?mpg to find out.\nMake a scatterplot of hwy vs cyl.\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?"
  },
  {
    "objectID": "data-visualize.html#aesthetic-mappings",
    "href": "data-visualize.html#aesthetic-mappings",
    "title": "2  Data visualization",
    "section": "\n2.3 Aesthetic mappings",
    "text": "2.3 Aesthetic mappings\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n\nIn the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?\n\n\n\n\n\nLet’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).\nYou can add a third variable, like class, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe aesthetic properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue:\n\n\n\n\n\nYou can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class))\n\n\n\n\n(If you prefer British English, like Hadley, you can use colour instead of color.)\nTo map an aesthetic to a variable, associate the name of the aesthetic with the name of the variable inside aes(). ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values.\nThe colors reveal that many of the unusual points (with engine size greater than 5 liters and highway fuel efficiency greater than 20 miles per gallon) are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines.\nIn the above example, we mapped class to the color aesthetic, but we could have mapped class to the size aesthetic in the same way. In this case, the exact size of each point would reveal its class affiliation. We get a warning here, because mapping an unordered variable (class) to an ordered aesthetic (size) is generally not a good idea.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, size = class))\n#> Warning: Using size for a discrete variable is not advised.\n\n\n\n\nSimilarly, we could have mapped class to the alpha aesthetic, which controls the transparency of the points, or to the shape aesthetic, which controls the shape of the points.\n\n# Left\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))\n\n# Right\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class))\n\n\n\n\n\n\n\n\n\n\n\nWhat happened to the SUVs? ggplot2 will only use six shapes at a time. By default, additional groups will go unplotted when you use the shape aesthetic.\nFor each aesthetic, you use aes() to associate the name of the aesthetic with a variable to display. The aes() function gathers together each of the aesthetic mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves aesthetics, visual properties that you can map to variables to display information about the data.\nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values.\nYou can also set the aesthetic properties of your geom manually. For example, we can make all of the points in our plot blue:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\nHere, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an aesthetic manually, set the aesthetic by name as an argument of your geom function. In other words, it goes outside of aes(). You’ll need to pick a value that makes sense for that aesthetic:\n\nThe name of a color as a character string.\nThe size of a point in mm.\nThe shape of a point as a number, as shown in Figure 2.1.\n\n\n\n\n\nFigure 2.1: R has 25 built in shapes that are identified by numbers. There are some seeming duplicates: for example, 0, 15, and 22 are all squares. The difference comes from the interaction of the color and fill aesthetics. The hollow shapes (0–14) have a border determined by color; the solid shapes (15–20) are filled with color; the filled shapes (21–24) have a border of color and are filled with fill.\n\n\n\n\n\n2.3.1 Exercises\n\n\nWhat’s gone wrong with this code? Why are the points not blue?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\nWhich variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\nMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?\nWhat happens if you map the same variable to multiple aesthetics?\nWhat does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)\nWhat happens if you map an aesthetic to something other than a variable name, like aes(color = displ < 5)? Note, you’ll also need to specify x and y."
  },
  {
    "objectID": "data-visualize.html#common-problems",
    "href": "data-visualize.html#common-problems",
    "title": "2  Data visualization",
    "section": "\n2.4 Common problems",
    "text": "2.4 Common problems\nAs you start to run R code, you’re likely to run into problems. Don’t worry — it happens to everyone. We have all been writing R code for years, but every day we still write code that doesn’t work!\nStart by carefully comparing the code that you’re running to the code in the book. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.\nOne common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start. In other words, make sure you haven’t accidentally written code like this:\nggplot(data = mpg) \n+ geom_point(mapping = aes(x = displ, y = hwy))\nIf you’re still stuck, try the help. You can get help about any R function by running ?function_name in the console, or selecting the function name and pressing F1 in RStudio. Don’t worry if the help doesn’t seem that helpful - instead skip down to the examples and look for code that matches what you’re trying to do.\nIf that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to R, the answer might be in the error message but you don’t yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online."
  },
  {
    "objectID": "data-visualize.html#facets",
    "href": "data-visualize.html#facets",
    "title": "2  Data visualization",
    "section": "\n2.5 Facets",
    "text": "2.5 Facets\nOne way to add additional variables to a plot is by mapping them to an aesthetic. Another way, which is particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() is a formula, which you create with ~ followed by a variable name (here, “formula” is the bane if a data structure in R, not a synonym for “equation”). The variable that you pass to facet_wrap() should be discrete.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~cyl)\n\n\n\n\nTo facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ cyl)\n\n\n\n\nIf you prefer to not facet in the rows or columns dimension, use a . instead of a variable name, e.g. + facet_grid(. ~ cyl).\n\n2.5.1 Exercises\n\nWhat happens if you facet on a continuous variable?\n\nWhat do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))\n\n\n\n\n\n\nWhat plots does the following code make? What does . do?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\nTake the first faceted plot in this section:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\nWhat are the advantages to using faceting instead of the color aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\n\nRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\n\nWhich of the following two plots makes it easier to compare engine size (displ) across cars with different drive trains? What does this say about when to place a faceting variable across rows or columns?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ .)\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(. ~ drv)\n\n\n\n\n\n\n\n\n\nRecreate this plot using facet_wrap() instead of facet_grid(). How do the positions of the facet labels change?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)"
  },
  {
    "objectID": "data-visualize.html#geometric-objects",
    "href": "data-visualize.html#geometric-objects",
    "title": "2  Data visualization",
    "section": "\n2.6 Geometric objects",
    "text": "2.6 Geometric objects\nHow are these two plots similar?\n\n\n\n\n\n\n\n\n\n\nBoth plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different visual object to represent the data. In ggplot2 syntax, we say that they use different geoms.\nA geom is the geometrical object that a plot uses to represent data. People often describe plots by the type of geom that the plot uses. For example, bar charts use bar geoms, line charts use line geoms, boxplots use boxplot geoms, and so on. Scatterplots break the trend; they use the point geom. As we see above, you can use different geoms to plot the same data. The plot on the left uses the point geom, and the plot on the right uses the smooth geom, a smooth line fitted to the data.\nTo change the geom in your plot, change the geom function that you add to ggplot(). For instance, to make the plots above, you can use this code:\n\n# left\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n# right\nggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy))\n\nEvery geom function in ggplot2 takes a mapping argument. However, not every aesthetic works with every geom. You could set the shape of a point, but you couldn’t set the “shape” of a line. On the other hand, you could set the linetype of a line. geom_smooth() will draw a different line, with a different linetype, for each unique value of the variable that you map to linetype.\n\nggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))\n\n\n\n\nHere, geom_smooth() separates the cars into three lines based on their drv value, which describes a car’s drive train. One line describes all of the points that have a 4 value, one line describes all of the points that have an f value, and one line describes all of the points that have an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive.\nIf this sounds strange, we can make it more clear by overlaying the lines on top of the raw data and then coloring everything according to drv.\n\n\n\n\n\nNotice that this plot contains two geoms in the same graph! If this makes you excited, buckle up. You will learn how to place multiple geoms in the same plot very soon.\nggplot2 provides more than 40 geoms, and extension packages provide even more (see https://exts.ggplot2.tidyverse.org/gallery/ for a sampling). The best way to get a comprehensive overview is the ggplot2 cheatsheet, which you can find at http://rstudio.com/resources/cheatsheets. To learn more about any single geom, use the help (e.g. ?geom_smooth).\nMany geoms, like geom_smooth(), use a single geometric object to display multiple rows of data. For these geoms, you can set the group aesthetic to a categorical variable to draw multiple objects. ggplot2 will draw a separate object for each unique value of the grouping variable. In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.\n\nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n              \nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))\n    \nggplot(data = mpg) +\n  geom_smooth(\n    mapping = aes(x = displ, y = hwy, color = drv),\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo display multiple geoms in the same plot, add multiple geom functions to ggplot():\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n\n\n\n\nThis, however, introduces some duplication in our code. Imagine if you wanted to change the y-axis to display cty instead of hwy. You’d need to change the variable in two places, and you might forget to update one. You can avoid this type of repetition by passing a set of mappings to ggplot(). ggplot2 will treat these mappings as global mappings that apply to each geom in the graph. In other words, this code will produce the same plot as the previous code:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\nIf you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n\n\n\n\nYou can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth(data = filter(mpg, class == \"subcompact\"), se = FALSE)\n\n\n\n\n(You’ll learn how filter() works in the chapter on data transformations: for now, just know that this command selects only the subcompact cars.)\n\n2.6.1 Exercises\n\nWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\n\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n\n\n\nWhat does show.legend = FALSE do? What happens if you remove it?\nWhy do you think we used it earlier in the chapter?\n\nWhat does the se argument to geom_smooth() do?\n\nWill these two graphs look different? Why/why not?\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\nggplot() + \n  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))\n\n\n\nRecreate the R code necessary to generate the following graphs. Note that wherever a categorical variable is used in the plot, it’s drv."
  },
  {
    "objectID": "data-visualize.html#statistical-transformations",
    "href": "data-visualize.html#statistical-transformations",
    "title": "2  Data visualization",
    "section": "\n2.7 Statistical transformations",
    "text": "2.7 Statistical transformations\nNext, let’s take a look at a bar chart. Bar charts seem simple, but they are interesting because they reveal something subtle about plots. Consider a basic bar chart, as drawn with geom_bar(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut. The diamonds dataset is in the ggplot2 package and contains information on ~54,000 diamonds, including the price, carat, color, clarity, and cut of each diamond. The chart shows that more diamonds are available with high quality cuts than with low quality cuts.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut))\n\n\n\n\nOn the x-axis, the chart displays cut, a variable from diamonds. On the y-axis, it displays count, but count is not a variable in diamonds! Where does count come from? Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot:\n\nbar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin.\nsmoothers fit a model to your data and then plot predictions from the model.\nboxplots compute a robust summary of the distribution and then display that summary as a specially formatted box.\n\nThe algorithm used to calculate new values for a graph is called a stat, short for statistical transformation. The figure below describes how this process works with geom_bar().\n\n\n\n\n\nYou can learn which stat a geom uses by inspecting the default value for the stat argument. For example, ?geom_bar shows that the default value for stat is “count”, which means that geom_bar() uses stat_count(). stat_count() is documented on the same page as geom_bar(). If you scroll down, the section called “Computed variables” explains that it computes two new variables: count and prop.\nYou can generally use geoms and stats interchangeably. For example, you can recreate the previous plot using stat_count() instead of geom_bar():\n\nggplot(data = diamonds) + \n  stat_count(mapping = aes(x = cut))\n\n\n\n\nThis works because every geom has a default stat; and every stat has a default geom. This means that you can typically use geoms without worrying about the underlying statistical transformation. However, there are three reasons why you might need to use a stat explicitly:\n\n\nYou might want to override the default stat. In the code below, we change the stat of geom_bar() from count (the default) to identity. This lets me map the height of the bars to the raw values of a \\(y\\) variable. Unfortunately when people talk about bar charts casually, they might be referring to this type of bar chart, where the height of the bar is already present in the data, or the previous bar chart where the height of the bar is generated by counting rows.\n\ndemo <- tribble(\n  ~cut,         ~freq,\n  \"Fair\",       1610,\n  \"Good\",       4906,\n  \"Very Good\",  12082,\n  \"Premium\",    13791,\n  \"Ideal\",      21551\n)\n\nggplot(data = demo) +\n  geom_bar(mapping = aes(x = cut, y = freq), stat = \"identity\")\n\n\n\n\n(Don’t worry that you haven’t seen <- or tribble() before. You might be able to guess their meaning from the context, and you’ll learn exactly what they do soon!)\n\n\nYou might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportions, rather than counts:\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, y = after_stat(prop), group = 1))\n\n\n\n\nTo find the variables computed by the stat, look for the section titled “computed variables” in the help for geom_bar().\n\n\nYou might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarizes the y values for each unique x value, to draw attention to the summary that you’re computing:\n\nggplot(data = diamonds) + \n  stat_summary(\n    mapping = aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )\n\n\n\n\n\n\nggplot2 provides more than 20 stats for you to use. Each stat is a function, so you can get help in the usual way, e.g. ?stat_bin. To see a complete list of stats, try the ggplot2 cheatsheet.\n\n2.7.1 Exercises\n\nWhat is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?\nWhat does geom_col() do? How is it different from geom_bar()?\nMost geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common?\nWhat variables does stat_smooth() compute? What parameters control its behaviour?\n\nIn our proportion bar chart, we need to set group = 1. Why? In other words, what is the problem with these two graphs?\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, y = after_stat(prop)))\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = color, y = after_stat(prop)))"
  },
  {
    "objectID": "data-visualize.html#position-adjustments",
    "href": "data-visualize.html#position-adjustments",
    "title": "2  Data visualization",
    "section": "\n2.8 Position adjustments",
    "text": "2.8 Position adjustments\nThere’s one more piece of magic associated with bar charts. You can color a bar chart using either the color aesthetic, or, more usefully, fill:\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, color = cut))\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = cut))\n\n\n\n\n\n\n\n\n\n\n\nNote what happens if you map the fill aesthetic to another variable, like clarity: the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity))\n\n\n\n\nThe stacking is performed automatically using the position adjustment specified by the position argument. If you don’t want a stacked bar chart, you can use one of three other options: \"identity\", \"dodge\" or \"fill\".\n\n\nposition = \"identity\" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA.\n\nggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\nggplot(data = diamonds, mapping = aes(x = cut, color = clarity)) + \n  geom_bar(fill = NA, position = \"identity\")\n\n\n\n\n\n\n\n\n\n\n\nThe identity position adjustment is more useful for 2d geoms, like points, where it is the default.\n\n\nposition = \"fill\" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity), position = \"fill\")\n\n\n\n\n\n\nposition = \"dodge\" places overlapping objects directly beside one another. This makes it easier to compare individual values.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity), position = \"dodge\")\n\n\n\n\n\n\nThere’s one other type of adjustment that’s not useful for bar charts, but can be very useful for scatterplots. Recall our first scatterplot. Did you notice that the plot displays only 126 points, even though there are 234 observations in the dataset?\n\n\n\n\n\nThe underlying values of hwy and displ are rounded so the points appear on a grid and many points overlap each other. This problem is known as overplotting. This arrangement makes it difficult to see the distribution of the data. Are the data points spread equally throughout the graph, or is there one special combination of hwy and displ that contains 109 values?\nYou can avoid this gridding by setting the position adjustment to “jitter”. position = \"jitter\" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), position = \"jitter\")\n\n\n\n\nAdding randomness seems like a strange way to improve your plot, but while it makes your graph less accurate at small scales, it makes your graph more revealing at large scales. Because this is such a useful operation, ggplot2 comes with a shorthand for geom_point(position = \"jitter\"): geom_jitter().\nTo learn more about a position adjustment, look up the help page associated with each adjustment: ?position_dodge, ?position_fill, ?position_identity, ?position_jitter, and ?position_stack.\n\n2.8.1 Exercises\n\n\nWhat is the problem with this plot? How could you improve it?\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + \n  geom_point()\n\n\n\n\n\nWhat parameters to geom_jitter() control the amount of jittering?\nCompare and contrast geom_jitter() with geom_count().\nWhat’s the default position adjustment for geom_boxplot()? Create a visualization of the mpg dataset that demonstrates it."
  },
  {
    "objectID": "data-visualize.html#coordinate-systems",
    "href": "data-visualize.html#coordinate-systems",
    "title": "2  Data visualization",
    "section": "\n2.9 Coordinate systems",
    "text": "2.9 Coordinate systems\nCoordinate systems are probably the most complicated part of ggplot2. The default coordinate system is the Cartesian coordinate system where the x and y positions act independently to determine the location of each point. There are a three other coordinate systems that are occasionally helpful.\n\n\ncoord_flip() switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.\n\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot()\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nHowever, note that you can achieve the same result by flipping the aesthetic mappings of the two variables.\n\nggplot(data = mpg, mapping = aes(y = class, x = hwy)) + \n  geom_boxplot()\n\n\n\n\n\n\ncoord_quickmap() sets the aspect ratio correctly for maps. This is very important if you’re plotting spatial data with ggplot2 (which unfortunately we don’t have the space to cover in this book).\n\nnz <- map_data(\"nz\")\n\nggplot(nz, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\nggplot(nz, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.\n\nbar <- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = cut, fill = cut), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1) +\n  labs(x = NULL, y = NULL)\n\nbar + coord_flip()\nbar + coord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.9.1 Exercises\n\nTurn a stacked bar chart into a pie chart using coord_polar().\nWhat does labs() do? Read the documentation.\nWhat’s the difference between coord_quickmap() and coord_map()?\n\nWhat does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()"
  },
  {
    "objectID": "data-visualize.html#the-layered-grammar-of-graphics",
    "href": "data-visualize.html#the-layered-grammar-of-graphics",
    "title": "2  Data visualization",
    "section": "\n2.10 The layered grammar of graphics",
    "text": "2.10 The layered grammar of graphics\nIn the previous sections, you learned much more than just how to make scatterplots, bar charts, and boxplots. You learned a foundation that you can use to make any type of plot with ggplot2. To see this, let’s add position adjustments, stats, coordinate systems, and faceting to our code template:\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(\n     mapping = aes(<MAPPINGS>),\n     stat = <STAT>, \n     position = <POSITION>\n  ) +\n  <COORDINATE_FUNCTION> +\n  <FACET_FUNCTION>\nOur new template takes seven parameters, the bracketed words that appear in the template. In practice, you rarely need to supply all seven parameters to make a graph because ggplot2 will provide useful defaults for everything except the data, the mappings, and the geom function.\nThe seven parameters in the template compose the grammar of graphics, a formal system for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a geom, a set of mappings, a stat, a position adjustment, a coordinate system, and a faceting scheme.\nTo see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat).\n\n\n\n\n\nNext, you could choose a geometric object to represent each observation in the transformed data. You could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic.\n\n\n\n\n\nYou’d then select a coordinate system to place the geoms into, using the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables. At that point, you would have a complete graph, but you could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting). You could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment.\n\n\n\n\n\nYou could use this method to build any plot that you imagine. In other words, you can use the code template that you’ve learned in this chapter to build hundreds of thousands of unique plots."
  },
  {
    "objectID": "workflow-basics.html",
    "href": "workflow-basics.html",
    "title": "3  Workflow: basics",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz.\nYou now have some experience running R code. We didn’t give you many details, but you’ve obviously figured out the basics, or you would’ve thrown this book away in frustration! Frustration is natural when you start programming in R, because it is such a stickler for punctuation, and even one character out of place will cause it to complain. But while you should expect to be a little frustrated, take comfort in that this experience is both typical and temporary: it happens to everyone, and the only way to get over it is to keep trying.\nBefore we go any further, let’s make sure you’ve got a solid foundation in running R code, and that you know about some of the most helpful RStudio features."
  },
  {
    "objectID": "workflow-basics.html#coding-basics",
    "href": "workflow-basics.html#coding-basics",
    "title": "3  Workflow: basics",
    "section": "\n3.1 Coding basics",
    "text": "3.1 Coding basics\nLet’s review some basics we’ve so far omitted in the interests of getting you plotting as quickly as possible. You can use R as a calculator:\n\n1 / 200 * 30\n#> [1] 0.15\n(59 + 73 + 2) / 3\n#> [1] 44.66667\nsin(pi / 2)\n#> [1] 1\n\nYou can create new objects with the assignment operator <-:\n\nx <- 3 * 4\n\nYou can combine multiple elements into a vector with c():\n\nprimes <- c(1, 2, 3, 5, 7, 11, 13)\n\nAnd basic arithmetic is applied to every element of the vector:\n\nprimes * 2\n#> [1]  2  4  6 10 14 22 26\nprimes - 1\n#> [1]  0  1  2  4  6 10 12\n\nAll R statements where you create objects, assignment statements, have the same form:\n\nobject_name <- value\n\nWhen reading that code, say “object name gets value” in your head.\nYou will make lots of assignments and <- is a pain to type. Don’t be lazy and use =; it will work, but it will cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds <- with spaces, which is a good code formatting practice. Code is miserable to read on a good day, so giveyoureyesabreak and use spaces."
  },
  {
    "objectID": "workflow-basics.html#comments",
    "href": "workflow-basics.html#comments",
    "title": "3  Workflow: basics",
    "section": "\n3.2 Comments",
    "text": "3.2 Comments\nR will ignore any text after #. This allows to you to write comments, text that is ignored by R but read by other humans. We’ll sometimes include comments in examples explaining what’s happening with the code.\nComments can be helpful for briefly describing what the subsequent code does.\n\n# define primes\nprimes <- c(1, 2, 3, 5, 7, 11, 13)\n\n# multiply primes by 2\nprimes * 2\n#> [1]  2  4  6 10 14 22 26\n\nWith short pieces of code like this, it might not be necessary to leave a command for every single line of code. But as the code you’re writing gets more complex, comments can save you (and your collaborators) a lot of time in figuring out what was done in the code.\nHowever, ultimately, what was done is possible to figure out, even if it might be tedious at times, as the code is self-documenting. However, remembering or figuring out why something was done can be much more difficult, or impossible. For example, geom_smooth(), which draws a smooth curve to represent the patterns of the data has an argument called span, which controls the “wiggliness” of the smoother with larger values for span yielding a smoother curve. The default value of this argument is 0.75. Suppose you decide to change the value of span, and set it to 0.3. It would be very useful to add a comment noting why you decided to make this change, for yourself in the future and others reviewing your code. In the following example the first comment for the same code is not as good as the second one as it doesn’t say why the decision to change the span was made."
  },
  {
    "objectID": "workflow-basics.html#sec-whats-in-a-name",
    "href": "workflow-basics.html#sec-whats-in-a-name",
    "title": "3  Workflow: basics",
    "section": "\n3.3 What’s in a name?",
    "text": "3.3 What’s in a name?\nObject names must start with a letter, and can only contain letters, numbers, _ and .. You want your object names to be descriptive, so you’ll need to adopt a convention for multiple words. We recommend snake_case where you separate lowercase words with _.\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\nAnd_aFew.People_RENOUNCEconvention\n\nWe’ll come back to names again when we talk more about code style in Chapter 7.\nYou can inspect an object by typing its name:\n\nx\n#> [1] 12\n\nMake another assignment:\n\nthis_is_a_really_long_name <- 2.5\n\nTo inspect this object, try out RStudio’s completion facility: type “this”, press TAB, add characters until you have a unique prefix, then press return.\nOoops, you made a mistake! The value of this_is_a_really_long_name should be 3.5, not 2.5. Use another keyboard shortcut to help you fix it. Type “this” then press Cmd/Ctrl + ↑. Doing so will list all the commands you’ve typed that start with those letters. Use the arrow keys to navigate, then press enter to retype the command. Change 2.5 to 3.5 and rerun.\nMake yet another assignment:\n\nr_rocks <- 2 ^ 3\n\nLet’s try to inspect it:\n\nr_rock\n#> Error: object 'r_rock' not found\nR_rocks\n#> Error: object 'R_rocks' not found\n\nThis illustrates the implied contract between you and R: R will do the tedious computations for you, but in exchange, you must be completely precise in your instructions. Typos matter; R can’t read your mind and say “oh, they probably meant r_rocks when they typed r_rock”. Case matters; similarly R can’t read your mind and say “oh, they probably meant r_rocks when they typed R_rocks”."
  },
  {
    "objectID": "workflow-basics.html#calling-functions",
    "href": "workflow-basics.html#calling-functions",
    "title": "3  Workflow: basics",
    "section": "\n3.4 Calling functions",
    "text": "3.4 Calling functions\nR has a large collection of built-in functions that are called like this:\n\nfunction_name(arg1 = val1, arg2 = val2, ...)\n\nLet’s try using seq(), which makes regular sequences of numbers and, while we’re at it, learn more helpful features of RStudio. Type se and hit TAB. A popup shows you possible completions. Specify seq() by typing more (a q) to disambiguate, or by using ↑/↓ arrows to select. Notice the floating tooltip that pops up, reminding you of the function’s arguments and purpose. If you want more help, press F1 to get all the details in the help tab in the lower right pane.\nWhen you’ve selected the function you want, press TAB again. RStudio will add matching opening (() and closing ()) parentheses for you. Type the arguments 1, 10 and hit return.\n\nseq(1, 10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\nType this code and notice that RStudio provides similar assistance with the paired quotation marks:\n\nx <- \"hello world\"\n\nQuotation marks and parentheses must always come in a pair. RStudio does its best to help you, but it’s still possible to mess up and end up with a mismatch. If this happens, R will show you the continuation character “+”:\n> x <- \"hello\n+\nThe + tells you that R is waiting for more input; it doesn’t think you’re done yet. Usually, this means you’ve forgotten either a \" or a ). Either add the missing pair, or press ESCAPE to abort the expression and try again.\nNote that the environment tab in the upper right pane displays all of the objects that you’ve created:"
  },
  {
    "objectID": "workflow-basics.html#exercises",
    "href": "workflow-basics.html#exercises",
    "title": "3  Workflow: basics",
    "section": "\n3.5 Exercises",
    "text": "3.5 Exercises\n\n\nWhy does this code not work?\n\nmy_variable <- 10\nmy_varıable\n#> Error in eval(expr, envir, enclos): object 'my_varıable' not found\n\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)\n\n\nTweak each of the following R commands so that they run correctly:\n\nlibary(tidyverse)\n\nggplot(dota = mpg) + \n  geom_point(maping = aes(x = displ, y = hwy))\n\n\nPress Alt + Shift + K. What happens? How can you get to the same place using the menus?"
  },
  {
    "objectID": "data-transform.html",
    "href": "data-transform.html",
    "title": "4  Data transformation",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "data-transform.html#introduction",
    "href": "data-transform.html#introduction",
    "title": "4  Data transformation",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nVisualisation is an important tool for generating insight, but it’s rare that you get the data in exactly the right form you need for it. Often you’ll need to create some new variables or summaries to see the most important patterns, or maybe you just want to rename the variables or reorder the observations to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this chapter, which will introduce you to data transformation using the dplyr package and a new dataset on flights that departed New York City in 2013.\nThe goal of this chapter is to give you an overview of all the key tools for transforming a data frame. We’ll come back these functions in more detail in later chapters, as we start to dig into specific types of data (e.g. numbers, strings, dates).\n\n4.1.1 Prerequisites\nIn this chapter we’ll focus on the dplyr package, another core member of the tidyverse. We’ll illustrate the key ideas using data from the nycflights13 package, and use ggplot2 to help us understand the data.\n\nlibrary(nycflights13)\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n\nTake careful note of the conflicts message that’s printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag(). So far we’ve mostly ignored which package a function comes from because most of the time it doesn’t matter. However, knowing the package can help you find help and find related functions, so when we need to be precise about which function a package comes from, we’ll use the same syntax as R: packagename::functionname().\n\n4.1.2 nycflights13\nTo explore the basic dplyr verbs, we’re going to use nycflights13::flights. This dataset contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented in ?flights.\n\nflights\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nIf you’ve used R before, you might notice that this data frame prints a little differently to other data frames you’ve seen. That’s because it’s a tibble, a special type of data frame used by the tidyverse to avoid some common gotchas. The most important difference is the way it prints: tibbles are designed for large datasets, so they only show the first few rows and only the columns that fit on one screen. To see everything, use View(flights) to open the dataset in the RStudio viewer. We’ll come back to other important differences in Chapter 12.\nYou might have noticed the short abbreviations that follow each column name. These tell you the type of each variable: <int> is short for integer, <dbl> is short for double (aka real numbers), <chr> for character (aka strings), and <dttm> for date-time. These are important because the operations you can perform on a column depend so much on its “type”, and these types are used to organize the chapters in the next section of the book.\n\n4.1.3 dplyr basics\nYou’re about to learn the primary dplyr verbs which will allow you to solve the vast majority of your data manipulation challenges. But before we discuss their individual differences, it’s worth stating what they have in common:\n\nThe first argument is always a data frame.\nThe subsequent arguments describe what to do with the data frame, using the variable names (without quotes).\nThe result is always a new data frame.\n\nBecause the first argument is a data frame and the output is a data frame, dplyr verbs work work well with the pipe, |>. The pipe takes the thing on its left and passes it along to the function on its right so that x |> f(y) is equivalent to f(x, y), and x |> f(y) |> g(z) is equivalent to into g(f(x, y), z). The easiest way to pronounce the pipe is “then”. That makes it possible to get a sense of the following code even though you haven’t yet learnt the details:\n\nflights |>\n  filter(dest == \"IAH\") |> \n  group_by(year, month, day) |> \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\nThe code starts with the flights dataset, then filters it, then groups it, then summarizes it. We’ll come back to the pipe and its alternatives in Section 7.3.\ndplyr’s verbs are organised into four groups based on what they operate on: rows, columns, groups, or tables. In the following sections you’ll learn the most important verbs for rows, columns, and groups, then we’ll come back to verb that work on tables in Chapter 13. Let’s dive in!"
  },
  {
    "objectID": "data-transform.html#rows",
    "href": "data-transform.html#rows",
    "title": "4  Data transformation",
    "section": "\n4.2 Rows",
    "text": "4.2 Rows\nThe most important verbs that operate on rows are filter(), which changes which rows are present without changing their order, and arrange(), which changes the order of the rows without changing which are present. Both functions only affect the rows, and the columns are left unchanged.\n\n4.2.1 filter()\n\nfilter() allows you to keep rows based on the values of the columns1. The first argument is the data frame. The second and subsequent arguments are the conditions that must be true to keep the row. For example, we could find all flights that arrived more than 120 minutes (two hours) late:\n\nflights |> \n  filter(arr_delay > 120)\n#> # A tibble: 10,034 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      811         630     101    1047     830     137 MQ     \n#> 2  2013     1     1      848        1835     853    1001    1950     851 MQ     \n#> 3  2013     1     1      957         733     144    1056     853     123 UA     \n#> 4  2013     1     1     1114         900     134    1447    1222     145 UA     \n#> 5  2013     1     1     1505        1310     115    1638    1431     127 EV     \n#> 6  2013     1     1     1525        1340     105    1831    1626     125 B6     \n#> # … with 10,028 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nAs well as > (greater than), you can use >= (greater than or equal to), < (less than), <= (less than or equal to), == (equal to), and != (not equal to). You can also use & (and) or | (or) to combine multiple conditions:\n\n# Flights that departed on January 1\nflights |> \n  filter(month == 1 & day == 1)\n#> # A tibble: 842 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 836 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n# Flights that departed in January or February\nflights |> \n  filter(month == 1 | month == 2)\n#> # A tibble: 51,955 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 51,949 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThere’s a useful shortcut when you’re combining | and ==: %in%. It keeps rows where the variable equals one of the values on the right:\n\n# A shorter way to select flights that departed in January or February\nflights |> \n  filter(month %in% c(1, 2))\n#> # A tibble: 51,955 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 51,949 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nWe’ll come back to these comparisons and logical operators in more detail in Chapter 14.\nWhen you run filter() dplyr executes the filtering operation, creating a new data frame, and then prints it. It doesn’t modify the existing flights dataset because dplyr functions never modify their inputs. To save the result, you need to use the assignment operator, <-:\n\njan1 <- flights |> \n  filter(month == 1 & day == 1)\n\n\n4.2.2 Common mistakes\nWhen you’re starting out with R, the easiest mistake to make is to use = instead of == when testing for equality. filter() will let you know when this happens:\n\nflights |> \n  filter(month = 1)\n#> Error in `filter()`:\n#> ! We detected a named input.\n#> ℹ This usually means that you've used `=` instead of `==`.\n#> ℹ Did you mean `month == 1`?\n\nAnother mistakes is you write “or” statements like you would in English:\n\nflights |> \n  filter(month == 1 | 2)\n\nThis works, in the sense that it doesn’t throw an error, but it doesn’t do what you want. We’ll come back to what it does and why in Section 17.3.2.\n\n4.2.3 arrange()\n\narrange() changes the order of the rows based on the value of the columns. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. For example, the following code sorts by the departure time, which is spread over four columns.\n\nflights |> \n  arrange(year, month, day, dep_time)\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nYou can use desc() to re-order by a column in descending order. For example, this code shows the most delayed flights:\n\nflights |> \n  arrange(desc(dep_delay))\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     9      641         900    1301    1242    1530    1272 HA     \n#> 2  2013     6    15     1432        1935    1137    1607    2120    1127 MQ     \n#> 3  2013     1    10     1121        1635    1126    1239    1810    1109 MQ     \n#> 4  2013     9    20     1139        1845    1014    1457    2210    1007 AA     \n#> 5  2013     7    22      845        1600    1005    1044    1815     989 MQ     \n#> 6  2013     4    10     1100        1900     960    1342    2211     931 DL     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nYou can combine arrange() and filter() to solve more complex problems. For example, we could look for the flights that were most delayed on arrival that left on roughly on time:\n\nflights |> \n  filter(dep_delay <= 10 & dep_delay >= -10) |> \n  arrange(desc(arr_delay))\n#> # A tibble: 239,109 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013    11     1      658         700      -2    1329    1015     194 VX     \n#> 2  2013     4    18      558         600      -2    1149     850     179 AA     \n#> 3  2013     7     7     1659        1700      -1    2050    1823     147 US     \n#> 4  2013     7    22     1606        1615      -9    2056    1831     145 DL     \n#> 5  2013     9    19      648         641       7    1035     810     145 UA     \n#> 6  2013     4    18      655         700      -5    1213     950     143 AA     \n#> # … with 239,103 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n4.2.4 Exercises\n\n\nFind all flights that\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\nSort flights to find the fastest flights (Hint: try sorting by a calculation).\nWhich flights traveled the farthest? Which traveled the shortest?\nDoes it matter what order you used filter() and arrange() in if you’re using both? Why/why not? Think about the results and how much work the functions would have to do."
  },
  {
    "objectID": "data-transform.html#columns",
    "href": "data-transform.html#columns",
    "title": "4  Data transformation",
    "section": "\n4.3 Columns",
    "text": "4.3 Columns\nThere are four important verbs that affect the columns without changing the rows: mutate(), select(), rename(), and relocate(). mutate() creates new columns that are functions of the existing columns; select(), rename(), and relocate() change which columns are present, their names, or their positions.\n\n4.3.1 mutate()\n\nThe job of mutate() is to add new columns that are calculated from the existing columns. In the transform chapters, you’ll learn a large set of functions that you can use to manipulate different types of variables. For now, we’ll stick with basic algebra, which allows us to compute the gain, how much time a delayed flight made up in the air, and the speed in miles per hour:\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\n#> # A tibble: 336,776 × 21\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 11 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, gain <dbl>, speed <dbl>, and abbreviated\n#> #   variable names ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time,\n#> #   ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nBy default, mutate() adds new columns on the right hand side of your dataset, which makes it difficult to see what’s happening here. We can use the .before argument to instead add the variables to the left hand side2:\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\n#> # A tibble: 336,776 × 21\n#>    gain speed  year month   day dep_time sched…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵\n#>   <dbl> <dbl> <int> <int> <int>    <int>   <int>   <dbl>   <int>   <int>   <dbl>\n#> 1    -9  370.  2013     1     1      517     515       2     830     819      11\n#> 2   -16  374.  2013     1     1      533     529       4     850     830      20\n#> 3   -31  408.  2013     1     1      542     540       2     923     850      33\n#> 4    17  517.  2013     1     1      544     545      -1    1004    1022     -18\n#> 5    19  394.  2013     1     1      554     600      -6     812     837     -25\n#> 6   -16  288.  2013     1     1      554     558      -4     740     728      12\n#> # … with 336,770 more rows, 10 more variables: carrier <chr>, flight <int>,\n#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#> #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThe . is a sign that .before is an argument to the function, not the name of a new variable. You can also use .after to add after a variable, and in both .before and .after you can the name of a variable name instead of a position. For example, we could add the new variables after day:\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .after = day\n  )\n#> # A tibble: 336,776 × 21\n#>    year month   day  gain speed dep_time sched…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵\n#>   <int> <int> <int> <dbl> <dbl>    <int>   <int>   <dbl>   <int>   <int>   <dbl>\n#> 1  2013     1     1    -9  370.      517     515       2     830     819      11\n#> 2  2013     1     1   -16  374.      533     529       4     850     830      20\n#> 3  2013     1     1   -31  408.      542     540       2     923     850      33\n#> 4  2013     1     1    17  517.      544     545      -1    1004    1022     -18\n#> 5  2013     1     1    19  394.      554     600      -6     812     837     -25\n#> 6  2013     1     1   -16  288.      554     558      -4     740     728      12\n#> # … with 336,770 more rows, 10 more variables: carrier <chr>, flight <int>,\n#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#> #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nAlternatively, you can control which variables are kept with the .keep argument. A particularly useful argument is \"used\" which allows you to see the inputs and outputs from your calculations:\n\nflights |> \n  mutate(,\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 6\n#>   dep_delay arr_delay air_time  gain hours gain_per_hour\n#>       <dbl>     <dbl>    <dbl> <dbl> <dbl>         <dbl>\n#> 1         2        11      227    -9  3.78         -2.38\n#> 2         4        20      227   -16  3.78         -4.23\n#> 3         2        33      160   -31  2.67        -11.6 \n#> 4        -1       -18      183    17  3.05          5.57\n#> 5        -6       -25      116    19  1.93          9.83\n#> 6        -4        12      150   -16  2.5          -6.4 \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n4.3.2 select()\n\nIt’s not uncommon to get datasets with hundreds or even thousands of variables. In this situation, the first challenge is often just focusing on the variables you’re interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea of how it works:\n\n# Select columns by name\nflights |> \n  select(year, month, day)\n#> # A tibble: 336,776 × 3\n#>    year month   day\n#>   <int> <int> <int>\n#> 1  2013     1     1\n#> 2  2013     1     1\n#> 3  2013     1     1\n#> 4  2013     1     1\n#> 5  2013     1     1\n#> 6  2013     1     1\n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n# Select all columns between year and day (inclusive)\nflights |> \n  select(year:day)\n#> # A tibble: 336,776 × 3\n#>    year month   day\n#>   <int> <int> <int>\n#> 1  2013     1     1\n#> 2  2013     1     1\n#> 3  2013     1     1\n#> 4  2013     1     1\n#> 5  2013     1     1\n#> 6  2013     1     1\n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n# Select all columns except those from year to day (inclusive)\nflights |> \n  select(!year:day)\n#> # A tibble: 336,776 × 16\n#>   dep_time sched…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier flight tailnum origin\n#>      <int>   <int>   <dbl>   <int>   <int>   <dbl> <chr>    <int> <chr>   <chr> \n#> 1      517     515       2     830     819      11 UA        1545 N14228  EWR   \n#> 2      533     529       4     850     830      20 UA        1714 N24211  LGA   \n#> 3      542     540       2     923     850      33 AA        1141 N619AA  JFK   \n#> 4      544     545      -1    1004    1022     -18 B6         725 N804JB  JFK   \n#> 5      554     600      -6     812     837     -25 DL         461 N668DN  LGA   \n#> 6      554     558      -4     740     728      12 UA        1696 N39463  EWR   \n#> # … with 336,770 more rows, 6 more variables: dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated\n#> #   variable names ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time,\n#> #   ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n# Select all columns that are characters\nflights |> \n  select(where(is.character))\n#> # A tibble: 336,776 × 4\n#>   carrier tailnum origin dest \n#>   <chr>   <chr>   <chr>  <chr>\n#> 1 UA      N14228  EWR    IAH  \n#> 2 UA      N24211  LGA    IAH  \n#> 3 AA      N619AA  JFK    MIA  \n#> 4 B6      N804JB  JFK    BQN  \n#> 5 DL      N668DN  LGA    ATL  \n#> 6 UA      N39463  EWR    ORD  \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThere are a number of helper functions you can use within select():\n\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\n\nends_with(\"xyz\"): matches names that end with “xyz”.\n\ncontains(\"ijk\"): matches names that contain “ijk”.\n\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\nSee ?select for more details. Once you know regular expressions (the topic of Chapter 17) you’ll also be use matches() to select variables that match a pattern.\nYou can rename variables as you select() them by using =. The new name appears on the left hand side of the =, and the old variable appears on the right hand side:\n\nflights |> \n  select(tail_num = tailnum)\n#> # A tibble: 336,776 × 1\n#>   tail_num\n#>   <chr>   \n#> 1 N14228  \n#> 2 N24211  \n#> 3 N619AA  \n#> 4 N804JB  \n#> 5 N668DN  \n#> 6 N39463  \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n4.3.3 rename()\n\nIf you just want to keep all the existing variables and just want to rename a few, you can use rename() instead of select():\n\nflights |> \n  rename(tail_num = tailnum)\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tail_num <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nIt works exactly the same way as select(), but keeps all the variables that aren’t explicitly selected.\nIf you have a bunch of inconsistently named columns and it would be painful to fix them all by hand, check out janitor::clean_names() which provides some useful automated cleaning.\n\n4.3.4 relocate()\n\nYou can move variables around with relocate(). By default it moves variables to the front:\n\nflights |> \n  relocate(time_hour, air_time)\n#> # A tibble: 336,776 × 19\n#>   time_hour           air_time  year month   day dep_t…¹ sched…² dep_d…³ arr_t…⁴\n#>   <dttm>                 <dbl> <int> <int> <int>   <int>   <int>   <dbl>   <int>\n#> 1 2013-01-01 05:00:00      227  2013     1     1     517     515       2     830\n#> 2 2013-01-01 05:00:00      227  2013     1     1     533     529       4     850\n#> 3 2013-01-01 05:00:00      160  2013     1     1     542     540       2     923\n#> 4 2013-01-01 05:00:00      183  2013     1     1     544     545      -1    1004\n#> 5 2013-01-01 06:00:00      116  2013     1     1     554     600      -6     812\n#> 6 2013-01-01 05:00:00      150  2013     1     1     554     558      -4     740\n#> # … with 336,770 more rows, 10 more variables: sched_arr_time <int>,\n#> #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#> #   dest <chr>, distance <dbl>, hour <dbl>, minute <dbl>, and abbreviated\n#> #   variable names ¹​dep_time, ²​sched_dep_time, ³​dep_delay, ⁴​arr_time\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nBut you can use the same .before and .after arguments as mutate() to choose where to put them:\n\nflights |> \n  relocate(year:dep_time, .after = time_hour)\n#> # A tibble: 336,776 × 19\n#>   sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier flight tailnum origin dest \n#>        <int>   <dbl>   <int>   <int>   <dbl> <chr>    <int> <chr>   <chr>  <chr>\n#> 1        515       2     830     819      11 UA        1545 N14228  EWR    IAH  \n#> 2        529       4     850     830      20 UA        1714 N24211  LGA    IAH  \n#> 3        540       2     923     850      33 AA        1141 N619AA  JFK    MIA  \n#> 4        545      -1    1004    1022     -18 B6         725 N804JB  JFK    BQN  \n#> 5        600      -6     812     837     -25 DL         461 N668DN  LGA    ATL  \n#> 6        558      -4     740     728      12 UA        1696 N39463  EWR    ORD  \n#> # … with 336,770 more rows, 9 more variables: air_time <dbl>, distance <dbl>,\n#> #   hour <dbl>, minute <dbl>, time_hour <dttm>, year <int>, month <int>,\n#> #   day <int>, dep_time <int>, and abbreviated variable names ¹​sched_dep_time,\n#> #   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\nflights |> \n  relocate(starts_with(\"arr\"), .before = dep_time)\n#> # A tibble: 336,776 × 19\n#>    year month   day arr_time arr_delay dep_time sched_…¹ dep_d…² sched…³ carrier\n#>   <int> <int> <int>    <int>     <dbl>    <int>    <int>   <dbl>   <int> <chr>  \n#> 1  2013     1     1      830        11      517      515       2     819 UA     \n#> 2  2013     1     1      850        20      533      529       4     830 UA     \n#> 3  2013     1     1      923        33      542      540       2     850 AA     \n#> 4  2013     1     1     1004       -18      544      545      -1    1022 B6     \n#> 5  2013     1     1      812       -25      554      600      -6     837 DL     \n#> 6  2013     1     1      740        12      554      558      -4     728 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​sched_arr_time\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n4.3.5 Exercises\n\n\n\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\nWhat happens if you include the name of a variable multiple times in a select() call?\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvariables <- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\nDoes the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?\n\nselect(flights, contains(\"TIME\"))"
  },
  {
    "objectID": "data-transform.html#groups",
    "href": "data-transform.html#groups",
    "title": "4  Data transformation",
    "section": "\n4.4 Groups",
    "text": "4.4 Groups\nSo far you’ve learned about functions that work with rows and columns. dplyr gets even more powerful when you add in the ability to work with groups. In this section, we’ll focus on the most important functions: group_by(), summarize(), and the slice family of functions.\n\n4.4.1 group_by()\n\nUse group_by() to divide your dataset into groups meaningful for your analysis:\n\nflights |> \n  group_by(month)\n#> # A tibble: 336,776 × 19\n#> # Groups:   month [12]\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\ngroup_by() doesn’t change the data but, if you look closely at the output, you’ll notice that it’s now “grouped by” month. This means subsequent operations will now work “by month”.\n\n4.4.2 summarize()\n\nThe most important grouped operation is a summary. It collapses each group to a single row3. Here we compute the average departure delay by month:\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(dep_delay)\n  )\n#> # A tibble: 12 × 2\n#>   month delay\n#>   <int> <dbl>\n#> 1     1    NA\n#> 2     2    NA\n#> 3     3    NA\n#> 4     4    NA\n#> 5     5    NA\n#> 6     6    NA\n#> # … with 6 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nUhoh! Something has gone wrong and all of our results are NA (pronounced “N-A”), R’s symbol for missing value. We’ll come back to discuss missing values in Chapter 20, but for now we’ll remove them by using na.rm = TRUE:\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE)\n  )\n#> # A tibble: 12 × 2\n#>   month delay\n#>   <int> <dbl>\n#> 1     1  10.0\n#> 2     2  10.8\n#> 3     3  13.2\n#> 4     4  13.9\n#> 5     5  13.0\n#> 6     6  20.8\n#> # … with 6 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou can create any number of summaries in a single call to summarize(). You’ll learn various useful summaries in the upcoming chapters, but one very useful summary is n(), which returns the number of rows in each group:\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n#> # A tibble: 12 × 3\n#>   month delay     n\n#>   <int> <dbl> <int>\n#> 1     1  10.0 27004\n#> 2     2  10.8 24951\n#> 3     3  13.2 28834\n#> 4     4  13.9 28330\n#> 5     5  13.0 28796\n#> 6     6  20.8 28243\n#> # … with 6 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nMeans and counts can get you a surprisingly long way in data science!\n\n4.4.3 The slice_ functions\nThere are five handy functions that allow you pick off specific rows within each group:\n\n\ndf |> slice_head(n = 1) takes the first row from each group.\n\ndf |> slice_tail(n = 1) takes the last row in each group.\n\ndf |> slice_min(x, n = 1) takes the row with the smallest value of x.\n\ndf |> slice_max(x, n = 1) takes the row with the largest value of x.\n\ndf |> slice_sample(x, n = 1) takes one random row.\n\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group. For example, the following code finds the most delayed flight to each destination:\n\nflights |> \n  group_by(dest) |> \n  slice_max(arr_delay, n = 1)\n#> # A tibble: 107 × 19\n#> # Groups:   dest [104]\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     7    22     2145        2007      98     132    2259     153 B6     \n#> 2  2013     7    23     1139         800     219    1250     909     221 B6     \n#> 3  2013     1    25      123        2000     323     229    2101     328 EV     \n#> 4  2013     8    17     1740        1625      75    2042    2003      39 UA     \n#> 5  2013     7    22     2257         759     898     121    1026     895 DL     \n#> 6  2013     7    10     2056        1505     351    2347    1758     349 UA     \n#> # … with 101 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThis is similar to computing the max delay with summarize(), but you get the whole row instead of the single summary:\n\nflights |> \n  group_by(dest) |> \n  summarize(max_delay = max(arr_delay, na.rm = TRUE))\n#> Warning in max(arr_delay, na.rm = TRUE): no non-missing arguments to max;\n#> returning -Inf\n#> # A tibble: 105 × 2\n#>   dest  max_delay\n#>   <chr>     <dbl>\n#> 1 ABQ         153\n#> 2 ACK         221\n#> 3 ALB         328\n#> 4 ANC          39\n#> 5 ATL         895\n#> 6 AUS         349\n#> # … with 99 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n4.4.4 Grouping by multiple variables\nYou can create groups using more than one variable. For example, we could make a group for each day:\n\ndaily <- flights |>  \n  group_by(year, month, day)\ndaily\n#> # A tibble: 336,776 × 19\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nWhen you summarize a tibble grouped by more than one variable, each summary peels off the last group. In hindsight, this wasn’t great way to make this function work, but it’s difficult to change without breaking existing code. To make it obvious what’s happening, dplyr displays a message that tells you how you can change this behavior:\n\ndaily_flights <- daily |> \n  summarize(\n    n = n()\n  )\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the\n#> `.groups` argument.\n\nIf you’re happy with this behavior, you can explicitly request it in order to suppress the message:\n\ndaily_flights <- daily |> \n  summarize(\n    n = n(), \n    .groups = \"drop_last\"\n  )\n\nAlternatively, change the default behavior by setting a different value, e.g. \"drop\" to drop all grouping or \"keep\" to preserve the same groups.\n\n4.4.5 Ungrouping\nYou might also want to remove grouping outside of summarize(). You can do this with ungroup().\n\ndaily |> \n  ungroup() |>\n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    flights = n()\n  )\n#> # A tibble: 1 × 2\n#>   delay flights\n#>   <dbl>   <int>\n#> 1  12.6  336776\n\nAs you can see, when you summarize an ungrouped data frame, you get a single row back because dplyr treats all the rows in an ungrouped data frame as belonging to one group.\n\n4.4.6 Exercises\n\nWhich carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |> group_by(carrier, dest) |> summarize(n()))\nFind the most delayed flight to each destination.\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\nWhat happens if you supply a negative n to slice_min() and friends?\nExplain what count() does in terms of the dplyr verbs you just learn. What does the sort argument to count() do?"
  },
  {
    "objectID": "data-transform.html#sec-sample-size",
    "href": "data-transform.html#sec-sample-size",
    "title": "4  Data transformation",
    "section": "\n4.5 Case study: aggregates and sample size",
    "text": "4.5 Case study: aggregates and sample size\nWhenever you do any aggregation, it’s always a good idea to include a count (n()). That way, you can ensure that you’re not drawing conclusions based on very small amounts of data. For example, let’s look at the planes (identified by their tail number) that have the highest average delays:\n\ndelays <- flights |>  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |> \n  group_by(tailnum) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(delays, aes(delay)) + \n  geom_freqpoly(binwidth = 10)\n\n\n\n\nWow, there are some planes that have an average delay of 5 hours (300 minutes)! That seems pretty surprising, so lets draw a scatterplot of number of flights vs. average delay:\n\nggplot(delays, aes(n, delay)) + \n  geom_point(alpha = 1/10)\n\n\n\n\nNot surprisingly, there is much greater variation in the average delay when there are few flights for a given plane. The shape of this plot is very characteristic: whenever you plot a mean (or other summary) vs. group size, you’ll see that the variation decreases as the sample size increases4.\nWhen looking at this sort of plot, it’s often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups:\n\ndelays |>  \n  filter(n > 25) |> \n  ggplot(aes(n, delay)) + \n  geom_point(alpha = 1/10) + \n  geom_smooth(se = FALSE)\n\n\n\n\nNote the handy pattern for combining ggplot2 and dplyr. It’s a bit annoying that you have to switch from |> to +, but it’s not too much of a hassle once you get the hang of it.\nThere’s another common variation on this pattern that we can see in some data about baseball players. The following code uses data from the Lahman package to compare what proportion of times a player hits the ball vs. the number of attempts they take:\n\nbatters <- Lahman::Batting |> \n  group_by(playerID) |> \n  summarize(\n    perf = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    n = sum(AB, na.rm = TRUE)\n  )\nbatters\n#> # A tibble: 20,166 × 3\n#>   playerID    perf     n\n#>   <chr>      <dbl> <int>\n#> 1 aardsda01 0          4\n#> 2 aaronha01 0.305  12364\n#> 3 aaronto01 0.229    944\n#> 4 aasedo01  0          5\n#> 5 abadan01  0.0952    21\n#> 6 abadfe01  0.111      9\n#> # … with 20,160 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWhen we plot the skill of the batter (measured by the batting average, ba) against the number of opportunities to hit the ball (measured by at bat, ab), you see two patterns:\n\nAs above, the variation in our aggregate decreases as we get more data points.\nThere’s a positive correlation between skill (perf) and opportunities to hit the ball (n) because obviously teams want to give their best batters the most opportunities to hit the ball.\n\n\nbatters |> \n  filter(n > 100) |> \n  ggplot(aes(n, perf)) +\n    geom_point(alpha = 1 / 10) + \n    geom_smooth(se = FALSE)\n\n\n\n\nThis also has important implications for ranking. If you naively sort on desc(ba), the people with the best batting averages are clearly lucky, not skilled:\n\nbatters |> \n  arrange(desc(perf))\n#> # A tibble: 20,166 × 3\n#>   playerID   perf     n\n#>   <chr>     <dbl> <int>\n#> 1 abramge01     1     1\n#> 2 alberan01     1     1\n#> 3 banisje01     1     1\n#> 4 bartocl01     1     1\n#> 5 bassdo01      1     1\n#> 6 birasst01     1     2\n#> # … with 20,160 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou can find a good explanation of this problem and how to overcome it at http://varianceexplained.org/r/empirical_bayes_baseball/ and http://www.evanmiller.org/how-not-to-sort-by-average-rating.html."
  },
  {
    "objectID": "workflow-pipes.html",
    "href": "workflow-pipes.html",
    "title": "5  Workflow: Pipes",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is largely complete and just needs final proof reading. You can find the complete first edition at https://r4ds.had.co.nz.\nThe pipe, |>, is a powerful tool for clearly expressing a sequence of operations that transform an object. We briefly introduced pipes in the previous chapter, but before going too much farther, we want to give a few more details and discuss %>%, a predecessor to |>.\nTo add the pipe to your code, we recommend using the build-in keyboard shortcut Ctrl/Cmd + Shift + M. You’ll need to make one change to your RStudio options to use |> instead of %>% as shown in Figure 5.1; more on %>% shortly."
  },
  {
    "objectID": "workflow-pipes.html#why-use-a-pipe",
    "href": "workflow-pipes.html#why-use-a-pipe",
    "title": "5  Workflow: Pipes",
    "section": "\n5.1 Why use a pipe?",
    "text": "5.1 Why use a pipe?\nEach individual dplyr verb is quite simple, so solving complex problems typically requires combining multiple verbs. For example, the last chapter finished with a moderately complex pipe:\n\nflights |>  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |> \n  group_by(tailnum) |> \n  summarise(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\nEven though this pipe has four steps, it’s easy to skim because the verbs come at the start of each line: start with the flights data, then filter, then group, then summarize.\nWhat would happen if we didn’t have the pipe? We could nest each function call inside the previous call:\n\nsummarise(\n  group_by(\n    filter(\n      flights, \n      !is.na(arr_delay), !is.na(tailnum)\n    ),\n    tailnum\n  ), \n  delay = mean(arr_delay, na.rm = TRUE\n  ), \n  n = n()\n)\n\nOr we could use a bunch of intermediate variables:\n\nflights1 <- filter(flights, !is.na(arr_delay), !is.na(tailnum))\nflights2 <- group_by(flights1, tailnum) \nflights3 <- summarise(flight2,\n  delay = mean(arr_delay, na.rm = TRUE),\n  n = n()\n)\n\nWhile both of these forms have their time and place, the pipe generally produces data analysis code that’s both easier to write and easier to read."
  },
  {
    "objectID": "workflow-pipes.html#magrittr-and-the-pipe",
    "href": "workflow-pipes.html#magrittr-and-the-pipe",
    "title": "5  Workflow: Pipes",
    "section": "\n5.2 magrittr and the %>% pipe",
    "text": "5.2 magrittr and the %>% pipe\nIf you’ve been using the tidyverse for a while, you might be familiar with the %>% pipe provided by the magrittr package. The magrittr package is included in the core tidyverse, so you can use %>% whenever you load the tidyverse:\n\nlibrary(tidyverse)\n\nmtcars %>% \n  group_by(cyl) %>%\n  summarise(n = n())\n#> # A tibble: 3 × 2\n#>     cyl     n\n#>   <dbl> <int>\n#> 1     4    11\n#> 2     6     7\n#> 3     8    14\n\nFor simple cases |> and %>% behave identically. So why do we recommend the base pipe? Firstly, because it’s part of base R, it’s always available for you to use, even when you’re not using the tidyverse. Secondly, |> is quite a bit simpler than %>%: in the time between the invention of %>% in 2014 and the inclusion of |> in R 4.1.0 in 2021, we gained a better understanding of the pipe. This allowed the base implementation to jettison infrequently used and less important features."
  },
  {
    "objectID": "workflow-pipes.html#vs",
    "href": "workflow-pipes.html#vs",
    "title": "5  Workflow: Pipes",
    "section": "\n5.3 |> vs %>%\n",
    "text": "5.3 |> vs %>%\n\nWhile |> and %>% behave identically for simple cases, there are a few important differences. These are most likely to affect you if you’re a long-term user of %>% who has taken advantage of some of the more advanced features. But they’re still good to know about even if you’ve never used %>% because you’re likely to encounter some of them when reading wild-caught code.\n\nBy default, the pipe passes the object on its left hand side to the first argument of the function on the right-hand side. %>% allows you change the placement with a . placeholder. For example, x %>% f(1) is equivalent to f(x, 1) but x %>% f(1, .) is equivalent to f(1, x). R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named. For example, x |> f(1, y = _) is equivalent to f(1, y = x).\n\nThe |> placeholder is deliberately simple and can’t replicate many features of the %>% placeholder: you can’t pass it to multiple arguments, and it doesn’t have any special behavior when the placeholder is used inside another function. For example, df %>% split(.$var) is equivalent to split(df, df$var) and df %>% {split(.$x, .$y)} is equivalent to split(df$x, df$y).\nWith %>% you can use . on the left-hand side of operators like $, [[, [ (which you’ll learn about in Chapter 28), so you can extract a single column from a data frame with (e.g.) mtcars %>% .$cyl. A future version of R may add similar support for |> and _. For the special case of extracting a column out of a data frame, you can also use dplyr::pull():\n\nmtcars |> pull(cyl)\n#>  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n\n\n%>% allows you to drop the parentheses when calling a function with no other arguments; |> always requires the parentheses.\n%>% allows you to start a pipe with . to create a function rather than immediately executing the pipe; this is not supported by the base pipe.\n\nLuckily there’s no need to commit entirely to one pipe or the other — you can use the base pipe for the majority of cases where it’s sufficient, and use the magrittr pipe when you really need its special features."
  },
  {
    "objectID": "data-tidy.html",
    "href": "data-tidy.html",
    "title": "6  Data tidying",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is largely complete and just needs final proof reading. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "data-tidy.html#introduction",
    "href": "data-tidy.html#introduction",
    "title": "6  Data tidying",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.”\n— Leo Tolstoy\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham\n\nIn this chapter, you will learn a consistent way to organize your data in R using a system called tidy data. Getting your data into this format requires some work up front, but that work pays off in the long term. Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the data questions you care about.\nIn this chapter, you’ll first learn the definition of tidy data and see it applied to simple toy dataset. Then we’ll dive into the main tool you’ll use for tidying data: pivoting. Pivoting allows you to change the form of your data, without changing any of the values. We’ll finish up with a discussion of usefully untidy data, and how you can create it if needed. If you particularly enjoy this chapter and want to learn more about the underlying theory, you can learn more about the history and theoretical underpinnings in the Tidy Data paper published in the Journal of Statistical Software.\n\n6.1.1 Prerequisites\nIn this chapter we’ll focus on tidyr, a package that provides a bunch of tools to help tidy up your messy datasets. tidyr is a member of the core tidyverse.\n\nlibrary(tidyverse)\n\nFrom this chapter on, we’ll suppress the loading message from library(tidyverse)."
  },
  {
    "objectID": "data-tidy.html#sec-tidy-data",
    "href": "data-tidy.html#sec-tidy-data",
    "title": "6  Data tidying",
    "section": "\n6.2 Tidy data",
    "text": "6.2 Tidy data\nYou can represent the same underlying data in multiple ways. The example below shows the same data organised in four different ways. Each dataset shows the same values of four variables: country, year, population, and cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\ntable1\n#> # A tibble: 6 × 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\ntable2\n#> # A tibble: 12 × 4\n#>   country      year type           count\n#>   <chr>       <int> <chr>          <int>\n#> 1 Afghanistan  1999 cases            745\n#> 2 Afghanistan  1999 population  19987071\n#> 3 Afghanistan  2000 cases           2666\n#> 4 Afghanistan  2000 population  20595360\n#> 5 Brazil       1999 cases          37737\n#> 6 Brazil       1999 population 172006362\n#> # … with 6 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\ntable3\n#> # A tibble: 6 × 3\n#>   country      year rate             \n#> * <chr>       <int> <chr>            \n#> 1 Afghanistan  1999 745/19987071     \n#> 2 Afghanistan  2000 2666/20595360    \n#> 3 Brazil       1999 37737/172006362  \n#> 4 Brazil       2000 80488/174504898  \n#> 5 China        1999 212258/1272915272\n#> 6 China        2000 213766/1280428583\n\n# Spread across two tibbles\ntable4a # cases\n#> # A tibble: 3 × 3\n#>   country     `1999` `2000`\n#> * <chr>        <int>  <int>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\ntable4b # population\n#> # A tibble: 3 × 3\n#>   country         `1999`     `2000`\n#> * <chr>            <int>      <int>\n#> 1 Afghanistan   19987071   20595360\n#> 2 Brazil       172006362  174504898\n#> 3 China       1272915272 1280428583\n\nThese are all representations of the same underlying data, but they are not equally easy to use. One of them, table1, will be much easier to work with inside the tidyverse because it’s tidy.\nThere are three interrelated rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFigure 6.1 shows the rules visually.\n\n\n\n\nFigure 6.1: The following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells.\n\n\n\n\nWhy ensure that your data is tidy? There are two main advantages:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. As you learned in Sections @ref(mutate) and @ref(summarize), most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural.\n\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data. Here are a couple of small examples showing how you might work with table1.\n\n# Compute rate per 10,000\ntable1 |>\n  mutate(\n    rate = cases / population * 10000\n  )\n#> # A tibble: 6 × 5\n#>   country      year  cases population  rate\n#>   <chr>       <int>  <int>      <int> <dbl>\n#> 1 Afghanistan  1999    745   19987071 0.373\n#> 2 Afghanistan  2000   2666   20595360 1.29 \n#> 3 Brazil       1999  37737  172006362 2.19 \n#> 4 Brazil       2000  80488  174504898 4.61 \n#> 5 China        1999 212258 1272915272 1.67 \n#> 6 China        2000 213766 1280428583 1.67\n\n# Compute cases per year\ntable1 |>\n  count(year, wt = cases)\n#> # A tibble: 2 × 2\n#>    year      n\n#>   <int>  <int>\n#> 1  1999 250740\n#> 2  2000 296920\n\n# Visualise changes over time\nggplot(table1, aes(year, cases)) +\n  geom_line(aes(group = country), color = \"grey50\") +\n  geom_point(aes(color = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000))\n\n\n\n\n\n6.2.1 Exercises\n\nUsing prose, describe how the variables and observations are organised in each of the sample tables.\n\nSketch out the process you’d use to calculate the rate for table2 and table4a + table4b. You will need to perform four operations:\n\nExtract the number of TB cases per country per year.\nExtract the matching population per country per year.\nDivide cases by population, and multiply by 10000.\nStore back in the appropriate place.\n\nYou haven’t yet learned all the functions you’d need to actually perform these operations, but you should still be able to think through the transformations you’d need.\n\nRecreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first?"
  },
  {
    "objectID": "data-tidy.html#pivoting",
    "href": "data-tidy.html#pivoting",
    "title": "6  Data tidying",
    "section": "\n6.3 Pivoting",
    "text": "6.3 Pivoting\nThe principles of tidy data might seem so obvious that you wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most real data is untidy. There are two main reasons:\n\nData is often organised to facilitate some goal other than analysis. For example, it’s common for data to be structured to make data entry, not analysis, easy.\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\n\nThis means that most real analyses will require at least a little tidying. You’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\ntidyr provides two functions for pivoting data: pivot_longer(), which makes datasets longer by increasing rows and reducing columns, and pivot_wider() which makes datasets wider by increasing columns and reducing rows. The following sections work through the use of pivot_longer() and pivot_wider() to tackle a wide range of realistic datasets. These examples are drawn from vignette(\"pivot\", package = \"tidyr\"), which you should check out if you want to see more variations and more challenging problems.\nLet’s dive in.\n\n6.3.1 Data in column names\nThe billboard dataset records the billboard rank of songs in the year 2000:\n\nbillboard\n#> # A tibble: 317 × 79\n#>   artist  track date.ent…¹   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8   wk9\n#>   <chr>   <chr> <date>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 2 Pac   Baby… 2000-02-26    87    82    72    77    87    94    99    NA    NA\n#> 2 2Ge+her The … 2000-09-02    91    87    92    NA    NA    NA    NA    NA    NA\n#> 3 3 Door… Kryp… 2000-04-08    81    70    68    67    66    57    54    53    51\n#> 4 3 Door… Loser 2000-10-21    76    76    72    69    67    65    55    59    62\n#> 5 504 Bo… Wobb… 2000-04-15    57    34    25    17    17    31    36    49    53\n#> 6 98^0    Give… 2000-08-19    51    39    34    26    26    19     2     2     3\n#> # … with 311 more rows, 67 more variables: wk10 <dbl>, wk11 <dbl>, wk12 <dbl>,\n#> #   wk13 <dbl>, wk14 <dbl>, wk15 <dbl>, wk16 <dbl>, wk17 <dbl>, wk18 <dbl>,\n#> #   wk19 <dbl>, wk20 <dbl>, wk21 <dbl>, wk22 <dbl>, wk23 <dbl>, wk24 <dbl>,\n#> #   wk25 <dbl>, wk26 <dbl>, wk27 <dbl>, wk28 <dbl>, wk29 <dbl>, wk30 <dbl>,\n#> #   wk31 <dbl>, wk32 <dbl>, wk33 <dbl>, wk34 <dbl>, wk35 <dbl>, wk36 <dbl>,\n#> #   wk37 <dbl>, wk38 <dbl>, wk39 <dbl>, wk40 <dbl>, wk41 <dbl>, wk42 <dbl>,\n#> #   wk43 <dbl>, wk44 <dbl>, wk45 <dbl>, wk46 <dbl>, wk47 <dbl>, wk48 <dbl>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week. Here, the column names are one variable (the week) and the cell values are another (the rank).\nTo tidy this data, we’ll use pivot_longer(). After the data, there are three key arguments:\n\n\ncols specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as select() so here we could use !c(artist, track, date.entered) or starts_with(\"wk\").\n\nnames_to names of the variable stored in the column names, here \"week\".\n\nvalues_to names the variable stored in the cell values, here \"rank\".\n\nThat gives the following call:\n\nbillboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n#> # A tibble: 24,092 × 5\n#>    artist track                   date.entered week   rank\n#>    <chr>  <chr>                   <date>       <chr> <dbl>\n#>  1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#>  2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#>  3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#>  4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#>  5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#>  6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#>  7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n#>  8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n#>  9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n#> 10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n#> # … with 24,082 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWhat happens if a song is in the top 100 for less than 76 weeks? Take 2 Pac’s “Baby Don’t Cry”, for example. The above output suggests that it was only the top 100 for 7 weeks, and all the remaining weeks are filled in with missing values. These NAs don’t really represent unknown observations; they’re forced to exist by the structure of the dataset1, so we can ask pivot_longer to get rid of them by setting values_drop_na = TRUE:\n\nbillboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n#> # A tibble: 5,307 × 5\n#>   artist track                   date.entered week   rank\n#>   <chr>  <chr>                   <date>       <chr> <dbl>\n#> 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#> 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#> 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#> 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#> 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#> 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#> # … with 5,301 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou might also wonder what happens if a song is in the top 100 for more than 76 weeks? We can’t tell from this data, but you might guess that additional columns wk77, wk78, … would be added to the dataset.\nThis data is now tidy, but we could make future computation a bit easier by converting week into a number using mutate() and parse_number(). You’ll learn more about parse_number() and friends in Chapter 8.\n\nbillboard_tidy <- billboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |> \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_tidy\n#> # A tibble: 5,307 × 5\n#>   artist track                   date.entered  week  rank\n#>   <chr>  <chr>                   <date>       <dbl> <dbl>\n#> 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n#> 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n#> 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n#> 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n#> 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n#> 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n#> # … with 5,301 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nNow we’re in a good position to look at how song ranks vary over time by drawing a plot. The code is shown below and the result is Figure 6.2.\n\nbillboard_tidy |> \n  ggplot(aes(week, rank, group = track)) + \n  geom_line(alpha = 1/3) + \n  scale_y_reverse()\n\n\n\nFigure 6.2: A line plot showing how the rank of a song changes over time.\n\n\n\n\n\n6.3.2 How does pivoting work?\nNow that you’ve seen what pivoting can do for you, it’s worth taking a little time to gain some intuition about what it does to the data. Let’s start with a very simple dataset to make it easier to see what’s happening:\n\ndf <- tribble(\n  ~var, ~col1, ~col2,\n   \"A\",     1,     2,\n   \"B\",     3,     4,\n   \"C\",     5,     6\n)\n\nHere we’ll say there are three variables: var (already in a variable), name (the column names in the column names), and value (the cell values). So we can tidy it with:\n\ndf |> \n  pivot_longer(\n    cols = col1:col2,\n    names_to = \"names\",\n    values_to = \"values\"\n  )\n#> # A tibble: 6 × 3\n#>   var   names values\n#>   <chr> <chr>  <dbl>\n#> 1 A     col1       1\n#> 2 A     col2       2\n#> 3 B     col1       3\n#> 4 B     col2       4\n#> 5 C     col1       5\n#> 6 C     col2       6\n\nHow does this transformation take place? It’s easier to see if we take it component by component. Columns that are already variables need to be repeated, once for each column in cols, as shown in Figure 6.3.\n\n\n\n\nFigure 6.3: Columns that are already variables need to be repeated, once for each column that is pivotted.\n\n\n\n\nThe column names become values in a new variable, whose name is given by names_to, as shown in Figure 6.4. They need to be repeated once for each row in the original dataset.\n\n\n\n\nFigure 6.4: The column names of pivoted columns become a new column.\n\n\n\n\nThe cell values also become values in a new variable, with a name given by values_to. They are unwound row by row. Figure 6.5 illustrates the process.\n\n\n\n\nFigure 6.5: The number of values is preserved (not repeated), but unwound row-by-row.\n\n\n\n\n\n6.3.3 Many variables in column names\nA more challenging situation occurs when you have multiple variables crammed into the column names. For example, take the who2 dataset:\n\nwho2 <- who |> \n  rename_with(~ str_remove(.x, \"new_?\")) |> \n  rename_with(~ str_replace(.x, \"([mf])\", \"\\\\1_\")) |> \n  select(!starts_with(\"iso\"))\nwho2\n#> # A tibble: 7,240 × 58\n#>   country   year sp_m_…¹ sp_m_…² sp_m_…³ sp_m_…⁴ sp_m_…⁵ sp_m_…⁶ sp_m_65 sp_f_…⁷\n#>   <chr>    <int>   <int>   <int>   <int>   <int>   <int>   <int>   <int>   <int>\n#> 1 Afghani…  1980      NA      NA      NA      NA      NA      NA      NA      NA\n#> 2 Afghani…  1981      NA      NA      NA      NA      NA      NA      NA      NA\n#> 3 Afghani…  1982      NA      NA      NA      NA      NA      NA      NA      NA\n#> 4 Afghani…  1983      NA      NA      NA      NA      NA      NA      NA      NA\n#> 5 Afghani…  1984      NA      NA      NA      NA      NA      NA      NA      NA\n#> 6 Afghani…  1985      NA      NA      NA      NA      NA      NA      NA      NA\n#> # … with 7,234 more rows, 48 more variables: sp_f_1524 <int>, sp_f_2534 <int>,\n#> #   sp_f_3544 <int>, sp_f_4554 <int>, sp_f_5564 <int>, sp_f_65 <int>,\n#> #   sn_m_014 <int>, sn_m_1524 <int>, sn_m_2534 <int>, sn_m_3544 <int>,\n#> #   sn_m_4554 <int>, sn_m_5564 <int>, sn_m_65 <int>, sn_f_014 <int>,\n#> #   sn_f_1524 <int>, sn_f_2534 <int>, sn_f_3544 <int>, sn_f_4554 <int>,\n#> #   sn_f_5564 <int>, sn_f_65 <int>, ep_m_014 <int>, ep_m_1524 <int>,\n#> #   ep_m_2534 <int>, ep_m_3544 <int>, ep_m_4554 <int>, ep_m_5564 <int>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nfamily <- tribble(\n  ~family,  ~dob_child1,  ~dob_child2, ~name_child1, ~name_child2,\n        1, \"1998-11-26\", \"2000-01-29\",      \"Susan\",       \"Jose\",\n        2, \"1996-06-22\",           NA,       \"Mark\",           NA,\n        3, \"2002-07-11\", \"2004-04-05\",        \"Sam\",       \"Seth\",\n        4, \"2004-10-10\", \"2009-08-27\",      \"Craig\",       \"Khai\",\n        5, \"2000-12-05\", \"2005-02-28\",     \"Parker\",     \"Gracie\",\n)\nfamily <- family %>% \n  mutate(across(starts_with(\"dob\"), parse_date))\nfamily\n#> # A tibble: 5 × 5\n#>   family dob_child1 dob_child2 name_child1 name_child2\n#>    <dbl> <date>     <date>     <chr>       <chr>      \n#> 1      1 1998-11-26 2000-01-29 Susan       Jose       \n#> 2      2 1996-06-22 NA         Mark        <NA>       \n#> 3      3 2002-07-11 2004-04-05 Sam         Seth       \n#> 4      4 2004-10-10 2009-08-27 Craig       Khai       \n#> 5      5 2000-12-05 2005-02-28 Parker      Gracie\n\nwho2\n#> # A tibble: 7,240 × 58\n#>   country   year sp_m_…¹ sp_m_…² sp_m_…³ sp_m_…⁴ sp_m_…⁵ sp_m_…⁶ sp_m_65 sp_f_…⁷\n#>   <chr>    <int>   <int>   <int>   <int>   <int>   <int>   <int>   <int>   <int>\n#> 1 Afghani…  1980      NA      NA      NA      NA      NA      NA      NA      NA\n#> 2 Afghani…  1981      NA      NA      NA      NA      NA      NA      NA      NA\n#> 3 Afghani…  1982      NA      NA      NA      NA      NA      NA      NA      NA\n#> 4 Afghani…  1983      NA      NA      NA      NA      NA      NA      NA      NA\n#> 5 Afghani…  1984      NA      NA      NA      NA      NA      NA      NA      NA\n#> 6 Afghani…  1985      NA      NA      NA      NA      NA      NA      NA      NA\n#> # … with 7,234 more rows, 48 more variables: sp_f_1524 <int>, sp_f_2534 <int>,\n#> #   sp_f_3544 <int>, sp_f_4554 <int>, sp_f_5564 <int>, sp_f_65 <int>,\n#> #   sn_m_014 <int>, sn_m_1524 <int>, sn_m_2534 <int>, sn_m_3544 <int>,\n#> #   sn_m_4554 <int>, sn_m_5564 <int>, sn_m_65 <int>, sn_f_014 <int>,\n#> #   sn_f_1524 <int>, sn_f_2534 <int>, sn_f_3544 <int>, sn_f_4554 <int>,\n#> #   sn_f_5564 <int>, sn_f_65 <int>, ep_m_014 <int>, ep_m_1524 <int>,\n#> #   ep_m_2534 <int>, ep_m_3544 <int>, ep_m_4554 <int>, ep_m_5564 <int>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThis dataset records information about tuberculosis data collected by the WHO. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like sp_m_014, ep_m_4554, and rel_m_3544. If you stare at these columns for long enough, you’ll notice there’s a pattern. Each column name is made up of three pieces separated by _. The first piece, sp/rel/ep, describes the method used for the diagnosis, the second piece, m/f is the gender, and the third piece, 014/1524/2535/3544/4554/65 is the age range.\nSo in this case we have six variables: two variables are already columns, three variables are contained in the column name, and one variable is in the cell name. This requires two changes to our call to pivot_longer(): names_to gets a vector of column names and names_sep describes how to split the variable name up into pieces:\n\nwho2 |> \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  )\n#> # A tibble: 405,440 × 6\n#>   country      year diagnosis gender age   count\n#>   <chr>       <int> <chr>     <chr>  <chr> <int>\n#> 1 Afghanistan  1980 sp        m      014      NA\n#> 2 Afghanistan  1980 sp        m      1524     NA\n#> 3 Afghanistan  1980 sp        m      2534     NA\n#> 4 Afghanistan  1980 sp        m      3544     NA\n#> 5 Afghanistan  1980 sp        m      4554     NA\n#> 6 Afghanistan  1980 sp        m      5564     NA\n#> # … with 405,434 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAn alternative to names_sep is names_pattern, which you can use to extract variables from more complicated naming scenarios, once you’ve learned about regular expressions in Chapter 17.\nConceptually, this is only a minor variation on the simpler case you’ve already seen. Figure 6.6 shows the basic idea: now, instead of the column names pivoting into a single column, they pivot into multiple columns. You can imagine this happening in two steps (first pivoting and then separating) but under the hood it happens in a single step because that gives better performance.\n\n\n\n\nFigure 6.6: Pivotting with many variables in the column names means that each column name now fills in values in multiple output columns.\n\n\n\n\n\n6.3.4 Data and variable names in the column headers\nThe next step up in complexity is when the column names include a mix of variable values and variable names. For example, take the household dataset:\n\n# Can't find the household dataset online, sorry\n#household\n\nThis dataset contains data about five families, with the names and dates of birth of up to two children. The new challenge in this dataset is that the column names contain the names of two variables (dob, name) and the values of another (child, with values 1 and 2). To solve this problem we again need to supply a vector to names_to but this time we use the special \".value\" sentinel. This overrides the usual values_to argument to use the first component of the pivoted column name as a variable name in the output.\n\n# household |> \n#   pivot_longer(\n#     cols = !family, \n#     names_to = c(\".value\", \"child\"), \n#     names_sep = \"_\", \n#     values_drop_na = TRUE\n#   ) |> \n#   mutate(\n#     child = parse_number(child)\n#   )\n\nWe again use values_drop_na = TRUE, since the shape of the input forces the creation of explicit missing variables (e.g. for families with only one child), and parse_number() to convert (e.g.) child1 into 1.\nFigure 6.7 illustrates the basic idea with a simpler example. When you use \".value\" in names_to, the column names in the input contribute to both values and variable names in the output.\n\n\n\n\nFigure 6.7: Pivoting with names_to = c(\".value\", \"id\") splits the column names into two components: the first part determines the output column name (x or y), and the second part determines the value of the id column.\n\n\n\n\n\n6.3.5 Widening data\nSo far we’ve used pivot_longer() to solve the common class of problems where values have ended up in column names. Next we’ll pivot (HA HA) to pivot_wider(), which helps when one observation is spread across multiple rows. This seems to arise less commonly in the wild, but it does seem to crop up a lot when dealing with governmental data.\nWe’ll start by looking at cms_patient_experience, a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\n#cms_patient_experience <- read.csv(\"Hospice_Provider_May2022\")\n#cms_patient_experience\n\nAn observation is an organisation, but each organisation is spread across six rows, with one row for each variable, or measure. We can see the complete set of values for measure_cd and measure_title by using distinct():\n\n# cms_patient_experience |> \n#   distinct(measure_cd, measure_title)\n\nNeither of these columns will make particularly great variable names: measure_cd doesn’t hint at the meaning of the variable and measure_title is a long sentence containing spaces. We’ll use measure_cd for now, but in a real analysis you might want to create your own variable names that are both short and meaningful.\npivot_wider() has the opposite interface to pivot_longer(): we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\n# cms_patient_experience |> \n#   pivot_wider(\n#     names_from = measure_cd,\n#     values_from = prf_rate\n#   )\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, by default, pivot_wider() will attempt to preserve all the existing columns including measure_title which has six distinct observations for each organisations. To fix this problem we need to tell pivot_wider() which columns identify each row; in this case those are the variables starting with org:\n\n# cms_patient_experience |> \n#   pivot_wider(\n#     id_cols = starts_with(\"org\"),\n#     names_from = measure_cd,\n#     values_from = prf_rate\n#   )\n\nThis gives us the output that we’re looking for.\n\n6.3.6 How does pivot_wider() work?\nTo understand how pivot_wider() works, let’s again start with a very simple dataset:\n\ndf <- tribble(\n  ~id, ~name, ~value,\n  \"A\", \"x\", 1,\n  \"B\", \"y\", 2,\n  \"B\", \"x\", 3, \n  \"A\", \"y\", 4,\n  \"A\", \"z\", 5,\n)\n\nWe’ll take the values from the value column and the names from the name column:\n\ndf |> \n  pivot_wider(\n    names_from = name,\n    values_from = value\n  )\n#> # A tibble: 2 × 4\n#>   id        x     y     z\n#>   <chr> <dbl> <dbl> <dbl>\n#> 1 A         1     4     5\n#> 2 B         3     2    NA\n\nThe connection between the position of the row in the input and the cell in the output is weaker than in pivot_longer() because the rows and columns in the output are primarily determined by the values of variables, not their locations.\nTo begin the process pivot_wider() needs to first figure out what will go in the rows and columns. Finding the column names is easy: it’s just the values of name.\n\ndf |> \n  distinct(name)\n#> # A tibble: 3 × 1\n#>   name \n#>   <chr>\n#> 1 x    \n#> 2 y    \n#> 3 z\n\nBy default, the rows in the output are formed by all the variables that aren’t going into the names or values. These are called the id_cols.\n\ndf |> \n  select(-name, -value) |> \n  distinct()\n#> # A tibble: 2 × 1\n#>   id   \n#>   <chr>\n#> 1 A    \n#> 2 B\n\npivot_wider() then combines these results to generate an empty data frame:\n\ndf |> \n  select(-name, -value) |> \n  distinct() |> \n  mutate(x = NA, y = NA, z = NA)\n#> # A tibble: 2 × 4\n#>   id    x     y     z    \n#>   <chr> <lgl> <lgl> <lgl>\n#> 1 A     NA    NA    NA   \n#> 2 B     NA    NA    NA\n\nIt then fills in all the missing values using the data in the input. In this case, not every cell in the output has corresponding value in the input as there’s no entry for id “B” and name “z”, so that cell remains missing. We’ll come back to this idea that pivot_wider() can “make” missing values in Chapter 20.\nYou might also wonder what happens if there are multiple rows in the input that correspond to one cell in the output. The example below has two rows that correspond to id “A” and name “x”:\n\ndf <- tribble(\n  ~id, ~name, ~value,\n  \"A\", \"x\", 1,\n  \"A\", \"x\", 2,\n  \"A\", \"y\", 3,\n  \"B\", \"x\", 4, \n  \"B\", \"y\", 5, \n)\n\nIf we attempt to pivot this we get an output that contains list-columns, which you’ll learn more about in Chapter 25:\n\ndf |> pivot_wider(\n  names_from = name,\n  values_from = value\n)\n#> Warning: Values from `value` are not uniquely identified; output will contain list-cols.\n#> * Use `values_fn = list` to suppress this warning.\n#> * Use `values_fn = {summary_fun}` to summarise duplicates.\n#> * Use the following dplyr code to identify duplicates.\n#>   {data} %>%\n#>     dplyr::group_by(id, name) %>%\n#>     dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n#>     dplyr::filter(n > 1L)\n#> # A tibble: 2 × 3\n#>   id    x         y        \n#>   <chr> <list>    <list>   \n#> 1 A     <dbl [2]> <dbl [1]>\n#> 2 B     <dbl [1]> <dbl [1]>\n\nSince you don’t know how to work with this sort of data yet, you’ll want to follow the hint in the warning to figure out where the problem is:\n\ndf %>%\n  group_by(id, name) %>%\n  summarize(n = n(), .groups = \"drop\") %>%\n  filter(n > 1L) \n#> # A tibble: 1 × 3\n#>   id    name      n\n#>   <chr> <chr> <int>\n#> 1 A     x         2\n\nIt’s then up to you to figure out what’s gone wrong with your data and either repair the underlying damage or use your grouping and summarizing skills to ensure that each combination of row and column values only has a single row."
  },
  {
    "objectID": "data-tidy.html#untidy-data",
    "href": "data-tidy.html#untidy-data",
    "title": "6  Data tidying",
    "section": "\n6.4 Untidy data",
    "text": "6.4 Untidy data\nWhile pivot_wider() is occasionally useful for making tidy data, its real strength is making untidy data. While that sounds like a bad thing, untidy isn’t a pejorative term: there are many untidy data structures that are extremely useful. Tidy data is a great starting point for most analyses but it’s not the only data format you’ll ever need.\nThe following sections will show a few examples of pivot_wider() making usefully untidy data for presenting data to other humans, for input to multivariate statistics algorithms, and for pragmatically solving data manipulation challenges.\n\n6.4.1 Presenting data to humans\nAs you’ve seen, dplyr::count() produces tidy data: it makes one row for each group, with one column for each grouping variable, and one column for the number of observations.\n\ndiamonds |> \n  count(clarity, color)\n#> # A tibble: 56 × 3\n#>   clarity color     n\n#>   <ord>   <ord> <int>\n#> 1 I1      D        42\n#> 2 I1      E       102\n#> 3 I1      F       143\n#> 4 I1      G       150\n#> 5 I1      H       162\n#> 6 I1      I        92\n#> # … with 50 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThis is easy to visualize or summarize further, but it’s not the most compact form for display. You can use pivot_wider() to create a form more suitable for display to other humans:\n\ndiamonds |> \n  count(clarity, color) |> \n  pivot_wider(\n    names_from = color, \n    values_from = n\n  )\n#> # A tibble: 8 × 8\n#>   clarity     D     E     F     G     H     I     J\n#>   <ord>   <int> <int> <int> <int> <int> <int> <int>\n#> 1 I1         42   102   143   150   162    92    50\n#> 2 SI2      1370  1713  1609  1548  1563   912   479\n#> 3 SI1      2083  2426  2131  1976  2275  1424   750\n#> 4 VS2      1697  2470  2201  2347  1643  1169   731\n#> 5 VS1       705  1281  1364  2148  1169   962   542\n#> 6 VVS2      553   991   975  1443   608   365   131\n#> # … with 2 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThis display also makes it easy to compare in two directions, horizontally and vertically, much like facet_grid().\npivot_wider() can be great for quickly sketching out a table. But for real presentation tables, we highly suggest learning a package like gt. gt is similar to ggplot2 in that it provides an extremely powerful grammar for laying out tables. It takes some work to learn but the payoff is the ability to make just about any table you can imagine.\n\n6.4.2 Multivariate statistics\nMost classical multivariate statistical methods (like dimension reduction and clustering) require your data in matrix form, where each column is a time point, or a location, or a gene, or a species, but definitely not a variable. Sometimes these formats have substantial performance or space advantages, or sometimes they’re just necessary to get closer to the underlying matrix mathematics.\nWe’re not going to cover these statistical methods here, but it is useful to know how to get your data into the form that they need. For example, let’s imagine you wanted to cluster the gapminder data to find countries that had similar progression of gdpPercap over time. To do this, we need one row for each country and one column for each year:\n\nlibrary(gapminder)\n\ncol_year <- gapminder |> \n  mutate(gdpPercap = log10(gdpPercap)) |> \n  pivot_wider(\n    id_cols = country, \n    names_from = year,\n    values_from = gdpPercap\n  ) \ncol_year\n#> # A tibble: 142 × 13\n#>   country  `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n#>   <fct>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n#> 1 Afghani…   2.89   2.91   2.93   2.92   2.87   2.90   2.99   2.93   2.81   2.80\n#> 2 Albania    3.20   3.29   3.36   3.44   3.52   3.55   3.56   3.57   3.40   3.50\n#> 3 Algeria    3.39   3.48   3.41   3.51   3.62   3.69   3.76   3.75   3.70   3.68\n#> 4 Angola     3.55   3.58   3.63   3.74   3.74   3.48   3.44   3.39   3.42   3.36\n#> 5 Argenti…   3.77   3.84   3.85   3.91   3.98   4.00   3.95   3.96   3.97   4.04\n#> 6 Austral…   4.00   4.04   4.09   4.16   4.23   4.26   4.29   4.34   4.37   4.43\n#> # … with 136 more rows, and 2 more variables: `2002` <dbl>, `2007` <dbl>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\npivot_wider() produces a tibble where each row is labelled by the country variable. But most classic statistical algorithms don’t want the identifier as an explicit variable; they want as a row name. We can turn the country variable into row names with column_to_rowname():\n\ncol_year <- col_year |> \n  column_to_rownames(\"country\") \n\nhead(col_year)\n#>                 1952     1957     1962     1967     1972     1977     1982\n#> Afghanistan 2.891786 2.914265 2.931000 2.922309 2.869221 2.895485 2.990344\n#> Albania     3.204407 3.288313 3.364155 3.440940 3.520277 3.548144 3.560012\n#> Algeria     3.388990 3.479140 3.406679 3.511481 3.621453 3.691118 3.759302\n#> Angola      3.546618 3.582965 3.630354 3.742157 3.738248 3.478371 3.440429\n#> Argentina   3.771684 3.836125 3.853282 3.905955 3.975112 4.003419 3.954141\n#> Australia   4.001716 4.039400 4.086973 4.162150 4.225015 4.263262 4.289522\n#>                 1987     1992     1997     2002     2007\n#> Afghanistan 2.930641 2.812473 2.803007 2.861376 2.988818\n#> Albania     3.572748 3.397495 3.504206 3.663155 3.773569\n#> Algeria     3.754452 3.700982 3.680996 3.723295 3.794025\n#> Angola      3.385644 3.419600 3.357390 3.442995 3.680991\n#> Argentina   3.960931 3.968876 4.040099 3.944366 4.106510\n#> Australia   4.340224 4.369675 4.431331 4.486965 4.537005\n\nThis makes a data frame, because tibbles don’t support row names2.\nWe’re now ready to cluster with (e.g.) kmeans():\n\ncluster <- stats::kmeans(col_year, centers = 6)\n\nExtracting the data out of this object into a form you can work with is a challenge you’ll need to come back to later in the book, once you’ve learned more about lists. But for now, you can get the clustering membership out with this code:\n\ncluster_id <- cluster$cluster |> \n  enframe() |> \n  rename(country = name, cluster_id = value)\ncluster_id\n#> # A tibble: 142 × 2\n#>   country     cluster_id\n#>   <chr>            <int>\n#> 1 Afghanistan          4\n#> 2 Albania              2\n#> 3 Algeria              6\n#> 4 Angola               2\n#> 5 Argentina            5\n#> 6 Australia            1\n#> # … with 136 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou could then combine this back with the original data using one of the joins you’ll learn about in Chapter 13.\n\ngapminder |> left_join(cluster_id)\n#> Joining, by = \"country\"\n#> # A tibble: 1,704 × 7\n#>   country     continent  year lifeExp      pop gdpPercap cluster_id\n#>   <chr>       <fct>     <int>   <dbl>    <int>     <dbl>      <int>\n#> 1 Afghanistan Asia       1952    28.8  8425333      779.          4\n#> 2 Afghanistan Asia       1957    30.3  9240934      821.          4\n#> 3 Afghanistan Asia       1962    32.0 10267083      853.          4\n#> 4 Afghanistan Asia       1967    34.0 11537966      836.          4\n#> 5 Afghanistan Asia       1972    36.1 13079460      740.          4\n#> 6 Afghanistan Asia       1977    38.4 14880372      786.          4\n#> # … with 1,698 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n6.4.3 Pragmatic computation\nSometimes it’s just easier to answer a question using untidy data. For example, if you’re interested in just the total number of missing values in cms_patient_experience, it’s easier to work with the untidy form:\n\n# cms_patient_experience |>\n#   group_by(org_pac_id) |>\n#   summarize(\n#     n_miss = sum(is.na(prf_rate)),\n#     n = n(),\n#   )\n\nThis is partly a reflection of our definition of tidy data, where we said tidy data has one variable in each column, but we didn’t actually define what a variable is (and it’s surprisingly hard to do so). It’s totally fine to be pragmatic and to say a variable is whatever makes your analysis easiest.\nSo if you’re stuck figuring out how to do some computation, maybe it’s time to switch up the organisation of your data. For computations involving a fixed number of values (like computing differences or ratios), it’s usually easier if the data is in columns; for those with a variable number of values (like sums or means) it’s usually easier in rows. Don’t be afraid to untidy, transform, and re-tidy if needed.\nLet’s explore this idea by looking at cms_patient_care, which has a similar structure to cms_patient_experience:\n\n#cms_patient_care\n\nIt contains information about 9 measures (beliefs_addressed, composite_process, dyspena_treatment, …) on 14 different facilities (identified by ccn with a name given by facility_name). Compared to cms_patient_experience, however, each measurement is recorded in two rows with a score, the percentage of patients who answered yes to the survey question, and a denominator, the number of patients that the question applies to. Depending on what you want to do next, you may find any of the following three structures useful:\n\n\nIf you want to compute the number of patients that answered yes to the question, you may pivot type into the columns:\n\n# cms_patient_care |> \n#   pivot_wider(\n#     names_from = type,\n#     values_from = score\n#   ) |> \n#   mutate(\n#     numerator = round(observed / 100 * denominator)\n#   )\n\n\n\nIf you want to display the distribution of each metric, you may keep it as is so you could facet by measure_abbr.\n\n# cms_patient_care |> \n#   filter(type == \"observed\") |> \n#   ggplot(aes(score)) + \n#   geom_histogram(binwidth = 2) + \n#   facet_wrap(vars(measure_abbr))\n\n\n\nIf you want to explore how different metrics are related, you may put the measure names in the columns so you could compare them in scatterplots.\n\n# cms_patient_care |> \n#   filter(type == \"observed\") |> \n#   select(-type) |> \n#   pivot_wider(\n#     names_from = measure_abbr,\n#     values_from = score\n#   ) |> \n#   ggplot(aes(dyspnea_screening, dyspena_treatment)) + \n#   geom_point() + \n#   coord_equal()"
  },
  {
    "objectID": "workflow-style.html",
    "href": "workflow-style.html",
    "title": "7  Workflow: code style",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz.\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. Even as a very new programmer it’s a good idea to work on your code style. Use a consistent style makes it easier for others (including future-you!) to read your work, and is particularly important if you need to get help from someone else. This chapter will introduce to the most important points of the tidyverse style guide, which is used throughout this book.\nStyling your code will feel a bit tedious to start with, but if you practice it, it will soon become second nature. Additionally, there are some great tools to quickly restyle existing code, like the styler package by Lorenz Walthert. Once you’ve installed it with install.packages(\"styler\"), an easy way to use it is via RStudio’s command palette. The command palette lets you use any build-in RStudio command, as well as many addins provided by packages. Open the palette by pressing Cmd/Ctrl + Shift + P, then type “styler” to see all the shortcuts provided by styler. Figure 7.1 shows the results."
  },
  {
    "objectID": "workflow-style.html#names",
    "href": "workflow-style.html#names",
    "title": "7  Workflow: code style",
    "section": "\n7.1 Names",
    "text": "7.1 Names\nWe talked briefly about names in Section 3.3. Remember that variable names (those created by <- and those created by mutate()) should use only lowercase letters, numbers, and _. Use _ to separate words within a name.\n\n# Strive for:\nshort_flights <- flights |> filter(air_time < 60)\n\n# Avoid:\nSHORTFLIGHTS  <- flights |> filter(air_time < 60)\n\nAs a general rule of thumb, it’s better to prefer long, descriptive names that are easy to understand, rather than concise names that are fast to type. Short names save relatively little time when writing code (especially since autocomplete will help you finish typing them), but can be time-consuming when you come back to old code and are forced to puzzle out a cryptic abbreviation.\nIf you have a bunch of names for related things, do your best to be consistent. It’s easy for inconsistencies to arise when you forget a previous convention, so don’t feel bad if you have to go back and rename things. In general, if you have a bunch of variables that are a variation on a theme you’re better off giving them a common prefix, rather than a common suffix, because autocomplete works best on the start of a variable."
  },
  {
    "objectID": "workflow-style.html#spaces",
    "href": "workflow-style.html#spaces",
    "title": "7  Workflow: code style",
    "section": "\n7.2 Spaces",
    "text": "7.2 Spaces\nPut spaces on either side of mathematical operators apart from ^ (i.e., +, -, ==, <, …), and around the assignment operator (<-).\n\n# Strive for\nz <- (a + b)^2 / d\n\n# Avoid\nz<-( a + b ) ^ 2/d\n\nDon’t put spaces inside or outside parentheses for regular function calls. Always put a space after a comma, just like in regular English.\n\n# Strive for\nmean(x, na.rm = TRUE)\n\n# Avoid\nmean (x ,na.rm=TRUE)\n\nIt’s OK to add extra spaces if it improves alignment. For example, if you’re creating multiple variables in mutate(), you might want to add spaces so that all the = line up. This makes it easier to skim the code.\n\nflights |> \n  mutate(\n    speed      = air_time / distance,\n    dep_hour   = dep_time %/% 100,\n    dep_minute = dep_time %%  100\n  )"
  },
  {
    "objectID": "workflow-style.html#sec-pipes",
    "href": "workflow-style.html#sec-pipes",
    "title": "7  Workflow: code style",
    "section": "\n7.3 Pipes",
    "text": "7.3 Pipes\n|> should always have a space before it and should typically be the last thing on a line. This makes makes it easier to add new steps, rearrange existing steps, modify elements within a step, and to get a 50,000 ft view by skimming the verbs on the left-hand side.\n\n# Strive for \nflights |>  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |> \n  count(dest)\n\n# Avoid\nflights|>filter(!is.na(arr_delay), !is.na(tailnum))|>count(dest)\n\nIf the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line. If the function doesn’t have named arguments (like select() or filter()) keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.\n\n# Strive for\nflights |>  \n  group_by(tailnum) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights |>\n  group_by(\n    tailnum\n  ) |> \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\n\nAfter the first step of the pipeline, indent each line by two spaces. If you’re putting each argument on its own line, indent by an extra two spaces. Make sure ) is on its own line, and un-indented to match the horizontal position of the function name.\n\n# Strive for \nflights |>  \n  group_by(tailnum) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights|>\n  group_by(tailnum) |> \n  summarize(\n             delay = mean(arr_delay, na.rm = TRUE), \n             n = n()\n           )\n\nflights|>\n  group_by(tailnum) |> \n  summarize(\n  delay = mean(arr_delay, na.rm = TRUE), \n  n = n()\n  )\n\nIt’s OK to shirk some of these rules if your pipeline fits easily on one line. But in our collective experience, it’s common for short snippets to grow longer, so you’ll usually save time in the long run by starting with all the vertical space you need.\n\n# This fits compactly on one line\ndf |> mutate(y = x + 1)\n\n# While this takes up 4x as many lines, it's easily extended to \n# more variables and more steps in the future\ndf |> \n  mutate(\n    y = x + 1\n  )\n\nFinally, be wary of writing very long pipes, say longer than 10-15 lines. Try to break them up into smaller sub-tasks, giving each task an informative name. The names will help cue the reader into what’s happening and makes it easier to check that intermediate results are as expected. Whenever you can give something an informative name, you should give it an informative name. Don’t expect to get it right the first time! This means breaking up long pipelines if there are intermediate states that can get good names."
  },
  {
    "objectID": "workflow-style.html#ggplot2",
    "href": "workflow-style.html#ggplot2",
    "title": "7  Workflow: code style",
    "section": "\n7.4 ggplot2",
    "text": "7.4 ggplot2\nThe same basic rules that apply to the pipe also apply to ggplot2; just treat + the same way as |>.\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |> \n  ggplot(aes(month, delay)) +\n  geom_point() + \n  geom_line()\n\nAgain, if you can fit all of the arguments to a function on to a single line, put each argument on its own line:\n\nflights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(air_time / distance, na.rm = TRUE)\n  ) |> \n  ggplot(aes(distance, speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    size = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "workflow-style.html#organization",
    "href": "workflow-style.html#organization",
    "title": "7  Workflow: code style",
    "section": "\n7.5 Organization",
    "text": "7.5 Organization\nUse comments to explain the “why” of your code, not the “how” or the “what”. If you simply describe what your code is doing in prose, you’ll have to be careful to update the comment and code in tandem: if you change the code and forget to update the comment, they’ll be inconsistent which will lead to confusion when you come back to your code in the future. For data analysis code, use comments to explain your overall plan of attack and record important insight as you encounter them. There’s no way to re-capture this knowledge from the code itself.\nAs your scripts get longer, use sectioning comments to break up your file into manageable pieces:\n\n# Load data --------------------------------------\n\n# Plot data --------------------------------------\n\nRStudio provides a keyboard shortcut to create these headers (Cmd/Ctrl + Shift + R), and will display them in the code navigation drop-down at the bottom-left of the editor, as shown in Figure 7.2.\n\n\n\n\nFigure 7.2: After adding sectioning comments to your script, you can easily navigate to them using the code navigation tool in the bottom-left of the script editor."
  },
  {
    "objectID": "workflow-style.html#exercises",
    "href": "workflow-style.html#exercises",
    "title": "7  Workflow: code style",
    "section": "\n7.6 Exercises",
    "text": "7.6 Exercises\n\n\nRestyle the following pipelines following the guidelines above.\n\nflights|>filter(dest==\"IAH\")|>group_by(year,month,day)|>summarize(n=n(),delay=mean(arr_delay,na.rm=TRUE))|>filter(n>10)\n\nflights|>filter(carrier==\"UA\",dest%in%c(\"IAH\",\"HOU\"),sched_dep_time>0900,sched_arr_time<2000)|>group_by(flight)|>summarize(delay=mean(arr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())|>filter(n>10)"
  },
  {
    "objectID": "data-import.html",
    "href": "data-import.html",
    "title": "8  Data import",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "data-import.html#introduction",
    "href": "data-import.html#introduction",
    "title": "8  Data import",
    "section": "\n8.1 Introduction",
    "text": "8.1 Introduction\nWorking with data provided by R packages is a great way to learn the tools of data science, but at some point you want to stop learning and start working with your own data. In this chapter, you’ll learn how to read plain-text rectangular files into R. Here, we’ll only scratch the surface of data import, but many of the principles will translate to other forms of data, which we’ll come back to in ?sec-wrangle.\n\n8.1.1 Prerequisites\nIn this chapter, you’ll learn how to load flat files in R with the readr package, which is part of the core tidyverse.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "data-import.html#getting-started",
    "href": "data-import.html#getting-started",
    "title": "8  Data import",
    "section": "\n8.2 Getting started",
    "text": "8.2 Getting started\nMost of readr’s functions are concerned with turning flat files into data frames:\n\nread_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter.\nread_fwf() reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions(). read_table() reads a common variation of fixed width files where columns are separated by white space.\nread_log() reads Apache style log files. (But also check out webreadr which is built on top of read_log() and provides many more helpful tools.)\n\nThese functions all have similar syntax: once you’ve mastered one, you can use the others with ease. For the rest of this chapter we’ll focus on read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand read_csv(), you can easily apply your knowledge to all the other functions in readr."
  },
  {
    "objectID": "data-import.html#reading-data-from-a-file",
    "href": "data-import.html#reading-data-from-a-file",
    "title": "8  Data import",
    "section": "\n8.3 Reading data from a file",
    "text": "8.3 Reading data from a file\nHere is what a simple CSV file with a row for column names (also commonly referred to as the header row) and six rows of data looks like.\n\n#> Student ID,Full Name,favourite.food,mealPlan,AGE\n#> 1,Sunil Huffmann,Strawberry yoghurt,Lunch only,4\n#> 2,Barclay Lynn,French fries,Lunch only,5\n#> 3,Jayendra Lyne,N/A,Breakfast and lunch,7\n#> 4,Leon Rossini,Anchovies,Lunch only,\n#> 5,Chidiegwu Dunkel,Pizza,Breakfast and lunch,five\n#> 6,Güvenç Attila,Ice cream,Lunch only,6\n\nNote that the ,s separate the columns. Table 8.1 shows a representation of the same data as a table.\n\n\n\n\nTable 8.1: Data from the students.csv file as a table.\n\n\n\n\n\n\n\n\nStudent ID\nFull Name\nfavourite.food\nmealPlan\nAGE\n\n\n\n1\nSunil Huffmann\nStrawberry yoghurt\nLunch only\n4\n\n\n2\nBarclay Lynn\nFrench fries\nLunch only\n5\n\n\n3\nJayendra Lyne\nN/A\nBreakfast and lunch\n7\n\n\n4\nLeon Rossini\nAnchovies\nLunch only\nNA\n\n\n5\nChidiegwu Dunkel\nPizza\nBreakfast and lunch\nfive\n\n\n6\nGüvenç Attila\nIce cream\nLunch only\n6\n\n\n\n\n\n\nThe first argument to read_csv() is the most important: it’s the path to the file to read.\n\nstudents <- read_csv(\"data/students.csv\")\n#> Rows: 6 Columns: 5\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (4): Full Name, favourite.food, mealPlan, AGE\n#> dbl (1): Student ID\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nWhen you run read_csv() it prints out a message that tells you how many rows (excluding the header row) and columns the data has along with the delimiter used, and the column specifications (names of columns organized by the type of data the column contains). It also prints out some information about how to retrieve the full column specification as well as how to quiet this message. This message is an important part of readr, which we’ll come back to in Section 22.2 on parsing a file.\nYou can also supply an inline csv file. This is useful for experimenting with readr and for creating reproducible examples to share with others:\n\nread_csv(\"a,b,c\n1,2,3\n4,5,6\")\n#> # A tibble: 2 × 3\n#>       a     b     c\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n\nIn both cases read_csv() uses the first line of the data for the column names, which is a very common convention. There are two cases where you might want to tweak this behavior:\n\n\nSometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = \"#\" to drop all lines that start with (e.g.) #.\n\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\", skip = 2)\n#> # A tibble: 1 × 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\", comment = \"#\")\n#> # A tibble: 1 × 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n\n\n\nThe data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings, and instead label them sequentially from X1 to Xn:\n\nread_csv(\"1,2,3\\n4,5,6\", col_names = FALSE)\n#> # A tibble: 2 × 3\n#>      X1    X2    X3\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n\n(\"\\n\" is a convenient shortcut for adding a new line. You’ll learn more about it and other types of string escape in Chapter 16.)\nAlternatively you can pass col_names a character vector which will be used as the column names:\n\nread_csv(\"1,2,3\\n4,5,6\", col_names = c(\"x\", \"y\", \"z\"))\n#> # A tibble: 2 × 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n\n\n\nAnother option that commonly needs tweaking is na: this specifies the value (or values) that are used to represent missing values in your file:\n\nread_csv(\"a,b,c\\n1,2,.\", na = \".\")\n#> # A tibble: 1 × 3\n#>       a     b c    \n#>   <dbl> <dbl> <lgl>\n#> 1     1     2 NA\n\nThis is all you need to know to read ~75% of CSV files that you’ll encounter in practice. You can also easily adapt what you’ve learned to read tab separated files with read_tsv() and fixed width files with read_fwf(). To read in more challenging files, you’ll need to learn more about how readr parses each column, turning them into R vectors.\n\n8.3.1 First steps\nLet’s take another look at the students data. In the favourite.food column, there are a bunch of food items and then the character string N/A, which should have been an real NA that R will recognize as “not available”. This is something we can address using the na argument.\n\nstudents <- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\nstudents\n#> # A tibble: 6 × 5\n#>   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#>          <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2            2 Barclay Lynn     French fries       Lunch only          5    \n#> 3            3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4            4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis. For example, the column names in the students file we read in are formatted in non-standard ways. You might consider renaming them one by one with dplyr::rename() or you might use the janitor::clean_names() function turn them all into snake case at once.1 This function takes in a data frame and returns a data frame with variable names converted to snake case.\n\nlibrary(janitor)\nstudents |>\n  clean_names()\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nAnother common task after reading in data is to consider variable types. For example, meal_type is a categorical variable with a known set of possible values. In R, factors can be used to work with categorical variables. We can convert this variable to a factor using the factor() function. You’ll learn more about factors in Chapter 18.\n\nstudents <- students |>\n  clean_names() |>\n  mutate(meal_plan = factor(meal_plan))\n\nstudents\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <fct>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nNote that the values in the meal_type variable has stayed exactly the same, but the type of variable denoted underneath the variable name has changed from character (<chr>) to factor (<fct>).\nBefore you move on to analyzing these data, you’ll probably want to fix the age column as well: currently it’s a character variable because of the one observation that is typed out as five instead of a numeric 5. We discuss the details of fixing this issue in Chapter 23 in further detail.\n\n8.3.2 Compared to base R\nIf you’ve used R before, you might wonder why we’re not using read.csv(). There are a few good reasons to favor readr functions over the base equivalents:\n\nThey are typically much faster (~10x) than their base equivalents. Long running jobs have a progress bar, so you can see what’s happening. If you’re looking for raw speed, try data.table::fread(). It doesn’t fit quite so well into the tidyverse, but it can be quite a bit faster.\nThey produce tibbles, and they don’t use row names or munge the column names. These are common sources of frustration with the base R functions.\nThey are more reproducible. Base R functions inherit some behavior from your operating system and environment variables, so import code that works on your computer might not work on someone else’s.\n\n8.3.3 Exercises\n\nWhat function would you use to read a file where fields were separated with “|”?\nApart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?\nWhat are the most important arguments to read_fwf()?\n\nSometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like \" or '. By default, read_csv() assumes that the quoting character will be \". What argument to read_csv() do you need to specify to read the following text into a data frame?\n\n\"x,y\\n1,'a,b'\"\n\n\n\nIdentify what is wrong with each of the following inline CSV files. What happens when you run the code?\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\nread_csv(\"a,b\\n\\\"1\")\nread_csv(\"a,b\\n1,2\\na,b\")\nread_csv(\"a;b\\n1;3\")"
  },
  {
    "objectID": "data-import.html#reading-data-from-multiple-files",
    "href": "data-import.html#reading-data-from-multiple-files",
    "title": "8  Data import",
    "section": "\n8.4 Reading data from multiple files",
    "text": "8.4 Reading data from multiple files\nSometimes your data is split across multiple files instead of being contained in a single file. For example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March. With read_csv() you can read these data in at once and stack them on top of each other in a single data frame.\n\nsales_files <- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\nread_csv(sales_files, id = \"file\")\n#> Rows: 19 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): month\n#> dbl (4): year, brand, item, n\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> # A tibble: 19 × 6\n#>   file              month    year brand  item     n\n#>   <chr>             <chr>   <dbl> <dbl> <dbl> <dbl>\n#> 1 data/01-sales.csv January  2019     1  1234     3\n#> 2 data/01-sales.csv January  2019     1  8721     9\n#> 3 data/01-sales.csv January  2019     1  1822     2\n#> 4 data/01-sales.csv January  2019     2  3333     1\n#> 5 data/01-sales.csv January  2019     2  2156     9\n#> 6 data/01-sales.csv January  2019     2  3987     6\n#> # … with 13 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWith the additional id parameter we have added a new column called file to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\nIf you have many files you want to read in, it can get cumbersome to write out their names as a list. Instead, you can use the dir_ls() function from the fs package to find the files for you by matching a pattern in the file names.\n\nlibrary(fs)\nsales_files <- dir_ls(\"data\", glob = \"*sales.csv\")\nsales_files\n#> data/01-sales.csv data/02-sales.csv data/03-sales.csv"
  },
  {
    "objectID": "data-import.html#sec-writing-to-a-file",
    "href": "data-import.html#sec-writing-to-a-file",
    "title": "8  Data import",
    "section": "\n8.5 Writing to a file",
    "text": "8.5 Writing to a file\nreadr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv(). Both functions increase the chances of the output file being read back in correctly by:\n\nAlways encoding strings in UTF-8.\nSaving dates and date-times in ISO8601 format so they are easily parsed elsewhere.\n\nIf you want to export a csv file to Excel, use write_excel_csv() — this writes a special character (a “byte order mark”) at the start of the file which tells Excel that you’re using the UTF-8 encoding.\nThe most important arguments are x (the data frame to save), and file (the location to save it). You can also specify how missing values are written with na, and if you want to append to an existing file.\n\nwrite_csv(students, \"students.csv\")\n\nNow let’s read that csv file back in. Note that the type information is lost when you save to csv:\n\nstudents\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <fct>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\nwrite_csv(students, \"students-2.csv\")\nread_csv(\"students-2.csv\")\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nThis makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. There are two alternatives:\n\n\nwrite_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS:\n\nwrite_rds(students, \"students.rds\")\nread_rds(\"students.rds\")\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <fct>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nThe feather package implements a fast binary file format that can be shared across programming languages:\n\nlibrary(feather)\nwrite_feather(students, \"students.feather\")\nread_feather(\"students.feather\")\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <fct>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    NA                 Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nFeather tends to be faster than RDS and is usable outside of R. RDS supports list-columns (which you’ll learn about in Chapter 25; feather currently does not."
  },
  {
    "objectID": "workflow-scripts.html",
    "href": "workflow-scripts.html",
    "title": "9  Workflow: scripts and projects",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz.\nSo far, you have used the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and dplyr pipes. To give yourself more room to work, it’s a great idea to use the script editor. Open it up either by clicking the File menu, and selecting New File, then R script, or using the keyboard shortcut Cmd/Ctrl + Shift + N. Now you’ll see four panes:\nThe script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. RStudio will automatically save the contents of the editor when you quit RStudio, and will automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up."
  },
  {
    "objectID": "workflow-scripts.html#naming-files",
    "href": "workflow-scripts.html#naming-files",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.1 Naming files",
    "text": "9.1 Naming files\nSaving your code in a script requires creating a new file that you will need to name. It might be tempting to name this file code.R or myscript.R, but you should think a bit harder before choosing a name for your file. Three important principles for file naming are as follows:\n\nFile names should be machine readable: Avoid spaces, punctuation, symbols, and accented character. Do not rely on case sensitivity to distinguish files. Make deliberate use of delimiters.\nFile names should be human readable: Use file names that describe what is in the file.\nFile names should play well with default ordering: Start file names with numbers that allow them to be sorted in the order they get used.\n\nSuppose you have the following files in a project folder.\nrun-first.R\nalternative model.R\ncode for exploratory analysis.R\nfinalreport.qmd\nFinalReport.qmd\nfig 1.png\nFigure_02.png\nmodel_first_try.R\ntemp.txt\nThere are a variety of problems here: the files are misordered, file names contain spaces, there are two files with basically the same name but different capitalization (finalreport vs. FinalReport), and some file names don’t reflect their contents (run-first and temp).\nBelow is an alternative way of naming and organizing the same set of files.\n01-load-data.R\n02-exploratory-analysis.R\n03-model-approach-1.R\n04-model-approach-2.R\nfig-01.png\nfig-02.png\nnotes-on-report-draft.txt\nreport-2022-03-20.qmd\nreport-2022-04-02.qmd\nNumbering and descriptive names that are similarly formatted allow for a more useful organization of the R scripts. Additionally, the figures are labelled similarly, the reports are distinguished by dates included in the file names, and temp is renamed to notes-on-report-draft to better describe its contents."
  },
  {
    "objectID": "workflow-scripts.html#running-code",
    "href": "workflow-scripts.html#running-code",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.2 Running code",
    "text": "9.2 Running code\nThe script editor is also a great place to build up complex ggplot2 plots or long sequences of dplyr manipulations. The key to using the script editor effectively is to memorize one of the most important keyboard shortcuts: Cmd/Ctrl + Enter. This executes the current R expression in the console. For example, take the code below. If your cursor is at █, pressing Cmd/Ctrl + Enter will run the complete command that generates not_cancelled. It will also move the cursor to the next statement (beginning with not_cancelled |>). That makes it easy to step through your complete script by repeatedly pressing Cmd/Ctrl + Enter.\n\nlibrary(dplyr)\nlibrary(nycflights13)\n\nnot_cancelled <- flights |> \n  filter(!is.na(dep_delay)█, !is.na(arr_delay))\n\nnot_cancelled |> \n  group_by(year, month, day) |> \n  summarize(mean = mean(dep_delay))\n\nInstead of running your code expression-by-expression, you can also execute the complete script in one step: Cmd/Ctrl + Shift + S. Doing this regularly is a great way to ensure that you’ve captured all the important parts of your code in the script.\nI recommend that you always start your script with the packages that you need. That way, if you share your code with others, they can easily see which packages they need to install. Note, however, that you should never include install.packages() or setwd() in a script that you share. It’s very antisocial to change settings on someone else’s computer!\nWhen working through future chapters, I highly recommend starting in the script editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it."
  },
  {
    "objectID": "workflow-scripts.html#rstudio-diagnostics",
    "href": "workflow-scripts.html#rstudio-diagnostics",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.3 RStudio diagnostics",
    "text": "9.3 RStudio diagnostics\nThe script editor will also highlight syntax errors with a red squiggly line and a cross in the sidebar:\n\n\n\n\n\nHover over the cross to see what the problem is:\n\n\n\n\n\nRStudio will also let you know about potential problems:"
  },
  {
    "objectID": "workflow-scripts.html#workflow-projects",
    "href": "workflow-scripts.html#workflow-projects",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.4 Workflow: projects",
    "text": "9.4 Workflow: projects\nOne day, you will need to quit R, go do something else, and return to your analysis later. One day, you will be working on multiple analyses simultaneously that all use R and you want to keep them separate. One day, you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions:\n\nWhat about your analysis is “real”, i.e. what will you save as your lasting record of what happened?\nWhere does your analysis “live”?"
  },
  {
    "objectID": "workflow-scripts.html#what-is-real",
    "href": "workflow-scripts.html#what-is-real",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.5 What is real?",
    "text": "9.5 What is real?\nAs a beginning R user, it’s OK to consider your environment (i.e. the objects listed in the environment pane) “real”. However, in the long run, you’ll be much better off if you consider your R scripts as “real”.\nWith your R scripts (and your data files), you can recreate the environment. It’s much harder to recreate your R scripts from your environment! You’ll either have to retype a lot of code from memory (inevitably, making mistakes along the way) or you’ll have to carefully mine your R history.\nTo encourage this behavior, I highly recommend that you instruct RStudio not to preserve your workspace between sessions:\n\n\n\n\n\nThis will cause you some short-term pain, because now when you restart RStudio, it will no longer remember the results of the code that you ran last time. But this short-term pain will save you long-term agony because it forces you to capture all important interactions in your code. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your workspace, not the calculation itself in your code.\nThere is a great pair of keyboard shortcuts that will work together to make sure you’ve captured the important parts of your code in the editor:\n\nPress Cmd/Ctrl + Shift + F10 to restart RStudio.\nPress Cmd/Ctrl + Shift + S to rerun the current script.\n\nI use this pattern hundreds of times a week."
  },
  {
    "objectID": "workflow-scripts.html#where-does-your-analysis-live",
    "href": "workflow-scripts.html#where-does-your-analysis-live",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.6 Where does your analysis live?",
    "text": "9.6 Where does your analysis live?\nR has a powerful notion of the working directory. This is where R looks for files that you ask it to load, and where it will put any files that you ask it to save. RStudio shows your current working directory at the top of the console:\n\n\n\n\n\nAnd you can print this out in R code by running getwd():\n\ngetwd()\n#> [1] \"/Users/hadley/Documents/r4ds/r4ds\"\n\nAs a beginning R user, it’s OK to let your home directory, documents directory, or any other weird directory on your computer be R’s working directory. But you’re six chapters into this book, and you’re no longer a rank beginner. Very soon now you should evolve to organizing your analytical projects into directories and, when working on a project, setting R’s working directory to the associated directory.\nI do not recommend it, but you can also set the working directory from within R:\n\nsetwd(\"/path/to/my/CoolProject\")\n\nBut you should never do this because there’s a better way; a way that also puts you on the path to managing your R work like an expert."
  },
  {
    "objectID": "workflow-scripts.html#paths-and-directories",
    "href": "workflow-scripts.html#paths-and-directories",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.7 Paths and directories",
    "text": "9.7 Paths and directories\nPaths and directories are a little complicated because there are two basic styles of paths: Mac/Linux and Windows. There are three chief ways in which they differ:\n\nThe most important difference is how you separate the components of the path. Mac and Linux uses slashes (e.g. plots/diamonds.pdf) and Windows uses backslashes (e.g. plots\\diamonds.pdf). R can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to R, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so I recommend always using the Linux/Mac style with forward slashes.\nAbsolute paths (i.e. paths that point to the same place regardless of your working directory) look different. In Windows they start with a drive letter (e.g. C:) or two backslashes (e.g. \\\\servername) and in Mac/Linux they start with a slash “/” (e.g. /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you.\nThe last minor difference is the place that ~ points to. ~ is a convenient shortcut to your home directory. Windows doesn’t really have the notion of a home directory, so it instead points to your documents directory."
  },
  {
    "objectID": "workflow-scripts.html#rstudio-projects",
    "href": "workflow-scripts.html#rstudio-projects",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.8 RStudio projects",
    "text": "9.8 RStudio projects\nR experts keep all the files associated with a given project together — input data, R scripts, analytical results, and figures. This is such a wise and common practice that RStudio has built-in support for this via projects.\nLet’s make a project for you to use while you’re working through the rest of this book. Click File > New Project, then:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCall your project r4ds and think carefully about which subdirectory you put the project in. If you don’t store it somewhere sensible, it will be hard to find it in the future!\nOnce this process is complete, you’ll get a new RStudio project just for this book. Check that the “home” directory of your project is the current working directory:\n\ngetwd()\n#> [1] /Users/hadley/Documents/r4ds/r4ds\n\nWhenever you refer to a file using a relative path, R will look for it here.\nNow enter the following commands in the script editor, and save the file, calling it “diamonds.R”. Next, run the complete script which will save a PDF and CSV file into your project directory. Don’t worry about the details, you’ll learn them later in the book.\n\nlibrary(tidyverse)\n\nggplot(diamonds, aes(carat, price)) + \n  geom_hex()\nggsave(\"diamonds.pdf\")\n\nwrite_csv(diamonds, \"diamonds.csv\")\n\nQuit RStudio. Inspect the folder associated with your project — notice the .Rproj file. Double-click that file to re-open the project. Notice you get back to where you left off: it’s the same working directory and command history, and all the files you were working on are still open. Because you followed my instructions above, you will, however, have a completely fresh environment, guaranteeing that you’re starting with a clean slate.\nIn your favorite OS-specific way, search your computer for diamonds.pdf and you will find the PDF (no surprise) but also the script that created it (diamonds.R). This is a huge win! One day, you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with R code and never with the mouse or the clipboard, you will be able to reproduce old work with ease!"
  },
  {
    "objectID": "workflow-scripts.html#summary",
    "href": "workflow-scripts.html#summary",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.9 Summary",
    "text": "9.9 Summary\nIn summary, RStudio projects give you a solid workflow that will serve you well in the future:\n\nCreate an RStudio project for each data analysis project.\nKeep data files there; we’ll talk about loading them into R in @ref(data-import).\nKeep scripts there; edit them, run them in bits or as a whole.\nSave your outputs (plots and cleaned data) there.\nOnly ever use relative paths, not absolute paths.\n\nEverything you need is in one place and cleanly separated from all the other projects that you are working on."
  },
  {
    "objectID": "workflow-scripts.html#exercises",
    "href": "workflow-scripts.html#exercises",
    "title": "9  Workflow: scripts and projects",
    "section": "\n9.10 Exercises",
    "text": "9.10 Exercises\n\nGo to the RStudio Tips Twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!\nWhat other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "10  Exploratory Data Analysis",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\nThis chapter will show you how to use visualization and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle. You:\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\nEDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others.\nEDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. To do data cleaning, you’ll need to deploy all the tools of EDA: visualization, transformation, and modelling.\n\n10.1.1 Prerequisites\nIn this chapter we’ll combine what you’ve learned about dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "EDA.html#questions",
    "href": "EDA.html#questions",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.2 Questions",
    "text": "10.2 Questions\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey\n\nYour goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.\nEDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions. It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery. You can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find.\nThere is no rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?\n\nThe rest of this chapter will look at these two questions. We’ll explain what variation and covariation are, and we’ll show you several ways to answer each question. To make the discussion easier, let’s define some terms:\n\nA variable is a quantity, quality, or property that you can measure.\nA value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\nAn observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\nTabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.\n\nSo far, all of the data that you’ve seen has been tidy. In real-life, most data isn’t tidy, so we’ll come back to these ideas again in Chapter 25."
  },
  {
    "objectID": "EDA.html#variation",
    "href": "EDA.html#variation",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.3 Variation",
    "text": "10.3 Variation\nVariation is the tendency of the values of a variable to change from measurement to measurement. You can see variation easily in real life; if you measure any continuous variable twice, you will get two different results. This is true even if you measure quantities that are constant, like the speed of light. Each of your measurements will include a small amount of error that varies from measurement to measurement. Variables can also vary if you measure across different subjects (e.g. the eye colors of different people) or different times (e.g. the energy levels of an electron at different moments). Every variable has its own pattern of variation, which can reveal interesting information about how that variable varies between measurements on the same observation as well as across observations. The best way to understand that pattern is to visualize the distribution of the variable’s values.\n\n10.3.1 Visualizing distributions\nHow you visualize the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is categorical if it can only take one of a small set of values. In R, categorical variables are usually saved as factors or character vectors. To examine the distribution of a categorical variable, you can use a bar chart:\n\nggplot(data = diamonds, mapping = aes(x = cut)) +\n  geom_bar()\n\n\n\n\nThe height of the bars displays how many observations occurred with each x value. You can compute these values manually with count():\n\ndiamonds |> \n  count(cut)\n#> # A tibble: 5 × 2\n#>   cut           n\n#>   <ord>     <int>\n#> 1 Fair       1610\n#> 2 Good       4906\n#> 3 Very Good 12082\n#> 4 Premium   13791\n#> 5 Ideal     21551\n\nA variable is continuous if it can take any of an infinite set of ordered values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, you can use a histogram:\n\nggplot(data = diamonds, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\nYou can compute this by hand by combining count() and cut_width():\n\ndiamonds |> \n  count(cut_width(carat, 0.5))\n#> # A tibble: 11 × 2\n#>   `cut_width(carat, 0.5)`     n\n#>   <fct>                   <int>\n#> 1 [-0.25,0.25]              785\n#> 2 (0.25,0.75]             29498\n#> 3 (0.75,1.25]             15977\n#> 4 (1.25,1.75]              5313\n#> 5 (1.75,2.25]              2002\n#> 6 (2.25,2.75]               322\n#> # … with 5 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. Note that even though it’s not possible to have a carat value that is smaller than 0 (since weights of diamonds, by definition, are positive values), the bins start at a negative value (-0.25) in order to create bins of equal width across the range of the data with the center of the first bin at 0. This behavior is also apparent in the histogram above, where the first bar ranges from -0.25 to 0.25. The tallest bar shows that almost 30,000 observations have a carat value between 0.25 and 0.75, which are the left and right edges of the bar centered at 0.5.\nYou can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. For example, here is how the graph above looks when we zoom into just the diamonds with a size of less than three carats and choose a smaller binwidth.\n\nsmaller <- diamonds |> \n  filter(carat < 3)\n  \nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\nIf you wish to overlay multiple histograms in the same plot, we recommend using geom_freqpoly() instead of geom_histogram(). geom_freqpoly() performs the same calculation as geom_histogram(), but instead of displaying the counts with bars, uses lines instead. It’s much easier to understand overlapping lines than bars.\n\nggplot(data = smaller, mapping = aes(x = carat, color = cut)) +\n  geom_freqpoly(binwidth = 0.1, size = 0.75)\n\n\n\n\nWe’ve also customized the thickness of the lines using the size argument in order to make them stand out a bit more against the background.\nThere are a few challenges with this type of plot, which we will come back to in Section 10.5.1 on visualizing a categorical and a continuous variable.\nNow that you can visualize variation, what should you look for in your plots? And what type of follow-up questions should you ask? We’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information. The key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?).\n\n10.3.2 Typical values\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. To turn this information into useful questions, look for anything unexpected:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?\n\nAs an example, the histogram below suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\n\nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\nClusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow are the observations within each cluster similar to each other?\nHow are the observations in separate clusters different from each other?\nHow can you explain or describe the clusters?\nWhy might the appearance of clusters be misleading?\n\nThe histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between.\n\nggplot(data = faithful, mapping = aes(x = eruptions)) + \n  geom_histogram(binwidth = 0.25)\n\n\n\n\nMany of the questions above will prompt you to explore a relationship between variables, for example, to see if the values of one variable can explain the behavior of another variable. We’ll get to that shortly.\n\n10.3.3 Unusual values\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis.\n\nggplot(data = diamonds, mapping = aes(x = y)) + \n  geom_histogram(binwidth = 0.5)\n\n\n\n\nThere are so many observations in the common bins that the rare bins are very short, making it very difficult to see them (although maybe if you stare intently at 0 you’ll spot something). To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian():\n\nggplot(data = diamonds, mapping = aes(x = y)) + \n  geom_histogram(binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))\n\n\n\n\ncoord_cartesian() also has an xlim() argument for when you need to zoom into the x-axis. ggplot2 also has xlim() and ylim() functions that work slightly differently: they throw away the data outside the limits.\nThis allows us to see that there are three unusual values: 0, ~30, and ~60. We pluck them out with dplyr:\n\nunusual <- diamonds |> \n  filter(y < 3 | y > 20) |> \n  select(price, x, y, z) |>\n  arrange(y)\nunusual\n#> # A tibble: 9 × 4\n#>   price     x     y     z\n#>   <int> <dbl> <dbl> <dbl>\n#> 1  5139  0      0    0   \n#> 2  6381  0      0    0   \n#> 3 12800  0      0    0   \n#> 4 15686  0      0    0   \n#> 5 18034  0      0    0   \n#> 6  2130  0      0    0   \n#> 7  2130  0      0    0   \n#> 8  2075  5.15  31.8  5.12\n#> 9 12210  8.09  58.9  8.06\n\nThe y variable measures one of the three dimensions of these diamonds, in mm. We know that diamonds can’t have a width of 0mm, so these values must be incorrect. We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars!\nIt’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to omit them, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.\n\n10.3.4 Exercises\n\nExplore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.\nExplore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)\nHow many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?\nCompare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?"
  },
  {
    "objectID": "EDA.html#sec-missing-values-eda",
    "href": "EDA.html#sec-missing-values-eda",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.4 Missing values",
    "text": "10.4 Missing values\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n\n\nDrop the entire row with the strange values:\n\ndiamonds2 <- diamonds |> \n  filter(between(y, 3, 20))\n\nWe don’t recommend this option because just because one measurement is invalid, doesn’t mean all the measurements are. Additionally, if you have low quality data, by time that you’ve applied this approach to every variable you might find that you don’t have any data left!\n\n\nInstead, we recommend replacing the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the if_else() function to replace unusual values with NA:\n\ndiamonds2 <- diamonds |> \n  mutate(y = if_else(y < 3 | y > 20, NA_real_, y))\n\n\n\nif_else() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is false. Alternatively to if_else(), use case_when(). case_when() is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables or would otherwise require multiple if_else() statements nested inside one another.\nLike R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. It’s not obvious where you should plot missing values, so ggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed:\n\nggplot(data = diamonds2, mapping = aes(x = x, y = y)) + \n  geom_point()\n#> Warning: Removed 9 rows containing missing values (geom_point).\n\n\n\n\nTo suppress that warning, set na.rm = TRUE:\n\nggplot(data = diamonds2, mapping = aes(x = x, y = y)) + \n  geom_point(na.rm = TRUE)\n\nOther times you want to understand what makes observations with missing values different to observations with recorded values. For example, in nycflights13::flights1, missing values in the dep_time variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and non-cancelled times. You can do this by making a new variable with is.na().\n\nnycflights13::flights |> \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + sched_min / 60\n  ) |> \n  ggplot(mapping = aes(sched_dep_time)) + \n  geom_freqpoly(mapping = aes(color = cancelled), binwidth = 1/4)\n\n\n\n\nHowever this plot isn’t great because there are many more non-cancelled flights than cancelled flights. In the next section we’ll explore some techniques for improving this comparison.\n\n10.4.1 Exercises\n\nWhat happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference in how missing values are handled in histograms and bar charts?\nWhat does na.rm = TRUE do in mean() and sum()?"
  },
  {
    "objectID": "EDA.html#covariation",
    "href": "EDA.html#covariation",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.5 Covariation",
    "text": "10.5 Covariation\nIf variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualize the relationship between two or more variables. How you do that depends again on the types of variables involved.\n\n10.5.1 A categorical and continuous variable\nIt’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. The default appearance of geom_freqpoly() is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it’s hard to see the differences in the shapes of their distributions. For example, let’s explore how the price of a diamond varies with its quality (measured by cut):\n\nggplot(data = diamonds, mapping = aes(x = price)) + \n  geom_freqpoly(mapping = aes(color = cut), binwidth = 500, size = 0.75)\n\n\n\n\nIt’s hard to see the difference in distribution because the overall counts differ so much:\n\nggplot(data = diamonds, mapping = aes(x = cut)) + \n  geom_bar()\n\n\n\n\nTo make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display the density, which is the count standardized so that the area under each frequency polygon is one.\n\nggplot(data = diamonds, mapping = aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(mapping = aes(color = cut), binwidth = 500, size = 0.75)\n\n\n\n\nNote that we’re mapping the density the y, but since density is not a variable in the diamonds dataset, we need to first calculate it. We use the after_stat() function to do so.\nThere’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot.\nAnother alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. A boxplot is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of:\n\nA box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the interquartile range (IQR). In the middle of the box is a line that displays the median, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.\nVisual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.\nA line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.\n\n\n\n\n\n\nLet’s take a look at the distribution of price by cut using geom_boxplot():\n\nggplot(data = diamonds, mapping = aes(x = cut, y = price)) +\n  geom_boxplot()\n\n\n\n\nWe see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). It supports the counter-intuitive finding that better quality diamonds are cheaper on average! In the exercises, you’ll be challenged to figure out why.\ncut is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is with the reorder() function.\nFor example, take the class variable in the mpg dataset. You might be interested to know how highway mileage varies across classes:\n\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) +\n  geom_boxplot()\n\n\n\n\nTo make the trend easier to see, we can reorder class based on the median value of hwy:\n\nggplot(data = mpg,\n       mapping = aes(x = fct_reorder(class, hwy, median), y = hwy)) +\n  geom_boxplot()\n\n\n\n\nIf you have long variable names, geom_boxplot() will work better if you flip it 90°. You can do that by exchanging the x and y aesthetic mappings.\n\nggplot(data = mpg,\n       mapping = aes(y = fct_reorder(class, hwy, median), x = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n10.5.1.1 Exercises\n\nUse what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights.\nWhat variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\nInstead of exchanging the x and y variables, add coord_flip() as a new layer to the vertical boxplot to create a horizontal one. How does this compare to using exchanging the variables?\nOne problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots?\nCompare and contrast geom_violin() with a faceted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method?\nIf you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.\n\n10.5.2 Two categorical variables\nTo visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination of levels of these categorical variables. One way to do that is to rely on the built-in geom_count():\n\nggplot(data = diamonds, mapping = aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\nThe size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values.\nA more commonly used way of representing the covariation between two categorical variables is using a segmented bar chart. In creating this bar chart, we map the variable we want to divide the data into first to the x aesthetic and the variable we then further want to divide each group into to the fill aesthetic.\n\nggplot(data = diamonds, mapping = aes(x = cut, fill = color)) +\n  geom_bar()\n\n\n\n\nHowever, in order to get a better sense of the relationship between these two variables, you should compare proportions instead of counts across groups.\n\nggplot(data = diamonds, mapping = aes(x = cut, fill = color)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nAnother approach for exploring the relationship between these variables is computing the counts with dplyr:\n\ndiamonds |> \n  count(color, cut)\n#> # A tibble: 35 × 3\n#>   color cut           n\n#>   <ord> <ord>     <int>\n#> 1 D     Fair        163\n#> 2 D     Good        662\n#> 3 D     Very Good  1513\n#> 4 D     Premium    1603\n#> 5 D     Ideal      2834\n#> 6 E     Fair        224\n#> # … with 29 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThen visualize with geom_tile() and the fill aesthetic:\n\ndiamonds |> \n  count(color, cut) |>  \n  ggplot(mapping = aes(x = color, y = cut)) +\n  geom_tile(mapping = aes(fill = n))\n\n\n\n\nIf the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. For larger plots, you might want to try the heatmaply package, which creates interactive plots.\n\n10.5.2.1 Exercises\n\nHow could you rescale the count dataset above to more clearly show the distribution of cut within color, or color within cut?\nHow does the segmented bar chart change if color is mapped to the x aesthetic and cut is mapped to the fill aesthetic? Calculate the counts that fall into each of the segments.\nUse geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?\nWhy is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above?\n\n10.5.3 Two continuous variables\nYou’ve already seen one great way to visualize the covariation between two continuous variables: draw a scatterplot with geom_point(). You can see covariation as a pattern in the points. For example, you can see an exponential relationship between the carat size and price of a diamond.\n\nggplot(data = diamonds, mapping = aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\nScatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above). You’ve already seen one way to fix the problem: using the alpha aesthetic to add transparency.\n\nggplot(data = diamonds, mapping = aes(x = carat, y = price)) + \n  geom_point(alpha = 1 / 100)\n\n\n\n\nBut using transparency can be challenging for very large datasets. Another solution is to use bin. Previously you used geom_histogram() and geom_freqpoly() to bin in one dimension. Now you’ll learn how to use geom_bin2d() and geom_hex() to bin in two dimensions.\ngeom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().\n\nggplot(data = smaller, mapping = aes(x = carat, y = price)) +\n  geom_bin2d()\n\n# install.packages(\"hexbin\")\nggplot(data = smaller, mapping = aes(x = carat, y = price)) +\n  geom_hex()\n\n\n\n\nAnother option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualizing the combination of a categorical and a continuous variable that you learned about. For example, you could bin carat and then for each group, display a boxplot:\n\nggplot(data = smaller, mapping = aes(x = carat, y = price)) + \n  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))\n\n\n\n\ncut_width(x, width), as used above, divides x into bins of width width. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summaries a different number of points. One way to show that is to make the width of the boxplot proportional to the number of points with varwidth = TRUE.\nAnother approach is to display approximately the same number of points in each bin. That’s the job of cut_number():\n\nggplot(data = smaller, mapping = aes(x = carat, y = price)) + \n  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))\n\n\n\n\n\n10.5.3.1 Exercises\n\nInstead of summarizing the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualization of the 2d distribution of carat and price?\nVisualize the distribution of carat, partitioned by price.\nHow does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you?\nCombine two of the techniques you’ve learned to visualize the combined distribution of cut, carat, and price.\n\nTwo dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.\n\nggplot(data = diamonds, mapping = aes(x = x, y = y)) +\n  geom_point() +\n  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))\n\n\n\n\nWhy is a scatterplot a better display than a binned plot for this case?"
  },
  {
    "objectID": "EDA.html#patterns-and-models",
    "href": "EDA.html#patterns-and-models",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.6 Patterns and models",
    "text": "10.6 Patterns and models\nPatterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:\n\nCould this pattern be due to coincidence (i.e. random chance)?\nHow can you describe the relationship implied by the pattern?\nHow strong is the relationship implied by the pattern?\nWhat other variables might affect the relationship?\nDoes the relationship change if you look at individual subgroups of the data?\n\nA scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above.\n\nggplot(data = faithful, mapping = aes(x = eruptions, y = waiting)) + \n  geom_point()\n\n\n\n\nPatterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.\nModels are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. Note that instead of using the raw values of price and carat, we log transform them first, and fit a model to the log-transformed values. Then, we exponentiate the residuals to put them back in the scale of raw prices.\n\nlibrary(tidymodels)\n\ndiamonds <- diamonds |>\n  mutate(\n    log_price = log(price),\n    log_carat = log(carat)\n  )\n\ndiamonds_fit <- linear_reg() |>\n  fit(log_price ~ log_carat, data = diamonds)\n\ndiamonds_aug <- augment(diamonds_fit, new_data = diamonds) |>\n  mutate(.resid = exp(.resid))\n\nggplot(data = diamonds_aug, mapping = aes(x = carat, y = .resid)) + \n  geom_point()\n\n\n\n\nOnce you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.\n\nggplot(data = diamonds_aug, mapping = aes(x = cut, y = .resid)) + \n  geom_boxplot()\n\n\n\n\nWe’re not discussing modelling in this book because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand."
  },
  {
    "objectID": "EDA.html#ggplot2-calls",
    "href": "EDA.html#ggplot2-calls",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.7 ggplot2 calls",
    "text": "10.7 ggplot2 calls\nAs we move on from these introductory chapters, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning:\n\nggplot(data = faithful, mapping = aes(x = eruptions)) + \n  geom_freqpoly(binwidth = 0.25)\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping, and the first two arguments to aes() are x and y. In the remainder of the book, we won’t supply those names. That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what’s different between plots. That’s a really important programming concern that we’ll come back to in Chapter 27.\nRewriting the previous plot more concisely yields:\n\nggplot(faithful, aes(eruptions)) + \n  geom_freqpoly(binwidth = 0.25)\n\nSometimes we’ll turn the end of a pipeline of data transformation into a plot. Watch for the transition from |> to +. We wish this transition wasn’t necessary but unfortunately ggplot2 was created before the pipe was discovered.\n\ndiamonds |> \n  count(cut, clarity) |> \n  ggplot(aes(clarity, cut, fill = n)) + \n  geom_tile()"
  },
  {
    "objectID": "EDA.html#learning-more",
    "href": "EDA.html#learning-more",
    "title": "10  Exploratory Data Analysis",
    "section": "\n10.8 Learning more",
    "text": "10.8 Learning more\nIf you want to learn more about the mechanics of ggplot2, we highly recommend reading the ggplot2 book. Another useful resource is the R Graphics Cookbook by Winston Chang."
  },
  {
    "objectID": "workflow-help.html",
    "href": "workflow-help.html",
    "title": "11  Workflow: Getting help",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz.\nThis book is not an island; there is no single resource that will allow you to master R. As you begin to apply the techniques described in this book to your own data, you will soon find questions that we do not answer. This section describes a few tips on how to get help, and to help you keep learning."
  },
  {
    "objectID": "workflow-help.html#google-is-your-friend",
    "href": "workflow-help.html#google-is-your-friend",
    "title": "11  Workflow: Getting help",
    "section": "\n11.1 Google is your friend",
    "text": "11.1 Google is your friend\nIf you get stuck, start with Google. Typically adding “R” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any R-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code; you’re more likely to find help for English error messages.)\nIf Google doesn’t help, try Stack Overflow. Start by spending a little time searching for an existing answer, including [R] to restrict your search to questions and answers that use R."
  },
  {
    "objectID": "workflow-help.html#making-a-reprex",
    "href": "workflow-help.html#making-a-reprex",
    "title": "11  Workflow: Getting help",
    "section": "\n11.2 Making a reprex",
    "text": "11.2 Making a reprex\nIf your googling doesn’t find anything useful, it’s a really good idea prepare a minimal reproducible example or reprex. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are two parts to creating a reprex:\n\nFirst, you need to make your code reproducible. This means that you need to capture everything, i.e., include any library() calls and create all necessary objects. The easiest way to make sure you’ve done this is to use the reprex package.\nSecond, you need to make it minimal. Strip away everything that is not directly related to your problem. This usually involves creating a much smaller and simpler R object than the one you’re facing in real life or even using built-in data.\n\nThat sounds like a lot of work! And it can be, but it has a great payoff:\n\n80% of the time creating an excellent reprex reveals the source of your problem. It’s amazing how often the process of writing up a self-contained and minimal example allows you to answer your own question.\nThe other 20% of time you will have captured the essence of your problem in a way that is easy for others to play with. This substantially improves your chances of getting help!\n\nWhen creating a reprex by hand, it’s easy to accidentally miss something that means your code can’t be run on someone else’s computer. Avoid this problem by using the reprex package which is installed as part of the tidyverse. Let’s say you copy this code onto your clipboard (or, on RStudio Server or Cloud, select it):\n\ny <- 1:4\nmean(y)\n\nThen call reprex(), where the default target venue is GitHub:\nreprex::reprex()\nA nicely rendered HTML preview will display in RStudio’s Viewer (if you’re in RStudio) or your default browser otherwise. The relevant bit of GitHub-flavored Markdown is ready to be pasted from your clipboard (on RStudio Server or Cloud, you will need to copy this yourself):\n``` r\ny <- 1:4\nmean(y)\n#> [1] 2.5\n```\nHere’s what that Markdown would look like rendered in a GitHub issue:\n\ny <- 1:4\nmean(y)\n#> [1] 2.5\n\nAnyone else can copy, paste, and run this immediately.\nInstead of reading from the clipboard, you can:\n\nreprex(mean(rnorm(10))) to get code from expression.\nreprex(input = \"mean(rnorm(10))\\n\") gets code from character vector (detected via length or terminating newline). Leading prompts are stripped from input source: reprex(input = \"> median(1:3)\\n\") produces same output as reprex(input = \"median(1:3)\\n\")\nreprex(input = \"my_reprex.R\") gets code from file\nUse one of the RStudio add-ins to use the selected text or current file.\n\nThere are three things you need to include to make your example reproducible: required packages, data, and code.\n\nPackages should be loaded at the top of the script, so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; it’s possible you’ve discovered a bug that’s been fixed since you installed or last updated the package. For packages in the tidyverse, the easiest way to check is to run tidyverse_update().\n\nThe easiest way to include data in a question is to use dput() to generate the R code to recreate it. For example, to recreate the mtcars dataset in R, I’d perform the following steps:\n\nRun dput(mtcars) in R\nCopy the output\nIn my reproducible script, type mtcars <- then paste.\n\nTry and find the smallest subset of your data that still reveals the problem.\n\n\nSpend a little bit of time ensuring that your code is easy for others to read:\n\nMake sure you’ve used spaces and your variable names are concise, yet informative.\nUse comments to indicate where your problem lies.\n\nDo your best to remove everything that is not related to the problem.\nThe shorter your code is, the easier it is to understand, and the easier it is to fix.\n\n\n\n\nFinish by checking that you have actually made a reproducible example by starting a fresh R session and copying and pasting your script in."
  },
  {
    "objectID": "workflow-help.html#investing-in-yourself",
    "href": "workflow-help.html#investing-in-yourself",
    "title": "11  Workflow: Getting help",
    "section": "\n11.3 Investing in yourself",
    "text": "11.3 Investing in yourself\nYou should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning R each day will pay off handsomely in the long run. One way is to follow what Hadley, Garrett, and everyone else at RStudio are doing on the RStudio blog. This is where we post announcements about new packages, new IDE features, and in-person courses. You might also want to follow Hadley (@hadleywickham), Mine (@minebocek), Garrett (@statgarrett) on Twitter, or follow @rstudiotips to keep up with new features in the IDE.\nTo keep up with the R community more broadly, we recommend reading http://www.r-bloggers.com: it aggregates over 500 blogs about R from around the world. If you’re an active Twitter user, follow the (#rstats) hashtag. Twitter is one of the key tools that Hadley and Mine use to keep up with new developments in the community."
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "Transform",
    "section": "",
    "text": "This part of the book proceeds as follows:\n\nIn Chapter 12, you’ll learn about the variant of the data frame that we use in this book: the tibble. You’ll learn what makes them different from regular data frames, and how you can construct them “by hand”.\nChapter 13 will give you tools for working with multiple interrelated datasets.\nChapter 15 …\nChapter 14 …\nChapter 20…\nChapter 16 will give you tools for working with strings and introduce regular expressions, a powerful tool for manipulating strings.\nChapter 17 …\nChapter 18 will introduce factors – how R stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string.\nChapter 19 will give you the key tools for working with dates and date-times.\nChapter 21 will give you tools for performing the same operation on multiple columns."
  },
  {
    "objectID": "tibble.html",
    "href": "tibble.html",
    "title": "12  Tibbles",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is largely complete and just needs final proof reading. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "tibble.html#introduction",
    "href": "tibble.html#introduction",
    "title": "12  Tibbles",
    "section": "\n12.1 Introduction",
    "text": "12.1 Introduction\nThroughout this book we work with “tibbles” instead of R’s traditional data.frame. Tibbles are data frames, but they tweak some older behaviors to make your life a little easier. R is an old language, and some things that were useful 10 or 20 years ago now get in your way. It’s difficult to change base R without breaking existing code, so most innovation occurs in packages. Here we will describe the tibble package, which provides opinionated data frames that make working in the tidyverse a little easier. In most places, we use the term tibble and data frame interchangeably; when we want to draw particular attention to R’s built-in data frame, we’ll call them data.frames.\nIf this chapter leaves you wanting to learn more about tibbles, you might enjoy vignette(\"tibble\").\n\n12.1.1 Prerequisites\nIn this chapter we’ll explore the tibble package, part of the core tidyverse.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "tibble.html#creating-tibbles",
    "href": "tibble.html#creating-tibbles",
    "title": "12  Tibbles",
    "section": "\n12.2 Creating tibbles",
    "text": "12.2 Creating tibbles\nIf you need to make a tibble “by hand”, you can use tibble() or tribble(). tibble() works by assembling individual vectors:\n\nx <- c(1, 2, 5)\ny <- c(\"a\", \"b\", \"h\")\n\ntibble(x, y)\n#> # A tibble: 3 × 2\n#>       x y    \n#>   <dbl> <chr>\n#> 1     1 a    \n#> 2     2 b    \n#> 3     5 h\n\nYou can also optionally name the inputs, provide data inline with c(), and perform computation:\n\ntibble(\n  x1 = x,\n  x2 = c(10, 15, 25),\n  y = sqrt(x1^2 + x2^2)\n)\n#> # A tibble: 3 × 3\n#>      x1    x2     y\n#>   <dbl> <dbl> <dbl>\n#> 1     1    10  10.0\n#> 2     2    15  15.1\n#> 3     5    25  25.5\n\nEvery column in a data frame or tibble must be same length, so you’ll get an error if the lengths are different:\n\ntibble(\n  x = c(1, 5),\n  y = c(\"a\", \"b\", \"c\")\n)\n#> Error:\n#> ! Tibble columns must have compatible sizes.\n#> • Size 2: Existing data.\n#> • Size 3: Column `y`.\n#> ℹ Only values of size one are recycled.\n\nAs the error suggests, individual values will be recycled to the same length as everything else:\n\ntibble(\n  x = 1:5,\n  y = \"a\",\n  z = TRUE\n)\n#> # A tibble: 5 × 3\n#>       x y     z    \n#>   <int> <chr> <lgl>\n#> 1     1 a     TRUE \n#> 2     2 a     TRUE \n#> 3     3 a     TRUE \n#> 4     4 a     TRUE \n#> 5     5 a     TRUE\n\nAnother way to create a tibble is with tribble(), which short for transposed tibble. tribble() is customized for data entry in code: column headings start with ~ and entries are separated by commas. This makes it possible to lay out small amounts of data in an easy to read form:\n\ntribble(\n  ~x, ~y, ~z,\n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)\n#> # A tibble: 2 × 3\n#>   x         y     z\n#>   <chr> <dbl> <dbl>\n#> 1 a         2   3.6\n#> 2 b         1   8.5\n\nFinally, if you have a regular data.frame you can turn it into to a tibble with as_tibble():\n\nas_tibble(mtcars)\n#> # A tibble: 32 × 11\n#>     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n#> 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n#> 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n#> 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n#> 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n#> 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n#> # … with 26 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThe inverse of as_tibble() is as.data.frame(); it converts a tibble back into a regular data.frame."
  },
  {
    "objectID": "tibble.html#non-syntactic-names",
    "href": "tibble.html#non-syntactic-names",
    "title": "12  Tibbles",
    "section": "\n12.3 Non-syntactic names",
    "text": "12.3 Non-syntactic names\nIt’s possible for a tibble to have column names that are not valid R variable names, names that are non-syntactic. For example, the variables might not start with a letter or they might contain unusual characters like a space. To refer to these variables, you need to surround them with backticks, `:\n\ntb <- tibble(\n  `:)` = \"smile\", \n  ` ` = \"space\",\n  `2000` = \"number\"\n)\ntb\n#> # A tibble: 1 × 3\n#>   `:)`  ` `   `2000`\n#>   <chr> <chr> <chr> \n#> 1 smile space number\n\nYou’ll also need the backticks when working with these variables in other packages, like ggplot2, dplyr, and tidyr."
  },
  {
    "objectID": "tibble.html#tibbles-vs.-data.frame",
    "href": "tibble.html#tibbles-vs.-data.frame",
    "title": "12  Tibbles",
    "section": "\n12.4 Tibbles vs. data.frame",
    "text": "12.4 Tibbles vs. data.frame\nThere are two main differences in the usage of a tibble vs. a classic data.frame: printing and subsetting. If these difference cause problems when working with older packages, you can turn a tibble back to a regular data frame with as.data.frame().\n\n12.4.1 Printing\nTibbles have a refined print method that shows only the first 10 rows, and all the columns that fit on screen. This makes it much easier to work with large data. In addition to its name, each column reports its type, a nice feature inspired by str():\n\ntibble(\n  a = lubridate::now() + runif(1e3) * 86400,\n  b = lubridate::today() + runif(1e3) * 30,\n  c = 1:1e3,\n  d = runif(1e3),\n  e = sample(letters, 1e3, replace = TRUE)\n)\n#> # A tibble: 1,000 × 5\n#>   a                   b              c     d e    \n#>   <dttm>              <date>     <int> <dbl> <chr>\n#> 1 2022-08-14 20:29:57 2022-08-21     1 0.368 n    \n#> 2 2022-08-15 14:35:06 2022-08-26     2 0.612 l    \n#> 3 2022-08-15 08:58:46 2022-09-05     3 0.415 p    \n#> 4 2022-08-14 22:20:03 2022-09-04     4 0.212 m    \n#> 5 2022-08-14 18:44:19 2022-09-01     5 0.733 i    \n#> 6 2022-08-15 05:45:16 2022-08-28     6 0.460 n    \n#> # … with 994 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWhere possible, tibbles also use color to draw your eye to important differences. One of the most important distinctions is between the string \"NA\" and the missing value, NA:\n\ntibble(x = c(\"NA\", NA))\n#> # A tibble: 2 × 1\n#>   x    \n#>   <chr>\n#> 1 NA   \n#> 2 <NA>\n\nTibbles are designed to avoid overwhelming your console when you print large data frames. But sometimes you need more output than the default display. There are a few options that can help.\nFirst, you can explicitly print() the data frame and control the number of rows (n) and the width of the display. width = Inf will display all columns:\n\nlibrary(nycflights13)\n\nflights |> \n  print(n = 10, width = Inf)\n#> # A tibble: 336,776 × 19\n#>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#>  1  2013     1     1      517            515         2      830            819\n#>  2  2013     1     1      533            529         4      850            830\n#>  3  2013     1     1      542            540         2      923            850\n#>  4  2013     1     1      544            545        -1     1004           1022\n#>  5  2013     1     1      554            600        -6      812            837\n#>  6  2013     1     1      554            558        -4      740            728\n#>  7  2013     1     1      555            600        -5      913            854\n#>  8  2013     1     1      557            600        -3      709            723\n#>  9  2013     1     1      557            600        -3      838            846\n#> 10  2013     1     1      558            600        -2      753            745\n#>    arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#>        <dbl> <chr>    <int> <chr>   <chr>  <chr>    <dbl>    <dbl> <dbl>  <dbl>\n#>  1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n#>  2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n#>  3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#>  4       -18 B6         725 N804JB  JFK    BQN        183     1576     5     45\n#>  5       -25 DL         461 N668DN  LGA    ATL        116      762     6      0\n#>  6        12 UA        1696 N39463  EWR    ORD        150      719     5     58\n#>  7        19 B6         507 N516JB  EWR    FLL        158     1065     6      0\n#>  8       -14 EV        5708 N829AS  LGA    IAD         53      229     6      0\n#>  9        -8 B6          79 N593JB  JFK    MCO        140      944     6      0\n#> 10         8 AA         301 N3ALAA  LGA    ORD        138      733     6      0\n#>    time_hour          \n#>    <dttm>             \n#>  1 2013-01-01 05:00:00\n#>  2 2013-01-01 05:00:00\n#>  3 2013-01-01 05:00:00\n#>  4 2013-01-01 05:00:00\n#>  5 2013-01-01 06:00:00\n#>  6 2013-01-01 05:00:00\n#>  7 2013-01-01 06:00:00\n#>  8 2013-01-01 06:00:00\n#>  9 2013-01-01 06:00:00\n#> 10 2013-01-01 06:00:00\n#> # … with 336,766 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou can also control the default print behavior by setting options:\n\noptions(tibble.print_max = n, tibble.print_min = m): if more than n rows, print only m rows. Use options(tibble.print_min = Inf) to always show all rows.\nUse options(tibble.width = Inf) to always print all columns, regardless of the width of the screen.\n\nYou can see a complete list of options by looking at the package help with package?tibble.\nA final option is to use RStudio’s built-in data viewer to get a scrollable view of the complete dataset. This is also often useful at the end of a long chain of manipulations.\n\nflights |> View()\n\n\n12.4.2 Extracting variables\nSo far all the tools you’ve learned have worked with complete data frames. If you want to pull out a single variable, you can use dplyr::pull():\n\ntb <- tibble(\n  id = LETTERS[1:5],\n  x1  = 1:5,\n  y1  = 6:10\n)\n\ntb |> pull(x1) # by name\n#> [1] 1 2 3 4 5\ntb |> pull(1)  # by position\n#> [1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\npull() also takes an optional name argument that specifies the column to be used as names for a named vector, which you’ll learn about in Chapter 28.\n\ntb |> pull(x1, name = id)\n#> A B C D E \n#> 1 2 3 4 5\n\nYou can also use the base R tools $ and [[. [[ can extract by name or position; $ only extracts by name but is a little less typing.\n\n# Extract by name\ntb$x1\n#> [1] 1 2 3 4 5\ntb[[\"x1\"]]\n#> [1] 1 2 3 4 5\n\n# Extract by position\ntb[[1]]\n#> [1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\nCompared to a data.frame, tibbles are more strict: they never do partial matching, and they will generate a warning if the column you are trying to access does not exist.\n\n# Tibbles complain a lot:\ntb$x\n#> Warning: Unknown or uninitialised column: `x`.\n#> NULL\ntb$z\n#> Warning: Unknown or uninitialised column: `z`.\n#> NULL\n\n# Data frame use partial matching and don't complain if a column doesn't exist\ndf <- as.data.frame(tb)\ndf$x\n#> [1] 1 2 3 4 5\ndf$z\n#> NULL\n\nFor this reason we sometimes joke that tibbles are lazy and surly: they do less and complain more.\n\n12.4.3 Subsetting\nLastly, there are some important differences when using [. With data.frames, [ sometimes returns a data.frame, and sometimes returns a vector, which is a common source of bugs. With tibbles, [ always returns another tibble. This can sometimes cause problems when working with older code. If you hit one of those functions, just use as.data.frame() to turn your tibble back to a data.frame.\n\n12.4.4 Exercises\n\nHow can you tell if an object is a tibble? (Hint: try printing mtcars, which is a regular data.frame).\n\nCompare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data.frame behaviors cause you frustration?\n\ndf <- data.frame(abc = 1, xyz = \"a\")\ndf$x\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]\n\n\nIf you have the name of a variable stored in an object, e.g. var <- \"mpg\", how can you extract the reference variable from a tibble?\n\nPractice referring to non-syntactic names in the following data frame by:\n\nExtracting the variable called 1.\nPlotting a scatterplot of 1 vs 2.\nCreating a new column called 3 which is 2 divided by 1.\nRenaming the columns to one, two and three.\n\n\nannoying <- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)\n\n\nWhat does tibble::enframe() do? When might you use it?\nWhat option controls how many additional column names are printed at the footer of a tibble?"
  },
  {
    "objectID": "joins.html",
    "href": "joins.html",
    "title": "13  Joins",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is undergoing heavy restructuring and may be confusing or incomplete. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "joins.html#introduction",
    "href": "joins.html#introduction",
    "title": "13  Joins",
    "section": "\n13.1 Introduction",
    "text": "13.1 Introduction\nWaiting on https://github.com/tidyverse/dplyr/pull/5910\n\nIt’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in.\nAll the verbs in this chapter use a pair of data frames. Fortunately this is enough, since you can combine three data frames by combining two pairs. Sometimes both elements of a pair will be the same data frame. This is needed if, for example, you have a data frame of people, and each person has a reference to their parents.\nThere are two important types of joins. Mutating joins adds new variables to one data frame from matching observations in another. Filtering joins, which filters observations from one data frame based on whether or not they match an observation in another.\nIf you’re familiar with SQL, you should find these ideas very familiar as their instantiation in dplyr is very similar. We’ll point out any important differences as we go. Don’t worry if you’re not familiar with SQL, we’ll back to it in Chapter 24.\n\n13.1.1 Prerequisites\nWe will explore relational data from nycflights13 using the join functions from dplyr.\n\nlibrary(tidyverse)\nlibrary(nycflights13)"
  },
  {
    "objectID": "joins.html#sec-nycflights13-relational",
    "href": "joins.html#sec-nycflights13-relational",
    "title": "13  Joins",
    "section": "\n13.2 nycflights13",
    "text": "13.2 nycflights13\nnycflights13 contains five tibbles : airlines, airports, weather and planes which are all related to the flights data frame that you used in Chapter 4 on data transformation:\n\n\nairlines lets you look up the full carrier name from its abbreviated code:\n\nairlines\n#> # A tibble: 16 × 2\n#>   carrier name                    \n#>   <chr>   <chr>                   \n#> 1 9E      Endeavor Air Inc.       \n#> 2 AA      American Airlines Inc.  \n#> 3 AS      Alaska Airlines Inc.    \n#> 4 B6      JetBlue Airways         \n#> 5 DL      Delta Air Lines Inc.    \n#> 6 EV      ExpressJet Airlines Inc.\n#> # … with 10 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\nairports gives information about each airport, identified by the faa airport code:\n\nairports\n#> # A tibble: 1,458 × 8\n#>   faa   name                             lat   lon   alt    tz dst   tzone      \n#>   <chr> <chr>                          <dbl> <dbl> <dbl> <dbl> <chr> <chr>      \n#> 1 04G   Lansdowne Airport               41.1 -80.6  1044    -5 A     America/Ne…\n#> 2 06A   Moton Field Municipal Airport   32.5 -85.7   264    -6 A     America/Ch…\n#> 3 06C   Schaumburg Regional             42.0 -88.1   801    -6 A     America/Ch…\n#> 4 06N   Randall Airport                 41.4 -74.4   523    -5 A     America/Ne…\n#> 5 09J   Jekyll Island Airport           31.1 -81.4    11    -5 A     America/Ne…\n#> 6 0A9   Elizabethton Municipal Airport  36.4 -82.2  1593    -5 A     America/Ne…\n#> # … with 1,452 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\nplanes gives information about each plane, identified by its tailnum:\n\nplanes\n#> # A tibble: 3,322 × 9\n#>   tailnum  year type                    manuf…¹ model engines seats speed engine\n#>   <chr>   <int> <chr>                   <chr>   <chr>   <int> <int> <int> <chr> \n#> 1 N10156   2004 Fixed wing multi engine EMBRAER EMB-…       2    55    NA Turbo…\n#> 2 N102UW   1998 Fixed wing multi engine AIRBUS… A320…       2   182    NA Turbo…\n#> 3 N103US   1999 Fixed wing multi engine AIRBUS… A320…       2   182    NA Turbo…\n#> 4 N104UW   1999 Fixed wing multi engine AIRBUS… A320…       2   182    NA Turbo…\n#> 5 N10575   2002 Fixed wing multi engine EMBRAER EMB-…       2    55    NA Turbo…\n#> 6 N105UW   1999 Fixed wing multi engine AIRBUS… A320…       2   182    NA Turbo…\n#> # … with 3,316 more rows, and abbreviated variable name ¹​manufacturer\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\nweather gives the weather at each NYC airport for each hour:\n\nweather\n#> # A tibble: 26,115 × 15\n#>   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed wind_gust\n#>   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>      <dbl>     <dbl>\n#> 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4         NA\n#> 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06        NA\n#> 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5         NA\n#> 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7         NA\n#> 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7         NA\n#> 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5         NA\n#> # … with 26,109 more rows, and 4 more variables: precip <dbl>, pressure <dbl>,\n#> #   visib <dbl>, time_hour <dttm>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nThese datasets are connected as follows:\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time).\n\nOne way to show the relationships between the different data frames is with a diagram, as in Figure 13.1. This diagram is a little overwhelming, but it’s simple compared to some you’ll see in the wild! The key to understanding diagrams like this is that you’ll solve real problems by working with pairs of data frames. You don’t need to understand the whole thing; you just need to understand the chain of connections between the two data frames that you’re interested in.\n\n\n\n\nFigure 13.1: Connections between all six data frames in the nycflights package.\n\n\n\n\n\n13.2.1 Exercises\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What data frames would you need to combine?\nWe forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram?\nweather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights?"
  },
  {
    "objectID": "joins.html#keys",
    "href": "joins.html#keys",
    "title": "13  Joins",
    "section": "\n13.3 Keys",
    "text": "13.3 Keys\nThe variables used to connect each pair of data frames are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation. For example, each plane is uniquely identified by its tailnum. In other cases, multiple variables may be needed. For example, to identify an observation in weather you need five variables: year, month, day, hour, and origin.\nThere are two types of keys:\n\nA primary key uniquely identifies an observation in its own data frame. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes data frame.\nA foreign key uniquely identifies an observation in another data frame. For example, flights$tailnum is a foreign key because it appears in the flights data frame where it matches each flight to a unique plane.\n\nA variable can be both a primary key and a foreign key. For example, origin is part of the weather primary key, and is also a foreign key for the airports data frame.\nOnce you’ve identified the primary keys in your data frames, it’s good practice to verify that they do indeed uniquely identify each observation. One way to do that is to count() the primary keys and look for entries where n is greater than one:\n\nplanes |> \n  count(tailnum) |> \n  filter(n > 1)\n#> # A tibble: 0 × 2\n#> # … with 2 variables: tailnum <chr>, n <int>\n#> # ℹ Use `colnames()` to see all variable names\n\nweather |> \n  count(year, month, day, hour, origin) |> \n  filter(n > 1)\n#> # A tibble: 3 × 6\n#>    year month   day  hour origin     n\n#>   <int> <int> <int> <int> <chr>  <int>\n#> 1  2013    11     3     1 EWR        2\n#> 2  2013    11     3     1 JFK        2\n#> 3  2013    11     3     1 LGA        2\n\nSometimes a data frame doesn’t have an explicit primary key: each row is an observation, but no combination of variables reliably identifies it. For example, what’s the primary key in the flights data frame? You might think it would be the date plus the flight or tail number, but neither of those are unique:\n\nflights |> \n  count(year, month, day, flight) |> \n  filter(n > 1)\n#> # A tibble: 29,768 × 5\n#>    year month   day flight     n\n#>   <int> <int> <int>  <int> <int>\n#> 1  2013     1     1      1     2\n#> 2  2013     1     1      3     2\n#> 3  2013     1     1      4     2\n#> 4  2013     1     1     11     3\n#> 5  2013     1     1     15     2\n#> 6  2013     1     1     21     2\n#> # … with 29,762 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nflights |> \n  count(year, month, day, tailnum) |> \n  filter(n > 1)\n#> # A tibble: 64,928 × 5\n#>    year month   day tailnum     n\n#>   <int> <int> <int> <chr>   <int>\n#> 1  2013     1     1 N0EGMQ      2\n#> 2  2013     1     1 N11189      2\n#> 3  2013     1     1 N11536      2\n#> 4  2013     1     1 N11544      3\n#> 5  2013     1     1 N11551      2\n#> 6  2013     1     1 N12540      2\n#> # … with 64,922 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWhen starting to work with this data, we had naively assumed that each flight number would be only used once per day: that would make it much easier to communicate problems with a specific flight. Unfortunately that is not the case! If a data frame lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key.\nA primary key and the corresponding foreign key in another data frame form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines.\n\n13.3.1 Exercises\n\nAdd a surrogate key to flights.\nWe know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that data frame? How would it connect to the existing data frames?\n\nIdentify the keys in the following datasets\n\nLahman::Batting\nbabynames::babynames\nnasaweather::atmos\nfueleconomy::vehicles\nggplot2::diamonds\n\n(You might need to install some packages and read some documentation.)\n\n\nDraw a diagram illustrating the connections between the Batting, People, and Salaries data frames in the Lahman package. Draw another diagram that shows the relationship between People, Managers, AwardsManagers.\nHow would you characterise the relationship between the Batting, Pitching, and Fielding data frames?"
  },
  {
    "objectID": "joins.html#sec-mutating-joins",
    "href": "joins.html#sec-mutating-joins",
    "title": "13  Joins",
    "section": "\n13.4 Mutating joins",
    "text": "13.4 Mutating joins\nThe first tool we’ll look at for combining a pair of data frames is the mutating join. A mutating join allows you to combine variables from two data frames. It first matches observations by their keys, then copies across variables from one data frame to the other.\nLike mutate(), the join functions add variables to the right, so if you have a lot of variables already, the new variables won’t get printed out. For these examples, we’ll make it easier to see what’s going on in the examples by creating a narrower dataset:\n\nflights2 <- flights |> \n  select(year:day, hour, origin, dest, tailnum, carrier)\nflights2\n#> # A tibble: 336,776 × 8\n#>    year month   day  hour origin dest  tailnum carrier\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>  \n#> 1  2013     1     1     5 EWR    IAH   N14228  UA     \n#> 2  2013     1     1     5 LGA    IAH   N24211  UA     \n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA     \n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6     \n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL     \n#> 6  2013     1     1     5 EWR    ORD   N39463  UA     \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n(Remember, when you’re in RStudio, you can also use View() to avoid this problem.)\nImagine you want to add the full airline name to the flights2 data. You can combine the airlines and flights2 data frames with left_join():\n\nflights2 |>\n  select(!origin, !dest) |> \n  left_join(airlines, by = \"carrier\")\n#> # A tibble: 336,776 × 9\n#>    year month   day  hour dest  tailnum carrier origin name                  \n#>   <int> <int> <int> <dbl> <chr> <chr>   <chr>   <chr>  <chr>                 \n#> 1  2013     1     1     5 IAH   N14228  UA      EWR    United Air Lines Inc. \n#> 2  2013     1     1     5 IAH   N24211  UA      LGA    United Air Lines Inc. \n#> 3  2013     1     1     5 MIA   N619AA  AA      JFK    American Airlines Inc.\n#> 4  2013     1     1     5 BQN   N804JB  B6      JFK    JetBlue Airways       \n#> 5  2013     1     1     6 ATL   N668DN  DL      LGA    Delta Air Lines Inc.  \n#> 6  2013     1     1     5 ORD   N39463  UA      EWR    United Air Lines Inc. \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThe result of joining airlines to flights2 is an additional variable: name. This is why we call this type of join a mutating join. In this case, you could get the same result using mutate() and a pair of base R functions, [ and match():\n\nflights2 |>\n  select(!origin, !dest) |> \n  mutate(\n    name = airlines$name[match(carrier, airlines$carrier)]\n  )\n#> # A tibble: 336,776 × 9\n#>    year month   day  hour dest  tailnum carrier origin name                  \n#>   <int> <int> <int> <dbl> <chr> <chr>   <chr>   <chr>  <chr>                 \n#> 1  2013     1     1     5 IAH   N14228  UA      EWR    United Air Lines Inc. \n#> 2  2013     1     1     5 IAH   N24211  UA      LGA    United Air Lines Inc. \n#> 3  2013     1     1     5 MIA   N619AA  AA      JFK    American Airlines Inc.\n#> 4  2013     1     1     5 BQN   N804JB  B6      JFK    JetBlue Airways       \n#> 5  2013     1     1     6 ATL   N668DN  DL      LGA    Delta Air Lines Inc.  \n#> 6  2013     1     1     5 ORD   N39463  UA      EWR    United Air Lines Inc. \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nBut this is hard to generalize when you need to match multiple variables, and takes close reading to figure out the overall intent.\nThe following sections explain, in detail, how mutating joins work. You’ll start by learning a useful visual representation of joins. We’ll then use that to explain the four mutating join functions: the inner join, and the three outer joins. When working with real data, keys don’t always uniquely identify observations, so next we’ll talk about what happens when there isn’t a unique match. Finally, you’ll learn how to tell dplyr which variables are the keys for a given join."
  },
  {
    "objectID": "joins.html#join-types",
    "href": "joins.html#join-types",
    "title": "13  Joins",
    "section": "\n13.5 Join types",
    "text": "13.5 Join types\nTo help you learn how joins work, we’ll use a visual representation:\n\n\n\n\n\n\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     3, \"x3\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     4, \"y3\"\n)\n\nThe coloured column represents the “key” variable: these are used to match the rows between the data frames. The grey column represents the “value” column that is carried along for the ride. In these examples we’ve shown a single key variable, but the idea generalises in a straightforward way to multiple keys and multiple values.\nA join is a way of connecting each row in x to zero, one, or more rows in y. The following diagram shows each potential match as an intersection of a pair of lines.\n\n\n\n\n\nIf you look closely, you’ll notice that we’ve switched the order of the key and value columns in x. This is to emphasize that joins match based on the key; the other columns are just carried along for the ride.\nIn an actual join, matches will be indicated with dots. The number of dots = the number of matches = the number of rows in the output.\n\n\n\n\n\n\n13.5.1 Inner join\nThe simplest type of join is the inner join. An inner join matches pairs of observations whenever their keys are equal:\n\n\n\n\n\n(To be precise, this is an inner equijoin because the keys are matched using the equality operator. Since most joins are equijoins we usually drop that specification.)\nThe output of an inner join is a new data frame that contains the key, the x values, and the y values. We use by to tell dplyr which variable is the key:\n\nx |> \n  inner_join(y, by = \"key\")\n#> # A tibble: 2 × 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2\n\nThe most important property of an inner join is that unmatched rows are not included in the result. This means that generally inner joins are usually not appropriate for use in analysis because it’s too easy to lose observations.\n\n13.5.2 Outer joins\nAn inner join keeps observations that appear in both data frames. An outer join keeps observations that appear in at least one of the data frames. There are three types of outer joins:\n\nA left join keeps all observations in x.\nA right join keeps all observations in y.\nA full join keeps all observations in x and y.\n\nThese joins work by adding an additional “virtual” observation to each data frame. This observation has a key that always matches (if no other key matches), and a value filled with NA.\nGraphically, that looks like:\n\n\n\n\n\nThe most commonly used join is the left join: you use this whenever you look up additional data from another data frame, because it preserves the original observations even when there isn’t a match. The left join should be your default join: use it unless you have a strong reason to prefer one of the others.\nAnother way to depict the different types of joins is with a Venn diagram:\n\n\n\n\n\nHowever, this is not a great representation. It might jog your memory about which join preserves the observations in which data frame, but it suffers from a major limitation: a Venn diagram can’t show what happens when keys don’t uniquely identify an observation.\n\n13.5.3 Duplicate keys\nSo far all the diagrams have assumed that the keys are unique. But that’s not always the case. This section explains what happens when the keys are not unique. There are two possibilities:\nTODO: update for new warnings\n\n\nOne data frame has duplicate keys. This is useful when you want to add in additional information as there is typically a one-to-many relationship.\n\n\n\n\n\nNote that we’ve put the key column in a slightly different position in the output. This reflects that the key is a primary key in y and a foreign key in x.\n\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     2, \"x3\",\n     1, \"x4\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\"\n)\nleft_join(x, y, by = \"key\")\n#> # A tibble: 4 × 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x3    y2   \n#> 4     1 x4    y1\n\n\n\nBoth data frames have duplicate keys. This is usually an error because in neither data frame do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product:\n\n\n\n\n\n\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     2, \"x3\",\n     3, \"x4\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     2, \"y3\",\n     3, \"y4\"\n)\nleft_join(x, y, by = \"key\")\n#> # A tibble: 6 × 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x2    y3   \n#> 4     2 x3    y2   \n#> 5     2 x3    y3   \n#> 6     3 x4    y4\n\n\n\n13.5.4 Defining the key columns\nSo far, the pairs of data frames have always been joined by a single variable, and that variable has the same name in both data frames. That constraint was encoded by by = \"key\". You can use other values for by to connect the data frames in other ways:\n\n\nThe default, by = NULL, uses all variables that appear in both data frames, the so called natural join. For example, the flights and weather data frames match on their common variables: year, month, day, hour and origin.\n\nflights2 |> \n  left_join(weather)\n#> Joining, by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\")\n#> # A tibble: 336,776 × 18\n#>    year month   day  hour origin dest  tailnum carrier  temp  dewp humid wind_…¹\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <dbl> <dbl> <dbl>   <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA       39.0  28.0  64.4     260\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA       39.9  25.0  54.8     250\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA       39.0  27.0  61.6     260\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6       39.0  27.0  61.6     260\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL       39.9  25.0  54.8     260\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA       39.0  28.0  64.4     260\n#> # … with 336,770 more rows, 6 more variables: wind_speed <dbl>,\n#> #   wind_gust <dbl>, precip <dbl>, pressure <dbl>, visib <dbl>,\n#> #   time_hour <dttm>, and abbreviated variable name ¹​wind_dir\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nA character vector, by = \"x\". This is like a natural join, but uses only some of the common variables. For example, flights and planes have year variables, but they mean different things so we only want to join by tailnum.\n\nflights2 |> \n  left_join(planes, by = \"tailnum\")\n#> # A tibble: 336,776 × 16\n#>   year.x month   day  hour origin dest  tailnum carrier year.y type      manuf…¹\n#>    <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>    <int> <chr>     <chr>  \n#> 1   2013     1     1     5 EWR    IAH   N14228  UA        1999 Fixed wi… BOEING \n#> 2   2013     1     1     5 LGA    IAH   N24211  UA        1998 Fixed wi… BOEING \n#> 3   2013     1     1     5 JFK    MIA   N619AA  AA        1990 Fixed wi… BOEING \n#> 4   2013     1     1     5 JFK    BQN   N804JB  B6        2012 Fixed wi… AIRBUS \n#> 5   2013     1     1     6 LGA    ATL   N668DN  DL        1991 Fixed wi… BOEING \n#> 6   2013     1     1     5 EWR    ORD   N39463  UA        2012 Fixed wi… BOEING \n#> # … with 336,770 more rows, 5 more variables: model <chr>, engines <int>,\n#> #   seats <int>, speed <int>, engine <chr>, and abbreviated variable name\n#> #   ¹​manufacturer\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nNote that the year variables (which appear in both input data frames, but are not constrained to be equal) are disambiguated in the output with a suffix.\n\n\nA named character vector: by = c(\"a\" = \"b\"). This will match variable a in data frame x to variable b in data frame y. The variables from x will be used in the output.\nFor example, if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to:\n\nflights2 |> \n  left_join(airports, c(\"dest\" = \"faa\"))\n#> # A tibble: 336,776 × 15\n#>    year month   day  hour origin dest  tailnum carrier name      lat   lon   alt\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <chr>   <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA      George…  30.0 -95.3    97\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA      George…  30.0 -95.3    97\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA      Miami …  25.8 -80.3     8\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6      <NA>     NA    NA      NA\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL      Hartsf…  33.6 -84.4  1026\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA      Chicag…  42.0 -87.9   668\n#> # … with 336,770 more rows, and 3 more variables: tz <dbl>, dst <chr>,\n#> #   tzone <chr>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nflights2 |> \n  left_join(airports, c(\"origin\" = \"faa\"))\n#> # A tibble: 336,776 × 15\n#>    year month   day  hour origin dest  tailnum carrier name      lat   lon   alt\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <chr>   <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA      Newark…  40.7 -74.2    18\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA      La Gua…  40.8 -73.9    22\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA      John F…  40.6 -73.8    13\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6      John F…  40.6 -73.8    13\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL      La Gua…  40.8 -73.9    22\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA      Newark…  40.7 -74.2    18\n#> # … with 336,770 more rows, and 3 more variables: tz <dbl>, dst <chr>,\n#> #   tzone <chr>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\n13.5.5 Exercises\n\n\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:\n\nairports |>\n  semi_join(flights, c(\"faa\" = \"dest\")) |>\n  ggplot(aes(lon, lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()\n\n(Don’t worry if you don’t understand what semi_join() does — you’ll learn about it next.)\nYou might want to use the size or colour of the points to display the average delay for each airport.\n\nAdd the location of the origin and destination (i.e. the lat and lon) to flights.\nIs there a relationship between the age of a plane and its delays?\nWhat weather conditions make it more likely to see a delay?\nWhat happened on June 13 2013? Display the spatial pattern of delays, and then use Google to cross-reference with the weather."
  },
  {
    "objectID": "joins.html#non-equi-joins",
    "href": "joins.html#non-equi-joins",
    "title": "13  Joins",
    "section": "\n13.6 Non-equi joins",
    "text": "13.6 Non-equi joins\njoin_by()\nRolling joins\nOverlap joins"
  },
  {
    "objectID": "joins.html#sec-filtering-joins",
    "href": "joins.html#sec-filtering-joins",
    "title": "13  Joins",
    "section": "\n13.7 Filtering joins",
    "text": "13.7 Filtering joins\nFiltering joins match observations in the same way as mutating joins, but affect the observations, not the variables. There are two types:\n\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\n\nanti_join(x, y) drops all observations in x that have a match in y.\n\nSemi-joins are useful for matching filtered summary data frames back to the original rows. For example, imagine you’ve found the top ten most popular destinations:\n\ntop_dest <- flights |>\n  count(dest, sort = TRUE) |>\n  head(10)\ntop_dest\n#> # A tibble: 10 × 2\n#>   dest      n\n#>   <chr> <int>\n#> 1 ORD   17283\n#> 2 ATL   17215\n#> 3 LAX   16174\n#> 4 BOS   15508\n#> 5 MCO   14082\n#> 6 CLT   14064\n#> # … with 4 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nNow you want to find each flight that went to one of those destinations. You could construct a filter yourself:\n\nflights |> \n  filter(dest %in% top_dest$dest)\n#> # A tibble: 141,145 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      542         540       2     923     850      33 AA     \n#> 2  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 3  2013     1     1      554         558      -4     740     728      12 UA     \n#> 4  2013     1     1      555         600      -5     913     854      19 B6     \n#> 5  2013     1     1      557         600      -3     838     846      -8 B6     \n#> 6  2013     1     1      558         600      -2     753     745       8 AA     \n#> # … with 141,139 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nBut it’s difficult to extend that approach to multiple variables. For example, imagine that you’d found the 10 days with highest average delays. How would you construct the filter statement that used year, month, and day to match it back to flights?\nInstead you can use a semi-join, which connects the two data frames like a mutating join, but instead of adding new columns, only keeps the rows in x that have a match in y:\n\nflights |> \n  semi_join(top_dest)\n#> Joining, by = \"dest\"\n#> # A tibble: 141,145 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      542         540       2     923     850      33 AA     \n#> 2  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 3  2013     1     1      554         558      -4     740     728      12 UA     \n#> 4  2013     1     1      555         600      -5     913     854      19 B6     \n#> 5  2013     1     1      557         600      -3     838     846      -8 B6     \n#> 6  2013     1     1      558         600      -2     753     745       8 AA     \n#> # … with 141,139 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nGraphically, a semi-join looks like this:\n\n\n\n\n\nOnly the existence of a match is important; it doesn’t matter which observation is matched. This means that filtering joins never duplicate rows like mutating joins do:\n\n\n\n\n\nThe inverse of a semi-join is an anti-join. An anti-join keeps the rows that don’t have a match:\n\n\n\n\n\nAnti-joins are useful for diagnosing join mismatches. For example, when connecting flights and planes, you might be interested to know that there are many flights that don’t have a match in planes:\n\nflights |>\n  anti_join(planes, by = \"tailnum\") |>\n  count(tailnum, sort = TRUE)\n#> # A tibble: 722 × 2\n#>   tailnum     n\n#>   <chr>   <int>\n#> 1 <NA>     2512\n#> 2 N725MQ    575\n#> 3 N722MQ    513\n#> 4 N723MQ    507\n#> 5 N713MQ    483\n#> 6 N735MQ    396\n#> # … with 716 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n13.7.1 Exercises\n\nWhat does it mean for a flight to have a missing tailnum? What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.)\nFilter flights to only show flights with planes that have flown at least 100 flights.\nCombine fueleconomy::vehicles and fueleconomy::common to find only the records for the most common models.\nFind the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns?\nWhat does anti_join(flights, airports, by = c(\"dest\" = \"faa\")) tell you? What does anti_join(airports, flights, by = c(\"faa\" = \"dest\")) tell you?\nYou might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above."
  },
  {
    "objectID": "joins.html#join-problems",
    "href": "joins.html#join-problems",
    "title": "13  Joins",
    "section": "\n13.8 Join problems",
    "text": "13.8 Join problems\nThe data you’ve been working with in this chapter has been cleaned up so that you’ll have as few problems as possible. Your own data is unlikely to be so nice, so there are a few things that you should do with your own data to make your joins go smoothly.\n\n\nStart by identifying the variables that form the primary key in each data frame. You should usually do this based on your understanding of the data, not empirically by looking for a combination of variables that give a unique identifier. If you just look for variables without thinking about what they mean, you might get (un)lucky and find a combination that’s unique in your current data but the relationship might not be true in general.\nFor example, the altitude and longitude uniquely identify each airport, but they are not good identifiers!\n\nairports |> count(alt, lon) |> filter(n > 1)\n#> # A tibble: 0 × 3\n#> # … with 3 variables: alt <dbl>, lon <dbl>, n <int>\n#> # ℹ Use `colnames()` to see all variable names\n\n\nCheck that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation!\n\nCheck that your foreign keys match primary keys in another data frame. The best way to do this is with an anti_join(). It’s common for keys not to match because of data entry errors. Fixing these is often a lot of work.\nIf you do have missing keys, you’ll need to be thoughtful about your use of inner vs. outer joins, carefully considering whether or not you want to drop rows that don’t have a match.\n\n\nBe aware that simply checking the number of rows before and after the join is not sufficient to ensure that your join has gone smoothly. If you have an inner join with duplicate keys in both data frames, you might get unlucky as the number of dropped rows might exactly equal the number of duplicated rows!"
  },
  {
    "objectID": "logicals.html",
    "href": "logicals.html",
    "title": "14  Logical vectors",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "logicals.html#introduction",
    "href": "logicals.html#introduction",
    "title": "14  Logical vectors",
    "section": "\n14.1 Introduction",
    "text": "14.1 Introduction\nIn this chapter, you’ll learn tools for working with logical vectors. Logical vectors are the simplest type of vector because each element can only be one of three possible values: TRUE, FALSE, and NA. It’s relatively rare to find logical vectors in your raw data, but you’ll create and manipulate in the course of almost every analysis.\nWe’ll begin by discussing the most common way of creating logical vectors: with numeric comparisons. Then you’ll learn about how you can use Boolean algebra to combine different logical vectors, as well as some useful summaries. We’ll finish off with some tools for making conditional changes, and a useful function for turning logical vectors into groups.\n\n14.1.1 Prerequisites\nMost of the functions you’ll learn about in this chapter are provided by base R, so we don’t need the tidyverse, but we’ll still load it so we can use mutate(), filter(), and friends to work with data frames. We’ll also continue to draw examples from the nycflights13 dataset.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nHowever, as we start to cover more tools, there won’t always be a perfect real example. So we’ll start making up some dummy data with c():\n\nx <- c(1, 2, 3, 5, 7, 11, 13)\nx * 2\n#> [1]  2  4  6 10 14 22 26\n\nThis makes it easier to explain individual functions at the cost of making it harder to see how it might apply to your data problems. Just remember that any manipulation we do to a free-floating vector, you can do to a variable inside data frame with mutate() and friends.\n\ndf <- tibble(x)\ndf |> \n  mutate(y = x *  2)\n#> # A tibble: 7 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     2\n#> 2     2     4\n#> 3     3     6\n#> 4     5    10\n#> 5     7    14\n#> 6    11    22\n#> # … with 1 more row\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "logicals.html#comparisons",
    "href": "logicals.html#comparisons",
    "title": "14  Logical vectors",
    "section": "\n14.2 Comparisons",
    "text": "14.2 Comparisons\nA very common way to create a logical vector is via a numeric comparison with <, <=, >, >=, !=, and ==. So far, we’ve mostly created logical variables transiently within filter() — they are computed, used, and then thrown away. For example, the following filter finds all daytime departures that leave roughly on time:\n\nflights |> \n  filter(dep_time > 600 & dep_time < 2000 & abs(arr_delay) < 20)\n#> # A tibble: 172,286 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      601         600       1     844     850      -6 B6     \n#> 2  2013     1     1      602         610      -8     812     820      -8 DL     \n#> 3  2013     1     1      602         605      -3     821     805      16 MQ     \n#> 4  2013     1     1      606         610      -4     858     910     -12 AA     \n#> 5  2013     1     1      606         610      -4     837     845      -8 DL     \n#> 6  2013     1     1      607         607       0     858     915     -17 UA     \n#> # … with 172,280 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nIt’s useful to know that this is a shortcut and you can explicitly create the underlying logical variables with mutate():\n\nflights |> \n  mutate(\n    daytime = dep_time > 600 & dep_time < 2000,\n    approx_ontime = abs(arr_delay) < 20,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 4\n#>   dep_time arr_delay daytime approx_ontime\n#>      <int>     <dbl> <lgl>   <lgl>        \n#> 1      517        11 FALSE   TRUE         \n#> 2      533        20 FALSE   FALSE        \n#> 3      542        33 FALSE   FALSE        \n#> 4      544       -18 FALSE   TRUE         \n#> 5      554       -25 FALSE   FALSE        \n#> 6      554        12 FALSE   TRUE         \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThis is particularly useful for more complicated logic because naming the intermediate steps makes it easier to both read your code and check that each step has been computed correctly.\nAll up, the initial filter is equivalent to:\n\nflights |> \n  mutate(\n    daytime = dep_time > 600 & dep_time < 2000,\n    approx_ontime = abs(arr_delay) < 20,\n  ) |> \n  filter(daytime & approx_ontime)\n\n\n14.2.1 Floating point comparison\nBeware of using == with numbers. For example, it looks like this vector contains the numbers 1 and 2:\n\nx <- c(1 / 49 * 49, sqrt(2) ^ 2)\nx\n#> [1] 1 2\n\nBut if you test them for equality, you get FALSE:\n\nx == c(1, 2)\n#> [1] FALSE FALSE\n\nWhat’s going on? Computers store numbers with a fixed number of decimal places so there’s no way to exactly represent 1/49 or sqrt(2) and subsequent computations will be very slightly off. We can see the exact values by calling print() with the the digits1 argument:\n\nprint(x, digits = 16)\n#> [1] 0.9999999999999999 2.0000000000000004\n\nYou can see why R defaults to rounding these numbers; they really are very close to what you expect.\nNow that you’ve seen why == is failing, what can you do about it? One option is to use dplyr::near() which ignores small differences:\n\nnear(x, c(1, 2))\n#> [1] TRUE TRUE\n\n\n14.2.2 Missing values\nMissing values represent the unknown so they are “contagious”: almost any operation involving an unknown value will also be unknown:\n\nNA > 5\n#> [1] NA\n10 == NA\n#> [1] NA\n\nThe most confusing result is this one:\n\nNA == NA\n#> [1] NA\n\nIt’s easiest to understand why this is true if we artificially supply a little more context:\n\n# Let x be Mary's age. We don't know how old she is.\nx <- NA\n\n# Let y be John's age. We don't know how old he is.\ny <- NA\n\n# Are John and Mary the same age?\nx == y\n#> [1] NA\n# We don't know!\n\nSo if you want to find all flights with dep_time is missing, the following code doesn’t work because dep_time == NA will yield a NA for every single row, and filter() automatically drops missing values:\n\nflights |> \n  filter(dep_time == NA)\n#> # A tibble: 0 × 19\n#> # … with 19 variables: year <int>, month <int>, day <int>, dep_time <int>,\n#> #   sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,\n#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,\n#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#> #   hour <dbl>, minute <dbl>, time_hour <dttm>\n#> # ℹ Use `colnames()` to see all variable names\n\nInstead we’ll need a new tool: is.na().\n\n14.2.3 is.na()\n\nis.na(x) works with any type of vector and returns TRUE for missing values and FALSE for everything else:\n\nis.na(c(TRUE, NA, FALSE))\n#> [1] FALSE  TRUE FALSE\nis.na(c(1, NA, 3))\n#> [1] FALSE  TRUE FALSE\nis.na(c(\"a\", NA, \"b\"))\n#> [1] FALSE  TRUE FALSE\n\nWe can use is.na() to find all the rows with a missing dep_time:\n\nflights |> \n  filter(is.na(dep_time))\n#> # A tibble: 8,255 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1       NA        1630      NA      NA    1815      NA EV     \n#> 2  2013     1     1       NA        1935      NA      NA    2240      NA AA     \n#> 3  2013     1     1       NA        1500      NA      NA    1825      NA AA     \n#> 4  2013     1     1       NA         600      NA      NA     901      NA B6     \n#> 5  2013     1     2       NA        1540      NA      NA    1747      NA EV     \n#> 6  2013     1     2       NA        1620      NA      NA    1746      NA EV     \n#> # … with 8,249 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nis.na() can also be useful in arrange(). arrange() usually puts all the missing values at the end but you can override this default by first sorting by is.na():\n\nflights |> \n  filter(month == 1, day == 1) |> \n  arrange(dep_time)\n#> # A tibble: 842 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 836 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nflights |> \n  filter(month == 1, day == 1) |> \n  arrange(desc(is.na(dep_time)), dep_time)\n#> # A tibble: 842 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1       NA        1630      NA      NA    1815      NA EV     \n#> 2  2013     1     1       NA        1935      NA      NA    2240      NA AA     \n#> 3  2013     1     1       NA        1500      NA      NA    1825      NA AA     \n#> 4  2013     1     1       NA         600      NA      NA     901      NA B6     \n#> 5  2013     1     1      517         515       2     830     819      11 UA     \n#> 6  2013     1     1      533         529       4     850     830      20 UA     \n#> # … with 836 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n14.2.4 Exercises\n\nHow does dplyr::near() work? Type near to see the source code.\nUse mutate(), is.na(), and count() together to describe how the missing values in dep_time, sched_dep_time and dep_delay are connected."
  },
  {
    "objectID": "logicals.html#boolean-algebra",
    "href": "logicals.html#boolean-algebra",
    "title": "14  Logical vectors",
    "section": "\n14.3 Boolean algebra",
    "text": "14.3 Boolean algebra\nOnce you have multiple logical vectors, you can combine them together using Boolean algebra. In R, & is “and”, | is “or”, and ! is “not”, and xor() is exclusive or2. Figure 14.1 shows the complete set of Boolean operations and how they work.\n\n\n\n\nFigure 14.1: The complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects.\n\n\n\n\nAs well as & and |, R also has && and ||. Don’t use them in dplyr functions! These are called short-circuiting operators and only ever return a single TRUE or FALSE. They’re important for programming and you’ll learn more about them in Section 27.4.\n\n14.3.1 Missing values\nThe rules for missing values in Boolean algebra are a little tricky to explain because they seem inconsistent at first glance:\n\ndf <- tibble(x = c(TRUE, FALSE, NA))\n\ndf |> \n  mutate(\n    and = x & NA,\n    or = x | NA\n  )\n#> # A tibble: 3 × 3\n#>   x     and   or   \n#>   <lgl> <lgl> <lgl>\n#> 1 TRUE  NA    TRUE \n#> 2 FALSE FALSE NA   \n#> 3 NA    NA    NA\n\nTo understand what’s going on, think about NA | TRUE. A missing value in a logical vector means that the value could either be TRUE or FALSE. TRUE | TRUE and FALSE | TRUE are both TRUE, so NA | TRUE must also be TRUE. Similar reasoning applies with NA & FALSE.\n\n14.3.2 Order of operations\nNote that the order of operations doesn’t work like English. Take the following code finds all flights that departed in November or December:\n\nflights |> \n   filter(month == 11 | month == 12)\n\nYou might be tempted to write it like you’d say in English: “find all flights that departed in November or December”:\n\nflights |> \n   filter(month == 11 | 12)\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      533         529       4     850     830      20 UA     \n#> 3  2013     1     1      542         540       2     923     850      33 AA     \n#> 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n#> 5  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 6  2013     1     1      554         558      -4     740     728      12 UA     \n#> # … with 336,770 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThis code doesn’t error but it also doesn’t seem to have worked. What’s going on? Here R first evaluates month == 11 creating a logical vector, which we call nov. It computes nov | 12. When you use a number with a logical operator it converts everything apart from 0 to TRUE, so this is equivalent to nov | TRUE which will always be TRUE, so every row will be selected:\n\nflights |> \n  mutate(\n    nov = month == 11,\n    final = nov | 12,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 3\n#>   month nov   final\n#>   <int> <lgl> <lgl>\n#> 1     1 FALSE TRUE \n#> 2     1 FALSE TRUE \n#> 3     1 FALSE TRUE \n#> 4     1 FALSE TRUE \n#> 5     1 FALSE TRUE \n#> 6     1 FALSE TRUE \n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n14.3.3 %in%\n\nAn easy way to avoid the problem of getting your ==s and |s in the right order is to use %in%. x %in% y returns a logical vector the same length as x that is TRUE whenever a value in x is anywhere in y .\n\n1:12 %in% c(1, 5, 11)\n#>  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\nletters[1:10] %in% c(\"a\", \"e\", \"i\", \"o\", \"u\")\n#>  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\nSo to find all flights in November and December we could write:\n\nflights |> \n  filter(month %in% c(11, 12))\n\nNote that %in% obeys different rules for NA to ==, as NA %in% NA is TRUE.\n\nc(1, 2, NA) == NA\n#> [1] NA NA NA\nc(1, 2, NA) %in% NA\n#> [1] FALSE FALSE  TRUE\n\nThis can make for a useful shortcut:\n\nflights |> \n  filter(dep_time %in% c(NA, 0800))\n#> # A tibble: 8,803 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      800         800       0    1022    1014       8 DL     \n#> 2  2013     1     1      800         810     -10     949     955      -6 MQ     \n#> 3  2013     1     1       NA        1630      NA      NA    1815      NA EV     \n#> 4  2013     1     1       NA        1935      NA      NA    2240      NA AA     \n#> 5  2013     1     1       NA        1500      NA      NA    1825      NA AA     \n#> 6  2013     1     1       NA         600      NA      NA     901      NA B6     \n#> # … with 8,797 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n14.3.4 Exercises\n\nFind all flights where arr_delay is missing but dep_delay is not. Find all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is.\nHow many flights have a missing dep_time? What other variables are missing in these rows? What might these rows represent?\nAssuming that a missing dep_time implies that a flight is cancelled, look at the number of cancelled flights per day. Is there a pattern? Is there a connection between the proportion of cancelled flights and average delay of non-cancelled flights?"
  },
  {
    "objectID": "logicals.html#sec-logical-summaries",
    "href": "logicals.html#sec-logical-summaries",
    "title": "14  Logical vectors",
    "section": "\n14.4 Summaries",
    "text": "14.4 Summaries\nThe following sections describe some useful techniques for summarizing logical vectors. As well as functions that only work specifically with logical vectors, you can also use functions that work with numeric vectors.\n\n14.4.1 Logical summaries\nThere are two main logical summaries: any() and all(). any(x) is the equivalent of |; it’ll return TRUE if there are any TRUE’s in x. all(x) is equivalent of &; it’ll return TRUE only if all values of x are TRUE’s. Like all summary functions, they’ll return NA if there are any missing values present, and as usual you can make the missing values go away with na.rm = TRUE.\nFor example, we could use all() to find out if there were days where every flight was delayed:\n\nflights |> \n  group_by(year, month, day) |> \n  summarise(\n    all_delayed = all(arr_delay >= 0, na.rm = TRUE),\n    any_delayed = any(arr_delay >= 0, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day all_delayed any_delayed\n#>   <int> <int> <int> <lgl>       <lgl>      \n#> 1  2013     1     1 FALSE       TRUE       \n#> 2  2013     1     2 FALSE       TRUE       \n#> 3  2013     1     3 FALSE       TRUE       \n#> 4  2013     1     4 FALSE       TRUE       \n#> 5  2013     1     5 FALSE       TRUE       \n#> 6  2013     1     6 FALSE       TRUE       \n#> # … with 359 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nIn most cases, however, any() and all() are a little too crude, and it would be nice to be able to get a little more detail about how many values are TRUE or FALSE. That leads us to the numeric summaries.\n\n14.4.2 Numeric summaries\nWhen you use a logical vector in a numeric context, TRUE becomes 1 and FALSE becomes 0. This makes sum() and mean() very useful with logical vectors because sum(x) will give the number of TRUEs and mean(x) the proportion of TRUEs. That lets us see the distribution of delays across the days of the year as shown in Figure 14.2.\n\nflights |> \n  group_by(year, month, day) |> \n  summarise(\n    prop_delayed = mean(arr_delay > 0, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |> \n  ggplot(aes(prop_delayed)) + \n  geom_histogram(binwidth = 0.05)\n\n\n\nFigure 14.2: A histogram showing the proportion of delayed flights each day.\n\n\n\n\nOr we could ask how many flights left before 5am, which are often flights that were delayed from the previous day:\n\nflights |> \n  group_by(year, month, day) |> \n  summarise(\n    n_early = sum(dep_time < 500, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |> \n  arrange(desc(n_early))\n#> # A tibble: 365 × 4\n#>    year month   day n_early\n#>   <int> <int> <int>   <int>\n#> 1  2013     6    28      32\n#> 2  2013     4    10      30\n#> 3  2013     7    28      30\n#> 4  2013     3    18      29\n#> 5  2013     7     7      29\n#> 6  2013     7    10      29\n#> # … with 359 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n14.4.3 Logical subsetting\nThere’s one final use for logical vectors in summaries: you can use a logical vector to filter a single variable to a subset of interest. This makes use of the base [ (pronounced subset) operator, which you’ll learn more about this in Section 28.4.5.\nImagine we wanted to look at the average delay just for flights that were actually delayed. One way to do so would be to first filter the flights:\n\nflights |> \n  filter(arr_delay > 0) |> \n  group_by(year, month, day) |> \n  summarise(\n    behind = mean(arr_delay),\n    n = n(),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day behind     n\n#>   <int> <int> <int>  <dbl> <int>\n#> 1  2013     1     1   32.5   461\n#> 2  2013     1     2   32.0   535\n#> 3  2013     1     3   27.7   460\n#> 4  2013     1     4   28.3   297\n#> 5  2013     1     5   22.6   238\n#> 6  2013     1     6   24.4   381\n#> # … with 359 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThis works, but what if we wanted to also compute the average delay for flights that arrived early? We’d need to perform a separate filter step, and then figure out how to combine the two data frames together3. Instead you could use [ to perform an inline filtering: arr_delay[arr_delay > 0] will yield only the positive arrival delays.\nThis leads to:\n\nflights |> \n  group_by(year, month, day) |> \n  summarise(\n    behind = mean(arr_delay[arr_delay > 0], na.rm = TRUE),\n    ahead = mean(arr_delay[arr_delay < 0], na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 6\n#>    year month   day behind ahead     n\n#>   <int> <int> <int>  <dbl> <dbl> <int>\n#> 1  2013     1     1   32.5 -12.5   842\n#> 2  2013     1     2   32.0 -14.3   943\n#> 3  2013     1     3   27.7 -18.2   914\n#> 4  2013     1     4   28.3 -17.0   915\n#> 5  2013     1     5   22.6 -14.0   720\n#> 6  2013     1     6   24.4 -13.6   832\n#> # … with 359 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAlso note the difference in the group size: in the first chunk n() gives the number of delayed flights per day; in the second, n() gives the total number of flights.\n\n14.4.4 Exercises\n\nWhat will sum(is.na(x)) tell you? How about mean(is.na(x))?\nWhat does prod() return when applied to a logical vector? What logical summary function is it equivalent to? What does min() return applied to a logical vector? What logical summary function is it equivalent to? Read the documentation and perform a few experiments."
  },
  {
    "objectID": "logicals.html#conditional-transformations",
    "href": "logicals.html#conditional-transformations",
    "title": "14  Logical vectors",
    "section": "\n14.5 Conditional transformations",
    "text": "14.5 Conditional transformations\nOne of the most powerful features of logical vectors are their use for conditional transformations, i.e. doing one thing for condition x, and something different for condition y. There are two important tools for this: if_else() and case_when().\n\n14.5.1 if_else()\n\nIf you want to use one value when a condition is true and another value when it’s FALSE, you can use dplyr::if_else()4. You’ll always use the first three argument of if_else(). The first argument, condition, is a logical vector, the second, true, gives the output when the condition is true, and the third, false, gives the output if the condition is false.\nLet’s begin with a simple example of labeling a numeric vector as either “+ve” or “-ve”:\n\nx <- c(-3:3, NA)\nif_else(x > 0, \"+ve\", \"-ve\")\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" NA\n\nThere’s an optional fourth argument, missing which will be used if the input is NA:\n\nif_else(x > 0, \"+ve\", \"-ve\", \"???\")\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nYou can also use vectors for the the true and false arguments. For example, this allows us to create a minimal implementation of abs():\n\nif_else(x < 0, -x, x)\n#> [1]  3  2  1  0  1  2  3 NA\n\nSo far all the arguments have used the same vectors, but you can of course mix and match. For example, you could implement a simple version of coalesce() like this:\n\nx1 <- c(NA, 1, 2, NA)\ny1 <- c(3, NA, 4, 6)\nif_else(is.na(x1), y1, x1)\n#> [1] 3 1 2 6\n\nYou might have noticed a small infelicity in our labeling: zero is neither positive nor negative. We could resolve this by adding an additional if_else():\n\nif_else(x == 0, \"0\", if_else(x < 0, \"-ve\", \"+ve\"), \"???\")\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nThis is already a little hard to read, and you can imagine it would only get harder if you have more conditions. Instead, you can switch to dplyr::case_when().\n\n14.5.2 case_when()\n\ndplyr’s case_when() is inspired by SQL’s CASE statement and provides a flexible way of performing different computations for different computations. It has a special syntax that unfortunately looks like nothing else you’ll use in the tidyverse. It takes pairs that look like condition ~ output. condition must be a logical vector; when it’s TRUE, output will be used.\nThis means we could recreate our previous nested if_else() as follows:\n\ncase_when(\n  x == 0   ~ \"0\",\n  x < 0    ~ \"-ve\", \n  x > 0    ~ \"+ve\",\n  is.na(x) ~ \"???\"\n)\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nThis is more code, but it’s also more explicit.\nTo explain how case_when() works, lets explore some simpler cases. If none of the cases match, the output gets an NA:\n\ncase_when(\n  x < 0 ~ \"-ve\",\n  x > 0 ~ \"+ve\"\n)\n#> [1] \"-ve\" \"-ve\" \"-ve\" NA    \"+ve\" \"+ve\" \"+ve\" NA\n\nIf you want to create a “default”/catch all value, use TRUE on the left hand side:\n\ncase_when(\n  x < 0 ~ \"-ve\",\n  x > 0 ~ \"+ve\",\n  TRUE ~ \"???\"\n)\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"???\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nAnd note that if multiple conditions match, only the first will be used:\n\ncase_when(\n  x > 0 ~ \"+ve\",\n  x > 3 ~ \"big\"\n)\n#> [1] NA    NA    NA    NA    \"+ve\" \"+ve\" \"+ve\" NA\n\nJust like with if_else() you can use variables on both sides of the ~ and you can mix and match variables as needed for your problem. For example, we could use case_when() to provide some human readable labels for the arrival delay:\n\nflights |> \n  mutate(\n    status = case_when(\n      is.na(arr_delay)      ~ \"cancelled\",\n      arr_delay > 60        ~ \"very late\",\n      arr_delay > 15        ~ \"late\",\n      abs(arr_delay) <= 15  ~ \"on time\",\n      arr_delay < -15       ~ \"early\",\n      arr_delay < -30       ~ \"very early\",\n    ),\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 2\n#>   arr_delay status \n#>       <dbl> <chr>  \n#> 1        11 on time\n#> 2        20 late   \n#> 3        33 late   \n#> 4       -18 early  \n#> 5       -25 early  \n#> 6        12 on time\n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "logicals.html#sec-groups-from-logical",
    "href": "logicals.html#sec-groups-from-logical",
    "title": "14  Logical vectors",
    "section": "\n14.6 Making groups",
    "text": "14.6 Making groups\nBefore we move on to the next chapter, we want to show you one last trick that’s useful for grouping data. Sometimes you want to start a new group every time some event occurs. For example, when you’re looking at website data, it’s common to want to break up events into sessions, where a session is defined as a gap of more than x minutes since the last activity.\nHere’s some made up data that illustrates the problem. So far computed the time lag between the events, and figured out if there’s a gap that’s big enough to qualify:\n\nevents <- tibble(\n  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)\n)\nevents <- events |> \n  mutate(\n    diff = time - lag(time, default = first(time)),\n    gap = diff >= 5\n  )\nevents\n#> # A tibble: 14 × 3\n#>    time  diff gap  \n#>   <dbl> <dbl> <lgl>\n#> 1     0     0 FALSE\n#> 2     1     1 FALSE\n#> 3     2     1 FALSE\n#> 4     3     1 FALSE\n#> 5     5     2 FALSE\n#> 6    10     5 TRUE \n#> # … with 8 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nBut how do we go from that logical vector to something that we can group_by()? consecutive_id() comes to the rescue:\n\nconsecutive_id <- function(...) {\n  ellipsis::check_dots_unnamed()\n  data <- vctrs::data_frame(..., .name_repair = \"minimal\")\n\n  out <- vctrs::vec_identify_runs(data)\n  attr(out, \"n\") <- NULL\n\n  out\n}\nevents |> mutate(\n  group = consecutive_id(gap)\n)\n#> # A tibble: 14 × 4\n#>    time  diff gap   group\n#>   <dbl> <dbl> <lgl> <int>\n#> 1     0     0 FALSE     1\n#> 2     1     1 FALSE     1\n#> 3     2     1 FALSE     1\n#> 4     3     1 FALSE     1\n#> 5     5     2 FALSE     1\n#> 6    10     5 TRUE      2\n#> # … with 8 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nconsecutive_id() starts a new group every time one of its arguments changes. That makes it useful both here, with logical vectors, and in many other place. For example, inspired by this stackoverflow question, imagine you have a data frame with a bunch of repeated values:\n\ndf <- tibble(\n  x = c(\"a\", \"a\", \"a\", \"b\", \"c\", \"c\", \"d\", \"e\", \"a\", \"a\", \"b\", \"b\"),\n  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4, 8, 10, 199)\n)\ndf\n#> # A tibble: 12 × 2\n#>   x         y\n#>   <chr> <dbl>\n#> 1 a         1\n#> 2 a         2\n#> 3 a         3\n#> 4 b         2\n#> 5 c         4\n#> 6 c         1\n#> # … with 6 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou want to keep the first row from each repeated x. That’s easier to express with a combination of consecutive_id() and slice_head():\n\ndf |> \n  group_by(id = consecutive_id(x)) |> \n  slice_head(n = 1)\n#> # A tibble: 7 × 3\n#> # Groups:   id [7]\n#>   x         y    id\n#>   <chr> <dbl> <int>\n#> 1 a         1     1\n#> 2 b         2     2\n#> 3 c         4     3\n#> 4 d         3     4\n#> 5 e         9     5\n#> 6 a         4     6\n#> # … with 1 more row\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "numbers.html",
    "href": "numbers.html",
    "title": "15  Numbers",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "numbers.html#introduction",
    "href": "numbers.html#introduction",
    "title": "15  Numbers",
    "section": "\n15.1 Introduction",
    "text": "15.1 Introduction\nIn this chapter, you’ll learn useful tools for creating and manipulating numeric vectors. We’ll start by going into a little more detail of count() before diving into various numeric transformations. You’ll then learn about more general transformations that can be applied to other types of vector, but are often used with numeric vectors. Then you’ll learn about a few more useful summaries and how they can also be used with mutate().\n\n15.1.1 Prerequisites\nThis chapter mostly uses functions from base R, which are available without loading any packages. But we still need the tidyverse because we’ll use these base R functions inside of tidyverse functions like mutate() and filter(). Like in the last chapter, we’ll use real examples from nycflights13, as well as toy examples made with c() and tribble().\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n15.1.2 Counts\nIt’s surprising how much data science you can do with just counts and a little basic arithmetic, so dplyr strives to make counting as easy as possible with count(). This function is great for quick exploration and checks during analysis:\n\nflights |> count(dest)\n#> # A tibble: 105 × 2\n#>   dest      n\n#>   <chr> <int>\n#> 1 ABQ     254\n#> 2 ACK     265\n#> 3 ALB     439\n#> 4 ANC       8\n#> 5 ATL   17215\n#> 6 AUS    2439\n#> # … with 99 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n(Despite the advice in Chapter 7, we usually put count() on a single line because it’s usually used at the console for a quick check that a calculation is working as expected.)\nIf you want to see the most common values add sort = TRUE:\n\nflights |> count(dest, sort = TRUE)\n#> # A tibble: 105 × 2\n#>   dest      n\n#>   <chr> <int>\n#> 1 ORD   17283\n#> 2 ATL   17215\n#> 3 LAX   16174\n#> 4 BOS   15508\n#> 5 MCO   14082\n#> 6 CLT   14064\n#> # … with 99 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAnd remember that if you want to see all the values, you can use |> View() or |> print(n = Inf).\nYou can perform the same computation “by hand” with group_by(), summarise() and n(). This is useful because it allows you to compute other summaries at the same time:\n\nflights |> \n  group_by(dest) |> \n  summarise(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  )\n#> # A tibble: 105 × 3\n#>   dest      n delay\n#>   <chr> <int> <dbl>\n#> 1 ABQ     254  4.38\n#> 2 ACK     265  4.85\n#> 3 ALB     439 14.4 \n#> 4 ANC       8 -2.5 \n#> 5 ATL   17215 11.3 \n#> 6 AUS    2439  6.02\n#> # … with 99 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nn() is a special summary function that doesn’t take any arguments and instead access information about the “current” group. This means that it only works inside dplyr verbs:\n\nn()\n#> Error in `n()`:\n#> ! Must be used inside dplyr verbs.\n\nThere are a couple of variants of n() that you might find useful:\n\n\nn_distinct(x) counts the number of distinct (unique) values of one or more variables. For example, we could figure out which destinations are served by the most carriers:\n\nflights |> \n  group_by(dest) |> \n  summarise(\n    carriers = n_distinct(carrier)\n  ) |> \n  arrange(desc(carriers))\n#> # A tibble: 105 × 2\n#>   dest  carriers\n#>   <chr>    <int>\n#> 1 ATL          7\n#> 2 BOS          7\n#> 3 CLT          7\n#> 4 ORD          7\n#> 5 TPA          7\n#> 6 AUS          6\n#> # … with 99 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\nA weighted count is a sum. For example you could “count” the number of miles each plane flew:\n\nflights |> \n  group_by(tailnum) |> \n  summarise(miles = sum(distance))\n#> # A tibble: 4,044 × 2\n#>   tailnum  miles\n#>   <chr>    <dbl>\n#> 1 D942DN    3418\n#> 2 N0EGMQ  250866\n#> 3 N10156  115966\n#> 4 N102UW   25722\n#> 5 N103US   24619\n#> 6 N104UW   25157\n#> # … with 4,038 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWeighted counts are a common problem so count() has a wt argument that does the same thing:\n\nflights |> count(tailnum, wt = distance)\n#> # A tibble: 4,044 × 2\n#>   tailnum      n\n#>   <chr>    <dbl>\n#> 1 D942DN    3418\n#> 2 N0EGMQ  250866\n#> 3 N10156  115966\n#> 4 N102UW   25722\n#> 5 N103US   24619\n#> 6 N104UW   25157\n#> # … with 4,038 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\nYou can count missing values by combining sum() and is.na(). In the flights dataset this represents flights that are cancelled:\n\nflights |> \n  group_by(dest) |> \n  summarise(n_cancelled = sum(is.na(dep_time))) \n#> # A tibble: 105 × 2\n#>   dest  n_cancelled\n#>   <chr>       <int>\n#> 1 ABQ             0\n#> 2 ACK             0\n#> 3 ALB            20\n#> 4 ANC             0\n#> 5 ATL           317\n#> 6 AUS            21\n#> # … with 99 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\n15.1.3 Exercises\n\nHow can you use count() to count the number rows with a missing value for a given variable?\nExpand the following calls to count() to instead use group_by(), summarise(), and arrange():\n\nflights |> count(dest, sort = TRUE)\nflights |> count(tailnum, wt = distance)"
  },
  {
    "objectID": "numbers.html#numeric-transformations",
    "href": "numbers.html#numeric-transformations",
    "title": "15  Numbers",
    "section": "\n15.2 Numeric transformations",
    "text": "15.2 Numeric transformations\nTransformation functions work well with mutate() because their output is the same length as the input. The vast majority of transformation functions are already built into base R. It’s impractical to list them all so this section will show the most useful ones. As an example, while R provides all the trigonometric functions that you might dream of, we don’t list them here because they’re rarely needed for data science.\n\n15.2.1 Arithmetic and recycling rules\nWe introduced the basics of arithmetic (+, -, *, /, ^) in Chapter 3 and have used them a bunch since. These functions don’t need a huge amount of explanation because they do what you learned in grade school. But we need to briefly talk about the recycling rules which determine what happens when the left and right hand sides have different lengths. This is important for operations like flights |> mutate(air_time = air_time / 60) because there are 336,776 numbers on the left of / but only one on the right.\nR handles mismatched lengths by recycling, or repeating, the short vector. We can see this in operation more easily if we create some vectors outside of a data frame:\n\nx <- c(1, 2, 10, 20)\nx / 5\n#> [1] 0.2 0.4 2.0 4.0\n# is shorthand for\nx / c(5, 5, 5, 5)\n#> [1] 0.2 0.4 2.0 4.0\n\nGenerally, you only want to recycle single numbers (i.e. vectors of length 1), but R will recycle any shorter length vector. It usually (but not always) gives you a warning if the longer vector isn’t a multiple of the shorter:\n\nx * c(1, 2)\n#> [1]  1  4 10 40\nx * c(1, 2, 3)\n#> Warning in x * c(1, 2, 3): longer object length is not a multiple of shorter\n#> object length\n#> [1]  1  4 30 20\n\nThese recycling rules are also applied to logical comparisons (==, <, <=, >, >=, !=) and can lead to a surprising result if you accidentally use == instead of %in% and the data frame has an unfortunate number of rows. For example, take this code which attempts to find all flights in January and February:\n\nflights |> \n  filter(month == c(1, 2))\n#> # A tibble: 25,977 × 19\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1      542         540       2     923     850      33 AA     \n#> 3  2013     1     1      554         600      -6     812     837     -25 DL     \n#> 4  2013     1     1      555         600      -5     913     854      19 B6     \n#> 5  2013     1     1      557         600      -3     838     846      -8 B6     \n#> 6  2013     1     1      558         600      -2     849     851      -2 B6     \n#> # … with 25,971 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThe code runs without error, but it doesn’t return what you want. Because of the recycling rules it finds flights in odd numbered rows that departed in January and flights in even numbered rows that departed in February. And unforuntately there’s no warning because nycflights has an even number of rows.\nTo protect you from this type of silent failure, most tidyverse functions use a stricter form of recycling that only recycles single values. Unfortunately that doesn’t help here, or in many other cases, because the key computation is performed by the base R function ==, not filter().\n\n15.2.2 Minimum and maximum\nThe arithmetic functions work with pairs of variables. Two closely related functions are pmin() and pmax(), which when given two or more variables will return the smallest or largest value in each row:\n\ndf <- tribble(\n  ~x, ~y,\n  1,  3,\n  5,  2,\n  7, NA,\n)\n\ndf |> \n  mutate(\n    min = pmin(x, y, na.rm = TRUE),\n    max = pmax(x, y, na.rm = TRUE)\n  )\n#> # A tibble: 3 × 4\n#>       x     y   min   max\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     3     1     3\n#> 2     5     2     2     5\n#> 3     7    NA     7     7\n\nNote that these are different to the summary functions min() and max() which take multiple observations and return a single value. You can tell that you’ve used the wrong form when all the minimums and all the maximums have the same value:\n\ndf |> \n  mutate(\n    min = min(x, y, na.rm = TRUE),\n    max = max(x, y, na.rm = TRUE)\n  )\n#> # A tibble: 3 × 4\n#>       x     y   min   max\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     3     1     7\n#> 2     5     2     1     7\n#> 3     7    NA     1     7\n\n\n15.2.3 Modular arithmetic\nModular arithmetic is the technical name for the type of math you did before you learned about real numbers, i.e. division that yields a whole number and a remainder. In R, %/% does integer division and %% computes the remainder:\n\n1:10 %/% 3\n#>  [1] 0 0 1 1 1 2 2 2 3 3\n1:10 %% 3\n#>  [1] 1 2 0 1 2 0 1 2 0 1\n\nModular arithmetic is handy for the flights dataset, because we can use it to unpack the sched_dep_time variable into and hour and minute:\n\nflights |> \n  mutate(\n    hour = sched_dep_time %/% 100,\n    minute = sched_dep_time %% 100,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 3\n#>   sched_dep_time  hour minute\n#>            <int> <dbl>  <dbl>\n#> 1            515     5     15\n#> 2            529     5     29\n#> 3            540     5     40\n#> 4            545     5     45\n#> 5            600     6      0\n#> 6            558     5     58\n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWe can combine that with the mean(is.na(x)) trick from Section 14.4 to see how the proportion of cancelled flights varies over the course of the day. The results are shown in Figure 15.1.\n\nflights |> \n  group_by(hour = sched_dep_time %/% 100) |> \n  summarise(prop_cancelled = mean(is.na(dep_time)), n = n()) |> \n  filter(hour > 1) |> \n  ggplot(aes(hour, prop_cancelled)) +\n  geom_line(color = \"grey50\") + \n  geom_point(aes(size = n))\n\n\n\nFigure 15.1: A line plot with scheduled departure hour on the x-axis, and proportion of cancelled flights on the y-axis. Cancellations seem to accumulate over the course of the day until 8pm, very late flights are much less likely to be cancelled.\n\n\n\n\n\n15.2.4 Logarithms\nLogarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude. They also convert exponential growth to linear growth. For example, take compounding interest — the amount of money you have at year + 1 is the amount of money you had at year multiplied by the interest rate. That gives a formula like money = starting * interest ^ year:\n\nstarting <- 100\ninterest <- 1.05\n\nmoney <- tibble(\n  year = 2000 + 1:50,\n  money = starting * interest^(1:50)\n)\n\nIf you plot this data, you’ll get an exponential curve:\n\nggplot(money, aes(year, money)) +\n  geom_line()\n\n\n\n\nLog transforming the y-axis gives a straight line:\n\nggplot(money, aes(year, money)) +\n  geom_line() + \n  scale_y_log10()\n\n\n\n\nThis a straight line because a little algebra reveals that log(money) = log(starting) + n * log(interest), which matches the pattern for a line, y = m * x + b. This is a useful pattern: if you see a (roughly) straight line after log-transforming the y-axis, you know that there’s underlying exponential growth.\nIf you’re log-transforming your data with dplyr you have a choice of three logarithms provided by base R: log() (the natural log, base e), log2() (base 2), and log10() (base 10). We recommend using log2() or log10(). log2() is easy to interpret because difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving; whereas log10() is easy to back-transform because (e.g) 3 is 10^3 = 1000.\nThe inverse of log() is exp(); to compute the inverse of log2() or log10() you’ll need to use 2^ or 10^.\n\n15.2.5 Rounding\nUse round(x) to round a number to the nearest integer:\n\nround(123.456)\n#> [1] 123\n\nYou can control the precision of the rounding with the second argument, digits. round(x, digits) rounds to the nearest 10^-n so digits = 2 will round to the nearest 0.01. This definition is useful because it implies round(x, -3) will round to the nearest thousand, which indeed it does:\n\nround(123.456, 2)  # two digits\n#> [1] 123.46\nround(123.456, 1)  # one digit\n#> [1] 123.5\nround(123.456, -1) # round to nearest ten\n#> [1] 120\nround(123.456, -2) # round to nearest hundred\n#> [1] 100\n\nThere’s one weirdness with round() that seems surprising at first glance:\n\nround(c(1.5, 2.5))\n#> [1] 2 2\n\nround() uses what’s known as “round half to even” or Banker’s rounding: if a number is half way between two integers, it will be rounded to the even integer. This is a good strategy because it keeps the rounding unbiased: half of all 0.5s are rounded up, and half are rounded down.\nround() is paired with floor() which always rounds down and ceiling() which always rounds up:\n\nx <- 123.456\n\nfloor(x)\n#> [1] 123\nceiling(x)\n#> [1] 124\n\nThese functions don’t have a digits argument, so you can instead scale down, round, and then scale back up:\n\n# Round down to nearest two digits\nfloor(x / 0.01) * 0.01\n#> [1] 123.45\n# Round up to nearest two digits\nceiling(x / 0.01) * 0.01\n#> [1] 123.46\n\nYou can use the same technique if you want to round() to a multiple of some other number:\n\n# Round to nearest multiple of 4\nround(x / 4) * 4\n#> [1] 124\n\n# Round to nearest 0.25\nround(x / 0.25) * 0.25\n#> [1] 123.5\n\n\n15.2.6 Cutting numbers into ranges\nUse cut()1 to break up a numeric vector into discrete buckets:\n\nx <- c(1, 2, 5, 10, 15, 20)\ncut(x, breaks = c(0, 5, 10, 15, 20))\n#> [1] (0,5]   (0,5]   (0,5]   (5,10]  (10,15] (15,20]\n#> Levels: (0,5] (5,10] (10,15] (15,20]\n\nThe breaks don’t need to be evenly spaced:\n\ncut(x, breaks = c(0, 5, 10, 100))\n#> [1] (0,5]    (0,5]    (0,5]    (5,10]   (10,100] (10,100]\n#> Levels: (0,5] (5,10] (10,100]\n\nYou can optionally supply your own labels. Note that there should be one less labels than breaks.\n\ncut(x, \n  breaks = c(0, 5, 10, 15, 20), \n  labels = c(\"sm\", \"md\", \"lg\", \"xl\")\n)\n#> [1] sm sm sm md lg xl\n#> Levels: sm md lg xl\n\nAny values outside of the range of the breaks will become NA:\n\ny <- c(NA, -10, 5, 10, 30)\ncut(y, breaks = c(0, 5, 10, 15, 20))\n#> [1] <NA>   <NA>   (0,5]  (5,10] <NA>  \n#> Levels: (0,5] (5,10] (10,15] (15,20]\n\nSee the documentation for other useful arguments like right and include.lowest, which control if the intervals are [a, b) or (a, b] and if the lowest interval should be [a, b].\n\n15.2.7 Cumulative and rolling aggregates\nBase R provides cumsum(), cumprod(), cummin(), cummax() for running, or cumulative, sums, products, mins and maxes. dplyr provides cummean() for cumulative means. Cumulative sums tend to come up the most in practice:\n\nx <- 1:10\ncumsum(x)\n#>  [1]  1  3  6 10 15 21 28 36 45 55\n\nIf you need more complex rolling or sliding aggregates, try the slider package by Davis Vaughan. The following example illustrates some of its features.\n\nlibrary(slider)\n\n# Same as a cumulative sum\nslide_vec(x, sum, .before = Inf)\n#>  [1]  1  3  6 10 15 21 28 36 45 55\n# Sum the current element and the one before it\nslide_vec(x, sum, .before = 1)\n#>  [1]  1  3  5  7  9 11 13 15 17 19\n# Sum the current element and the two before and after it\nslide_vec(x, sum, .before = 2, .after = 2)\n#>  [1]  6 10 15 20 25 30 35 40 34 27\n# Only compute if the window is complete\nslide_vec(x, sum, .before = 2, .after = 2, .complete = TRUE)\n#>  [1] NA NA 15 20 25 30 35 40 NA NA\n\n\n15.2.8 Exercises\n\nExplain in words what each line of the code used to generate Figure 15.1 does.\nWhat trigonometric functions does R provide? Guess some names and look up the documentation. Do they use degrees or radians?\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. You can see the basic problem in this plot: there’s a gap between each hour.\n\nflights |> \n  filter(month == 1, day == 1) |> \n  ggplot(aes(sched_dep_time, dep_delay)) +\n  geom_point()\n#> Warning: Removed 4 rows containing missing values (geom_point).\n\n\n\n\nConvert them to a more truthful representation of time (either fractional hours or minutes since midnight)."
  },
  {
    "objectID": "numbers.html#general-transformations",
    "href": "numbers.html#general-transformations",
    "title": "15  Numbers",
    "section": "\n15.3 General transformations",
    "text": "15.3 General transformations\nThe following sections describe some general transformations which are often used with numeric vectors, but can be applied to all other column types.\n\n15.3.1 Fill in missing values\nYou can fill in missing values with dplyr’s coalesce():\n\nx <- c(1, NA, 5, NA, 10)\ncoalesce(x, 0)\n#> [1]  1  0  5  0 10\n\ncoalesce() is vectorised, so you can find the non-missing values from a pair of vectors:\n\ny <- c(2, 3, 4, NA, 5)\ncoalesce(x, y)\n#> [1]  1  3  5 NA 10\n\n\n15.3.2 Ranks\ndplyr provides a number of ranking functions inspired by SQL, but you should always start with dplyr::min_rank(). It uses the typical method for dealing with ties, e.g. 1st, 2nd, 2nd, 4th.\n\nx <- c(1, 2, 2, 3, 4, NA)\nmin_rank(x)\n#> [1]  1  2  2  4  5 NA\n\nNote that the smallest values get the lowest ranks; use desc(x) to give the largest values the smallest ranks:\n\nmin_rank(desc(x))\n#> [1]  5  3  3  2  1 NA\n\nIf min_rank() doesn’t do what you need, look at the variants dplyr::row_number(), dplyr::dense_rank(), dplyr::percent_rank(), and dplyr::cume_dist(). See the documentation for details.\n\ndf <- tibble(x = x)\ndf |> \n  mutate(\n    row_number = row_number(x),\n    dense_rank = dense_rank(x),\n    percent_rank = percent_rank(x),\n    cume_dist = cume_dist(x)\n  )\n#> # A tibble: 6 × 5\n#>       x row_number dense_rank percent_rank cume_dist\n#>   <dbl>      <int>      <int>        <dbl>     <dbl>\n#> 1     1          1          1         0          0.2\n#> 2     2          2          2         0.25       0.6\n#> 3     2          3          2         0.25       0.6\n#> 4     3          4          3         0.75       0.8\n#> 5     4          5          4         1          1  \n#> 6    NA         NA         NA        NA         NA\n\nYou can achieve many of the same results by picking the appropriate ties.method argument to base R’s rank(); you’ll probably also want to set na.last = \"keep\" to keep NAs as NA.\nrow_number() can also be used without any arguments when inside a dplyr verb. In this case, it’ll give the number of the “current” row. When combined with %% or %/% this can be a useful tool for dividing data into similarly sized groups:\n\ndf <- tibble(x = runif(10))\n\ndf |> \n  mutate(\n    row0 = row_number() - 1,\n    three_groups = row0 %% 3,\n    three_in_each_group = row0 %/% 3,\n  )\n#> # A tibble: 10 × 4\n#>         x  row0 three_groups three_in_each_group\n#>     <dbl> <dbl>        <dbl>               <dbl>\n#> 1 0.0808      0            0                   0\n#> 2 0.834       1            1                   0\n#> 3 0.601       2            2                   0\n#> 4 0.157       3            0                   1\n#> 5 0.00740     4            1                   1\n#> 6 0.466       5            2                   1\n#> # … with 4 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n15.3.3 Offsets\ndplyr::lead() and dplyr::lag() allow you to refer the values just before or just after the “current” value. They return a vector of the same length as the input, padded with NAs at the start or end:\n\nx <- c(2, 5, 11, 11, 19, 35)\nlag(x)\n#> [1] NA  2  5 11 11 19\nlead(x)\n#> [1]  5 11 11 19 35 NA\n\n\n\nx - lag(x) gives you the difference between the current and previous value.\n\nx - lag(x)\n#> [1] NA  3  6  0  8 16\n\n\n\nx == lag(x) tells you when the current value changes. This is often useful combined with the grouping trick described in Section 14.6.\n\nx == lag(x)\n#> [1]    NA FALSE FALSE  TRUE FALSE FALSE\n\n\n\nYou can lead or lag by more than one position by using the second argument, n.\n\n15.3.4 Exercises\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().\nWhich plane (tailnum) has the worst on-time record?\nWhat time of day should you fly if you want to avoid delays as much as possible?\nWhat does flights |> group_by(dest() |> filter(row_number() < 4) do? What does flights |> group_by(dest() |> filter(row_number(dep_delay) < 4) do?\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the average flight delay for an hour is related to the average delay for the previous hour.\n\nflights |> \n  mutate(hour = dep_time %/% 100) |> \n  group_by(year, month, day, hour) |> \n  summarise(\n    dep_delay = mean(dep_delay, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |> \n  filter(n > 5)\n\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\nFind all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination."
  },
  {
    "objectID": "numbers.html#summaries",
    "href": "numbers.html#summaries",
    "title": "15  Numbers",
    "section": "\n15.4 Summaries",
    "text": "15.4 Summaries\nJust using the counts, means, and sums that we’ve introduced already can get you a long way, but R provides many other useful summary functions. Here are a selection that you might find useful.\n\n15.4.1 Center\nSo far, we’ve mostly used mean() to summarize the center of a vector of values. Because the mean is the sum divided by the count, it is sensitive to even just a few unusually high or low values. An alternative is to use the median(), which finds a value that lies in the “middle” of the vector, i.e. 50% of the values is above it and 50% are below it. Depending on the shape of the distribution of the variable you’re interested in, mean or median might be a better measure of center. For example, for symmetric distributions we generally report the mean while for skewed distributions we usually report the median.\nFigure 15.2 compares the mean vs the median when looking at the hourly vs median departure delay. The median delay is always smaller than the mean delay because because flights sometimes leave multiple hours late, but never leave multiple hours early.\n\nflights |>\n  group_by(year, month, day) |>\n  summarise(\n    mean = mean(dep_delay, na.rm = TRUE),\n    median = median(dep_delay, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |> \n  ggplot(aes(mean, median)) + \n  geom_abline(slope = 1, intercept = 0, color = \"white\", size = 2) +\n  geom_point()\n\n\n\nFigure 15.2: A scatterplot showing the differences of summarising hourly depature delay with median instead of mean.\n\n\n\n\nYou might also wonder about the mode, or the most common value. This is a summary that only works well for very simple cases (which is why you might have learned about it in high school), but it doesn’t work well for many real datasets. If the data is discrete, there may be multiple most common values, and if the data is continuous, there might be no most common value because every value is ever so slightly different. For these reasons, the mode tends not to be used by statisticians and there’s no mode function included in base R2.\n\n15.4.2 Minimum, maximum, and quantiles\nWhat if you’re interested in locations other than the center? min() and max() will give you the largest and smallest values. Another powerful tool is quantile() which is a generalization of the median: quantile(x, 0.25) will find the value of x that is greater than 25% of the values, quantile(x, 0.5) is equivalent to the median, and quantile(x, 0.95) will find a value that’s greater than 95% of the values.\nFor the flights data, you might want to look at the 95% quantile of delays rather than the maximum, because it will ignore the 5% of most delayed flights which can be quite extreme.\n\nflights |>\n  group_by(year, month, day) |>\n  summarise(\n    max = max(dep_delay, na.rm = TRUE),\n    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day   max   q95\n#>   <int> <int> <int> <dbl> <dbl>\n#> 1  2013     1     1   853  70.1\n#> 2  2013     1     2   379  85  \n#> 3  2013     1     3   291  68  \n#> 4  2013     1     4   288  60  \n#> 5  2013     1     5   327  41  \n#> 6  2013     1     6   202  51  \n#> # … with 359 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n15.4.3 Spread\nSometimes you’re not so interested in where the bulk of the data lies, but how it is spread out. Two commonly used summaries are the standard deviation, sd(x), and the inter-quartile range, IQR(). We won’t explain sd() here since you’re probably already familiar with it, but IQR() might be new — it’s quantile(x, 0.75) - quantile(x, 0.25) and gives you the range that contains the middle 50% of the data.\nWe can use this to reveal a small oddity in the flights data. You might expect that the spread of the distance between origin and destination to be zero, since airports are always in the same place. But the code below makes it looks like one airport, EGE, might have moved.\n\nflights |> \n  group_by(origin, dest) |> \n  summarise(\n    distance_sd = IQR(distance), \n    n = n(),\n    .groups = \"drop\"\n  ) |> \n  filter(distance_sd > 0)\n#> # A tibble: 2 × 4\n#>   origin dest  distance_sd     n\n#>   <chr>  <chr>       <dbl> <int>\n#> 1 EWR    EGE             1   110\n#> 2 JFK    EGE             1   103\n\n\n15.4.4 Distributions\nIt’s worth remembering that all of the summary statistics described above are a way of reducing the distribution down to a single number. This means that they’re fundamentally reductive, and if you pick the wrong summary, you can easily miss important differences between groups. That’s why it’s always a good idea to visualize the distribution before committing to your summary statistics.\nFigure 15.3 shows the overall distribution of departure delays. The distribution is so skewed that we have to zoom in to see the bulk of the data. This suggests that the mean is unlikely to be a good summary and we might prefer the median instead.\n\nflights |>\n  ggplot(aes(dep_delay)) + \n  geom_histogram(binwidth = 15)\n#> Warning: Removed 8255 rows containing non-finite values (stat_bin).\n\nflights |>\n  filter(dep_delay <= 120) |> \n  ggplot(aes(dep_delay)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\n\n(a) Histogram shows the full range of delays.\n\n\n\n\n\n\n(b) Histogram is zoomed in to show delays less than 2 hours.\n\n\n\n\nFigure 15.3: The distribution of dep_delay appears highly skewed to the right in both histograms.\n\n\n\nIt’s also a good idea to check that distributions for subgroups resemble the whole. Figure 15.4 overlays a frequency polygon for each day. The distributions seem to follow a common pattern, suggesting it’s fine to use the same summary for each day.\n\nflights |>\n  filter(dep_delay < 120) |> \n  ggplot(aes(dep_delay, group = interaction(day, month))) + \n  geom_freqpoly(binwidth = 5, alpha = 1/5)\n\n\n\nFigure 15.4: 365 frequency polygons of dep_delay, one for each day. The frequency polygons appear to have the same shape, suggesting that it’s reasonable to compare days by looking at just a few summary statistics.\n\n\n\n\nDon’t be afraid to explore your own custom summaries specifically tailored for the data that you’re working with. In this case, that might mean separately summarizing the flights that left early vs the flights that left late, or given that the values are so heavily skewed, you might try a log-transformation. Finally, don’t forget what you learned in Section 4.5: whenever creating numerical summaries, it’s a good idea to include the number of observations in each group.\n\n15.4.5 Positions\nThere’s one final type of summary that’s useful for numeric vectors, but also works with every other type of value: extracting a value at specific position. You can do this with the base R [ function, but we’re not going to cover it until Section 28.4.5, because it’s a very powerful and general function. For now we’ll introduce three specialized functions that you can use to extract values at a specified position: first(x), last(x), and nth(x, n).\nFor example, we can find the first and last departure for each day:\n\nflights |> \n  group_by(year, month, day) |> \n  summarise(\n    first_dep = first(dep_time), \n    fifth_dep = nth(dep_time, 5),\n    last_dep = last(dep_time)\n  )\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the\n#> `.groups` argument.\n#> # A tibble: 365 × 6\n#> # Groups:   year, month [12]\n#>    year month   day first_dep fifth_dep last_dep\n#>   <int> <int> <int>     <int>     <int>    <int>\n#> 1  2013     1     1       517       554       NA\n#> 2  2013     1     2        42       535       NA\n#> 3  2013     1     3        32       520       NA\n#> 4  2013     1     4        25       531       NA\n#> 5  2013     1     5        14       534       NA\n#> 6  2013     1     6        16       555       NA\n#> # … with 359 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n(These functions currently lack an na.rm argument but will hopefully be fixed by the time you read this book: https://github.com/tidyverse/dplyr/issues/6242).\nIf you’re familiar with [, you might wonder if you ever need these functions. There are two main reasons: the default argument and the order_by argument. default allows you to set a default value that’s used if the requested position doesn’t exist, e.g. you’re trying to get the 3rd element from a two element group. order_by lets you locally override the existing ordering of the rows, so you can get the element at the position in the ordering by order_by().\nExtracting values at positions is complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row:\n\nflights |> \n  group_by(year, month, day) |> \n  mutate(r = min_rank(desc(sched_dep_time))) |> \n  filter(r %in% c(1, max(r)))\n#> # A tibble: 1,195 × 20\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#>   <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n#> 1  2013     1     1      517         515       2     830     819      11 UA     \n#> 2  2013     1     1     2353        2359      -6     425     445     -20 B6     \n#> 3  2013     1     1     2353        2359      -6     418     442     -24 B6     \n#> 4  2013     1     1     2356        2359      -3     425     437     -12 B6     \n#> 5  2013     1     2       42        2359      43     518     442      36 B6     \n#> 6  2013     1     2      458         500      -2     703     650      13 US     \n#> # … with 1,189 more rows, 10 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, r <int>, and abbreviated variable names\n#> #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n15.4.6 With mutate()\n\nAs the names suggest, the summary functions are typically paired with summarise(). However, because of the recycling rules we discussed in Section 28.4.3 they can also be usefully paired with mutate(), particularly when you want do some sort of group standardization. For example:\n\n\nx / sum(x) calculates the proportion of a total.\n\n(x - mean(x)) / sd(x) computes a Z-score (standardized to mean 0 and sd 1).\n\nx / first(x) computes an index based on the first observation.\n\n15.4.7 Exercises\n\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:\n\nA flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.\nA flight is always 10 minutes late.\nA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\n99% of the time a flight is on time. 1% of the time it’s 2 hours late.\n\nWhich do you think is more important: arrival delay or departure delay?\n\nWhich destinations show the greatest variation in air speed?\nCreate a plot to further explore the adventures of EGE. Can you find any evidence that the airport moved locations?"
  },
  {
    "objectID": "strings.html",
    "href": "strings.html",
    "title": "16  Strings",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is undergoing heavy restructuring and may be confusing or incomplete. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "strings.html#introduction",
    "href": "strings.html#introduction",
    "title": "16  Strings",
    "section": "\n16.1 Introduction",
    "text": "16.1 Introduction\nSo far, you’ve used a bunch of strings without learning much about the details. Now it’s time to dive into them, learning what makes strings tick, and mastering some of the powerful string manipulation tool you have at your disposal.\nWe’ll begin with the details of creating strings and character vectors. You’ll then dive into creating strings from data. Next, we’ll discuss the basics of regular expressions, a powerful tool for describing patterns in strings, then use those tools to extract data from strings. The chapter finishes up with functions that work with individual letters, a brief discussion of where your expectations from English might steer you wrong when working with other languages, and a few useful non-stringr functions.\nThis chapter is paired with two other chapters. Regular expression are a big topic, so we’ll come back to them again in Chapter 17. We’ll also come back to strings again in Chapter 30 where we’ll look at them from a programming perspective rather than a data analysis perspective.\n\n16.1.1 Prerequisites\nIn this chapter, we’ll use functions from the stringr package which is part of the core tidyverse. We’ll also use the babynames data since it provides some fun strings to manipulate.\n\nlibrary(tidyverse)\nlibrary(babynames)\nlibrary(stringr)\n\nSimilar functionality is available in base R (through functions like grepl(), gsub(), and regmatches()) but we think you’ll find stringr easier to use because it’s been carefully designed to be as consistent as possible. You can easily tell when you’re using a stringr function because all stringr functions start with str_. This is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you jog your memory of which functions are available."
  },
  {
    "objectID": "strings.html#creating-a-string",
    "href": "strings.html#creating-a-string",
    "title": "16  Strings",
    "section": "\n16.2 Creating a string",
    "text": "16.2 Creating a string\nWe’ve created strings in passing earlier in the book, but didn’t discuss the details. First, you can create a string using either single quotes (') or double quotes (\"). Unlike other languages, there is no difference in behavior, but in the interests of consistency the tidyverse style guide recommends using \", unless the string contains multiple \".\n\nstring1 <- \"This is a string\"\nstring2 <- 'If I want to include a \"quote\" inside a string, I use single quotes'\n\nIf you forget to close a quote, you’ll see +, the continuation character:\n> \"This is a string without a closing quote\n+ \n+ \n+ HELP I'M STUCK\nIf this happen to you and you can’t figure out which quote you need to close, press Escape to cancel, and try again.\n\n16.2.1 Escapes\nTo include a literal single or double quote in a string you can use \\ to “escape” it:\n\ndouble_quote <- \"\\\"\" # or '\"'\nsingle_quote <- '\\'' # or \"'\"\n\nAnd if you want to include a literal backslash in your string, you’ll need to double it up: \"\\\\\":\n\nbackslash <- \"\\\\\"\n\nBeware that the printed representation of a string is not the same as string itself, because the printed representation shows the escapes (in other words, when you print a string, you can copy and paste the output to recreate that string). To see the raw contents of the string, use str_view()1:\n\nx <- c(single_quote, double_quote, backslash)\nx\n#> [1] \"'\"  \"\\\"\" \"\\\\\"\nstr_view(x,\"\")\n\n\n\n\n\n\n16.2.2 Raw strings\nCreating a string with multiple quotes or backslashes gets confusing quickly. To illustrate the problem, lets create a string that contains the contents of the chunk where we define the double_quote and single_quote variables:\n\ntricky <- \"double_quote <- \\\"\\\\\\\"\\\" # or '\\\"'\nsingle_quote <- '\\\\'' # or \\\"'\\\"\"\nstr_view(tricky,\"\")\n\n\n\n\n\nThat’s a lot of backslashes! (This is sometimes called leaning toothpick syndome.) To eliminate the escaping you can instead use a raw string2:\n\ntricky <- r\"(double_quote <- \"\\\"\" # or '\"'\nsingle_quote <- '\\'' # or \"'\")\"\nstr_view(tricky,\"\")\n\n\n\n\n\nA raw string usually starts with r\"( and finishes with )\". But if your string contains )\" you can instead use r\"[]\" or r\"{}\", and if that’s still not enough, you can insert any number of dashes to make the opening and closing pairs unique, e.g. `r\"--()--\", `r\"---()---\", etc. Raw strings are flexible enough to handle any text.\n\n16.2.3 Other special characters\nAs well as \\\", \\', and \\\\ there are a handful of other special characters that may come in handy. The most common are \\n, newline, and \\t, tab. You’ll also sometimes see strings containing Unicode escapes that start with \\u or \\U. This is a way of writing non-English characters that works on all systems. You can see the complete list of other special characters in ?'\"'.\n\nx <- c(\"one\\ntwo\", \"one\\ttwo\", \"\\u00b5\", \"\\U0001f604\")\nx\n#> [1] \"one\\ntwo\" \"one\\ttwo\" \"µ\"        \"😄\"\nstr_view(x,\"\")\n\n\n\n\n\nNote that str_view() shows special whitespace characters (i.e. everything except spaces and newlines) with a blue background to make them easier to spot.\n\n16.2.4 Vectors\nYou can combine multiple strings into a character vector by using c():\n\nx <- c(\"first string\", \"second string\", \"third string\")\nx\n#> [1] \"first string\"  \"second string\" \"third string\"\n\nTechnically, a string is a length-1 character vector, but this doesn’t have much bearing on your data analysis life. We’ll come back to this idea is more detail when we think about vectors as a programming tool in Chapter 28.\n\n16.2.5 Exercises"
  },
  {
    "objectID": "strings.html#creating-strings-from-data",
    "href": "strings.html#creating-strings-from-data",
    "title": "16  Strings",
    "section": "\n16.3 Creating strings from data",
    "text": "16.3 Creating strings from data\nNow that you’ve learned the basics of creating strings by “hand”, we’ll go into the details of creating strings from other strings. It’s a common problem: you often have some fixed strings that you wrote that you want to combine some varying strings that come from the data. For example, to create a greeting you might combine “Hello” with a name variable. First, we’ll discuss two functions that make this easy. Then we’ll talk about a slightly different scenario where you want to summarise a character vector, collapsing any number of strings into one.\n\n16.3.1 str_c()\n\nstr_c()3 takes any number of vectors as arguments and returns a character vector:\n\nstr_c(\"x\", \"y\")\n#> [1] \"xy\"\nstr_c(\"x\", \"y\", \"z\")\n#> [1] \"xyz\"\nstr_c(\"Hello \", c(\"John\", \"Susan\"))\n#> [1] \"Hello John\"  \"Hello Susan\"\n\nstr_c() is designed to be used with mutate() so it obeys the usual rules for recycling and missing values:\n\ndf <- tibble(name = c(\"Timothy\", \"Dewey\", \"Mable\", NA))\ndf |> mutate(greeting = str_c(\"Hi \", name, \"!\"))\n#> # A tibble: 4 × 2\n#>   name    greeting   \n#>   <chr>   <chr>      \n#> 1 Timothy Hi Timothy!\n#> 2 Dewey   Hi Dewey!  \n#> 3 Mable   Hi Mable!  \n#> 4 <NA>    <NA>\n\nIf you want missing values to display in some other way, use coalesce() either inside or outside of str_c():\n\ndf |> mutate(\n  greeting1 = str_c(\"Hi \", coalesce(name, \"you\"), \"!\"),\n  greeting2 = coalesce(str_c(\"Hi \", name, \"!\"), \"Hi!\")\n)\n#> # A tibble: 4 × 3\n#>   name    greeting1   greeting2  \n#>   <chr>   <chr>       <chr>      \n#> 1 Timothy Hi Timothy! Hi Timothy!\n#> 2 Dewey   Hi Dewey!   Hi Dewey!  \n#> 3 Mable   Hi Mable!   Hi Mable!  \n#> 4 <NA>    Hi you!     Hi!\n\n\n16.3.2 str_glue()\n\nIf you are mixing many fixed and variable strings with str_c(), you’ll notice that you have to type \"\" repeatedly, and this can make it hard to see the overall goal of the code. An alternative approach is provided by the glue package via str_glue()4 . You give it a single string containing {} and anything inside {} will be evaluated like it’s outside of the string:\n\ndf |> mutate(greeting = str_glue(\"Hi {name}!\"))\n#> # A tibble: 4 × 2\n#>   name    greeting   \n#>   <chr>   <glue>     \n#> 1 Timothy Hi Timothy!\n#> 2 Dewey   Hi Dewey!  \n#> 3 Mable   Hi Mable!  \n#> 4 <NA>    Hi NA!\n\nYou can use any valid R code inside of {}, but it’s a good idea to pull complex calculations out into their own variables so you can more easily check your work.\nAs you can see above, str_glue() currently converts missing values to the string “NA” making it slightly inconsistent with str_c(). We’ll hopefully fix that by the time the book is printed: https://github.com/tidyverse/glue/issues/246\nYou also might wonder what happens if you need to include a regular { or } in your string. You might expect that you’ll need to escape it, and you’d be right. But glue uses a slightly different escaping technique; instead of prefixing with special character like \\, you just double up the { and }:\n\ndf |> mutate(greeting = str_glue(\"{{Hi {name}!}}\"))\n#> # A tibble: 4 × 2\n#>   name    greeting     \n#>   <chr>   <glue>       \n#> 1 Timothy {Hi Timothy!}\n#> 2 Dewey   {Hi Dewey!}  \n#> 3 Mable   {Hi Mable!}  \n#> 4 <NA>    {Hi NA!}\n\n\n16.3.3 str_flatten()\n\nstr_c() and glue() work well with mutate() because their output is the same length as their inputs. What if you want a function that works well with summarise(), i.e. something that always returns a single string? That’s the job of str_flatten()5: it takes a character vector and combines each element of the vector into a single string:\n\nstr_flatten(c(\"x\", \"y\", \"z\"))\n#> [1] \"xyz\"\nstr_flatten(c(\"x\", \"y\", \"z\"), \", \")\n#> [1] \"x, y, z\"\nstr_flatten(c(\"x\", \"y\", \"z\"), \", \"\n            #, last = `\",` & `\"` # unused arg?\n            )\n#> [1] \"x, y, z\"\n\nThis makes it work well with summarise():\n\ndf <- tribble(\n  ~ name, ~ fruit,\n  \"Carmen\", \"banana\",\n  \"Carmen\", \"apple\",\n  \"Marvin\", \"nectarine\",\n  \"Terence\", \"cantaloupe\",\n  \"Terence\", \"papaya\",\n  \"Terence\", \"madarine\"\n)\ndf |>\n  group_by(name) |> \n  summarise(fruits = str_flatten(fruit, \", \"))\n#> # A tibble: 3 × 2\n#>   name    fruits                      \n#>   <chr>   <chr>                       \n#> 1 Carmen  banana, apple               \n#> 2 Marvin  nectarine                   \n#> 3 Terence cantaloupe, papaya, madarine\n\n\n16.3.4 Exercises\n\n\nCompare and contrast the results of paste0() with str_c() for the following inputs:\n\nstr_c(\"hi \", NA)\nstr_c(letters[1:2], letters[1:3])\n\n\n\nConvert the following expressions from str_c() to glue() or vice versa:\n\nstr_c(\"The price of \", food, \" is \", price)\nglue(\"I'm {age} years old and live in {country}\")\nstr_c(\"\\\\section{\", title, \"}\")"
  },
  {
    "objectID": "strings.html#working-with-patterns",
    "href": "strings.html#working-with-patterns",
    "title": "16  Strings",
    "section": "\n16.4 Working with patterns",
    "text": "16.4 Working with patterns\nIt’s probably even more useful to be able to extract data from string than create strings from data, but before we can tackle that, we need to take a brief digression to talk about regular expressions. Regular expressions are a very concise language that describes patterns in strings. For example, \"^The\" is shorthand for any string that starts with “The”, and a.+e is a shorthand for “a” followed by one or more other characters, followed by an “e”.\nWe’ll start by using str_detect() which answers a simple question: “does this pattern occur anywhere in my vector?”. We’ll then ask progressively more complex questions by learning more about regular expressions and the stringr functions that use them.\n\n16.4.1 Detect matches\nThe term “regular expression” is a bit of a mouthful, so most people abbreviate to “regex”6 or “regexp”. To learn about regexes, we’ll start with the simplest function that uses them: str_detect(). It takes a character vector and a pattern, and returns a logical vector that says if the pattern was found at each element of the vector. The following code shows the simplest type of pattern, an exact match.\n\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_detect(x, \"e\")   # does the word contain an e?\n#> [1]  TRUE FALSE  TRUE\nstr_detect(x, \"b\")   # does the word contain a b?\n#> [1] FALSE  TRUE FALSE\nstr_detect(x, \"ear\") # does the word contain \"ear\"?\n#> [1] FALSE FALSE  TRUE\n\nstr_detect() returns a logical vector the same length as the first argument, so it pairs well with filter(). For example, this code finds all names that contain a lower-case “x”:\n\nbabynames |> filter(str_detect(name, \"x\"))\n#> # A tibble: 16,317 × 5\n#>    year sex   name          n      prop\n#>   <dbl> <chr> <chr>     <int>     <dbl>\n#> 1  1880 F     Roxie        62 0.000635 \n#> 2  1880 F     Dixie        15 0.000154 \n#> 3  1880 F     Roxanna       9 0.0000922\n#> 4  1880 F     Texas         5 0.0000512\n#> 5  1880 M     Alexander   211 0.00178  \n#> 6  1880 M     Alex        147 0.00124  \n#> # … with 16,311 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWe can also use str_detect() with summarize() by remembering that when you use a logical vector in a numeric context, FALSE becomes 0 and TRUE becomes 1. That means sum(str_detect(x, pattern)) will tell you the number of observations that match, while mean(str_detect(x, pattern)) tells you the proportion of observations that match. For example, the following snippet computes and visualizes the proportion of baby names that contain “x”, broken down by year:\n\nbabynames |> \n  group_by(year) |> \n  summarise(prop_x = mean(str_detect(name, \"x\"))) |> \n  ggplot(aes(year, prop_x)) + \n  geom_line()\n\n\n\n\n(Note that this gives us the proportion of names that contain an x; if you wanted the proportion of babies given a name containing an x, you’d need to perform a weighted mean).\n\n16.4.2 Introduction to regular expressions\nThe simplest patterns, like those above, are exact: they match any strings that contain the exact sequence of characters in the pattern:\n\nstr_detect(c(\"x\", \"X\"), \"x\")\n#> [1]  TRUE FALSE\nstr_detect(c(\"xyz\", \"xza\"), \"xy\")\n#> [1]  TRUE FALSE\n\nIn general, any letter or number will match exactly, but punctuation characters like ., +, *, [, ], ?, often have special meanings7. For example, . will match any character8, so \"a.\" will match any string that contains an “a” followed by another character :\n\nstr_detect(c(\"a\", \"ab\", \"ae\", \"bd\", \"ea\", \"eab\"), \"a.\")\n#> [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE\n\nTo get a better sense of what’s happening, lets switch to str_view_all(). This shows which characters are matched by surrounding it with <> and coloring it blue:\n\nstr_view_all(c(\"a\", \"ab\", \"ae\", \"bd\", \"ea\", \"eab\"), \"a.\")\n\n\n\n\n\nRegular expressions are a powerful and flexible language which we’ll come back to in Chapter 17. Here we’ll just introduce only the most important components: quantifiers and character classes.\nQuantifiers control how many times an element that can be applied to other pattern: ? makes a pattern optional (i.e. it matches 0 or 1 times), + lets a pattern repeat (i.e. it matches at least once), and * lets a pattern be optional or repeat (i.e. it matches any number of times, including 0).\n\n# ab? matches an \"a\", optionally followed by a \"b\".\nstr_view_all(c(\"a\", \"ab\", \"abb\"), \"ab?\")\n\n\n\n\n\n# ab+ matches an \"a\", followed by at least one \"b\".\nstr_view_all(c(\"a\", \"ab\", \"abb\"), \"ab+\")\n\n\n\n\n\n# ab* matches an \"a\", followed by any number of \"b\"s.\nstr_view_all(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n\n\n\n\n\nCharacter classes are defined by [] and let you match a set set of characters, e.g. [abcd] matches “a”, “b”, “c”, or “d”. You can also invert the match by starting with ^: [^abcd] matches anything except “a”, “b”, “c”, or “d”. We can use this idea to find the vowels in a few particularly special names:\n\nnames <- c(\"Hadley\", \"Mine\", \"Garrett\")\nstr_view_all(names, \"[aeiou]\")\n\n\n\n\n\nYou can combine character classes and quantifiers. Notice the difference between the following two patterns that look for consonants. The same characters are matched, but the number of matches is different.\n\nstr_view_all(names, \"[^aeiou]\")\n\n\n\n\nstr_view_all(names, \"[^aeiou]+\")\n\n\n\n\n\nRegular expressions are very compact and use a lot of punctuation characters, so they can seem overwhelming at first, and you’ll think a cat has walked across your keyboard. So don’t worry if they’re hard to understand at first; you’ll get better with practice. Lets start that practice with some other useful stringr functions.\n\n16.4.3 Count matches\nA variation on str_detect() is str_count(): rather than a simple yes or no, it tells you how many matches there are in a string:\n\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_count(x, \"p\")\n#> [1] 2 0 1\n\nNote that regular expression matches never overlap so str_count() only starts looking for a new match after the end of the last match. For example, in \"abababa\", how many times will the pattern \"aba\" match? Regular expressions say two, not three:\n\nstr_count(\"abababa\", \"aba\")\n#> [1] 2\nstr_view_all(\"abababa\", \"aba\")\n\n\n\n\n\nIt’s natural to use str_count() with mutate(). The following example uses str_count() with character classes to count the number of vowels and consonants in each name.\n\nbabynames |> \n  count(name) |> \n  mutate(\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n#> # A tibble: 97,310 × 4\n#>   name          n vowels consonants\n#>   <chr>     <int>  <int>      <int>\n#> 1 Aaban        10      2          3\n#> 2 Aabha         5      2          3\n#> 3 Aabid         2      2          3\n#> 4 Aabir         1      2          3\n#> 5 Aabriella     5      4          5\n#> 6 Aada          1      2          2\n#> # … with 97,304 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nIf you look closely, you’ll notice that there’s something off with our calculations: “Aaban” contains three “a”s, but our summary reports only two vowels. That’s because we’ve forgotten to tell you that regular expressions are case sensitive. There are three ways we could fix this:\n\nAdd the upper case vowels to the character class: str_count(name, \"[aeiouAEIOU]\").\nTell the regular expression to ignore case: str_count(regex(name, ignore.case = TRUE), \"[aeiou]\"). We’ll talk about this next.\nUse str_to_lower() to convert the names to lower case: str_count(str_to_lower(name), \"[aeiou]\"). We’ll come back to this function in Section 16.6.\n\nThis is pretty typical when working with strings — there are often multiple ways to reach your goal, either making your pattern more complicated or by doing some preprocessing on your string. If you get stuck trying one approach, it can often be useful to switch gears and tackle the problem from a different perspective.\n\n16.4.4 Replace matches\nstr_replace_all() allows you to replace a match with the text of your choosing. This can be particularly useful if you need to standardize a vector. Unlike the regexp functions we’ve encountered so far, str_replace_all() takes three arguments: a character vector, a pattern, and a replacement.\nThe simplest use is to replace a pattern with a fixed string:\n\nx <- c(\"apple\", \"pear\", \"banana\")\nstr_replace_all(x, \"[aeiou]\", \"-\")\n#> [1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\nstr_remove_all() is a short cut for str_replace_all(x, pattern, \"\") — it removes matching patterns from a string.\nUse in mutate()\nUsing pipe inside mutate. Recommendation to make a function, and think about testing it — don’t need formal tests, but useful to build up a set of positive and negative test cases as you.\n\n16.4.5 Advanced replacements\nYou can also perform multiple replacements by supplying a named vector. The name gives a regular expression to match, and the value gives the replacement.\n\nx <- c(\"1 house\", \"1 person has 2 cars\", \"3 people\")\nstr_replace_all(x, c(\"1\" = \"one\", \"2\" = \"two\", \"3\" = \"three\"))\n#> [1] \"one house\"               \"one person has two cars\"\n#> [3] \"three people\"\n\nAlternatively, you can provide a replacement function: it’s called with a vector of matches, and should return what to replacement them with. We’ll come back to this powerful tool in Chapter 30.\n\nx <- c(\"1 house\", \"1 person has 2 cars\", \"3 people\")\nstr_replace_all(x, \"[aeiou]+\", str_to_upper)\n#> [1] \"1 hOUsE\"             \"1 pErsOn hAs 2 cArs\" \"3 pEOplE\"\n\n\n16.4.6 Pattern control\nNow that you’ve learn about regular expressions, you might be worried about them working when you don’t want them to. You can opt-out of the regular expression rules by using fixed():\n\nstr_view(c(\"\", \"a\", \".\"), fixed(\".\"))\n\n\n\n\n\nBoth fixed strings and regular expressions are case sensitive by default. You can opt out by setting ignore_case = TRUE.\n\nstr_view_all(\"x  X  xy\", \"X\")\n\n\n\n\nstr_view_all(\"x  X  xy\", fixed(\"X\", ignore_case = TRUE))\n\n\n\n\nstr_view_all(\"x  X  xy\", regex(\".Y\", ignore_case = TRUE))\n\n\n\n\n\n\n16.4.7 Exercises\n\nWhat name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)\n\nFor each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nAre there any words that contain at least one of each different vowel?\n\n\nReplace all forward slashes in a string with backslashes.\nImplement a simple version of str_to_lower() using str_replace_all().\nSwitch the first and last letters in words. Which of those strings are still words?"
  },
  {
    "objectID": "strings.html#extract-data-from-strings",
    "href": "strings.html#extract-data-from-strings",
    "title": "16  Strings",
    "section": "\n16.5 Extract data from strings",
    "text": "16.5 Extract data from strings\nCommon for multiple variables worth of data to be stored in a single string. In this section you’ll learn how to use various functions tidyr to extract them.\nWaiting on: https://github.com/tidyverse/tidyups/pull/15"
  },
  {
    "objectID": "strings.html#sec-other-languages",
    "href": "strings.html#sec-other-languages",
    "title": "16  Strings",
    "section": "\n16.6 Locale dependent operations",
    "text": "16.6 Locale dependent operations\nSo far all of our examples have been using English. The details of the many ways other languages are different to English are too diverse to detail here, but we wanted to give a quick outline of the functions who’s behavior differs based on your locale, the set of settings that vary from country to country.\nLocale is specified with lower-case language abbreviation, optionally followed by a _ and a upper-case region identifier. For example, “en” is English, “en_GB” is British English, and “en_US” is American English. If you don’t already know the code for your language, Wikipedia has a good list, and you can see which are supported with stringi::stri_locale_list().\nBase R string functions automatically use your locale current locale. This means that string manipulation code works the way you expect when you’re working with text in your native language, but it might work differently when you share it with someone who lives in another country. To avoid this problem, stringr defaults to the “en” locale, and requires you to specify the locale argument to override it. This also makes it easy to tell if a function might have different behavior in different locales.\nFortunately there are three sets of functions where the locale matters:\n\n\nChanging case: while only relatively few languages have upper and lower case (Latin, Greek, and Cyrillic, plus a handful of lessor known languages). The rules are not te same in every language that uses these alphabets. For example, Turkish has two i’s: with and without a dot, and it has a different rule for capitalising them:\n\nstr_to_upper(c(\"i\", \"ı\"))\n#> [1] \"I\" \"I\"\nstr_to_upper(c(\"i\", \"ı\"), locale = \"tr\")\n#> [1] \"İ\" \"I\"\n\n\n\nComparing strings: str_equal() lets you compare if two strings are equal, optionally ignoring case:\n\n#str_equal(\"i\", \"I\", ignore_case = TRUE)\n#str_equal(\"i\", \"I\", ignore_case = TRUE, locale = \"tr\")\n\n\n\nSorting strings: str_sort() and str_order() sort vectors alphabetically, but the alphabet is not the same in every language9! Here’s an example: in Czech, “ch” is a compound letter that appears after h in the alphabet.\n\nstr_sort(c(\"a\", \"c\", \"ch\", \"h\", \"z\"))\n#> [1] \"a\"  \"c\"  \"ch\" \"h\"  \"z\"\nstr_sort(c(\"a\", \"c\", \"ch\", \"h\", \"z\"), locale = \"cs\")\n#> [1] \"a\"  \"c\"  \"h\"  \"ch\" \"z\"\n\nDanish has a similar problem. Normally, characters with diacritics (e.g. à, á, â) sort after the plain character (e.g. a). But in Danish ø and å are their own letters that come at the end of the alphabet:\n\nstr_sort(c(\"a\", \"å\", \"o\", \"ø\", \"z\"))\n#> [1] \"a\" \"å\" \"o\" \"ø\" \"z\"\nstr_sort(c(\"a\", \"å\", \"o\", \"ø\", \"z\"), locale = \"da\")\n#> [1] \"a\" \"o\" \"z\" \"ø\" \"å\"\n\nThis also comes up when sorting strings with dplyr::arrange() which is why it also has a locale argument."
  },
  {
    "objectID": "strings.html#letters",
    "href": "strings.html#letters",
    "title": "16  Strings",
    "section": "\n16.7 Letters",
    "text": "16.7 Letters\nFunctions that work with the components of strings called code points. Depending on the language involved, this might be a letter (like in most European languages), a syllable (like Japanese), or a logogram (like in Chinese). It might be something more exotic like an accent, or a special symbol used to join two emoji together. But to keep things simple, we’ll call these letters.\n\n16.7.1 Length\nstr_length() tells you the number of letters in the string:\n\nstr_length(c(\"a\", \"R for data science\", NA))\n#> [1]  1 18 NA\n\nYou could use this with count() to find the distribution of lengths of US babynames, and then with filter() to look at the longest names10:\n\nbabynames |>\n  count(length = str_length(name), wt = n)\n#> # A tibble: 14 × 2\n#>   length        n\n#>    <int>    <int>\n#> 1      2   338150\n#> 2      3  8589596\n#> 3      4 48506739\n#> 4      5 87011607\n#> 5      6 90749404\n#> 6      7 72120767\n#> # … with 8 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nbabynames |> \n  filter(str_length(name) == 15) |> \n  count(name, wt = n, sort = TRUE)\n#> # A tibble: 34 × 2\n#>   name                n\n#>   <chr>           <int>\n#> 1 Franciscojavier   123\n#> 2 Christopherjohn   118\n#> 3 Johnchristopher   118\n#> 4 Christopherjame   108\n#> 5 Christophermich    52\n#> 6 Ryanchristopher    45\n#> # … with 28 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n16.7.2 Subsetting\nYou can extract parts of a string using str_sub(string, start, end). The start and end arguments are inclusive, so the length of the returned string will be end - start + 1:\n\nx <- c(\"Apple\", \"Banana\", \"Pear\")\nstr_sub(x, 1, 3)\n#> [1] \"App\" \"Ban\" \"Pea\"\n\nYou can use negative values to count back from the end of the string: -1 is the last character, -2 is the second to last character, etc.\n\nstr_sub(x, -3, -1)\n#> [1] \"ple\" \"ana\" \"ear\"\n\nNote that str_sub() won’t fail if the string is too short: it will just return as much as possible:\n\nstr_sub(\"a\", 1, 5)\n#> [1] \"a\"\n\nWe could use str_sub() with mutate() to find the first and last letter of each name:\n\nbabynames |> \n  mutate(\n    first = str_sub(name, 1, 1),\n    last = str_sub(name, -1, -1)\n  )\n#> # A tibble: 1,924,665 × 7\n#>    year sex   name          n   prop first last \n#>   <dbl> <chr> <chr>     <int>  <dbl> <chr> <chr>\n#> 1  1880 F     Mary       7065 0.0724 M     y    \n#> 2  1880 F     Anna       2604 0.0267 A     a    \n#> 3  1880 F     Emma       2003 0.0205 E     a    \n#> 4  1880 F     Elizabeth  1939 0.0199 E     h    \n#> 5  1880 F     Minnie     1746 0.0179 M     e    \n#> 6  1880 F     Margaret   1578 0.0162 M     t    \n#> # … with 1,924,659 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n16.7.3 Long strings\nSometimes the reason you care about the length of a string is because you’re trying to fit it into a label on a plot or in a table. stringr provides two useful tools for cases where your string is too long:\n\nstr_trunc(x, 20) ensures that no string is longer than 20 characters, replacing any thing too long with ….\nstr_wrap(x, 20) wraps a string introducing new lines so that each line is at most 20 characters (it doesn’t hyphenate, however, so any word longer than 20 characters will make a longer time)\n\n\nx <- \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\"\n\nstr_trunc(x, 30)\n#> [1] \"Lorem ipsum dolor sit amet,...\"\nstr_view(str_wrap(x, 30),\"\")\n\n\n\n\n\nTODO: add example with a plot.\n\n16.7.4 Exercises\n\nUse str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?\nAre there any major trends in the length of babynames over time? What about the popularity of first and last letters?"
  },
  {
    "objectID": "strings.html#other-functions",
    "href": "strings.html#other-functions",
    "title": "16  Strings",
    "section": "\n16.8 Other functions",
    "text": "16.8 Other functions\nThe are a bunch of other places you can use regular expressions outside of stringr.\n\nmatches(): as you can tell from it’s lack of str_ prefix, this isn’t a stringr fuction. It’s a “tidyselect” function, a fucntion that you can use anywhere in the tidyverse when selecting variables (e.g. dplyr::select(), rename_with(), across(), …).\n\napropos() searches all objects available from the global environment. This is useful if you can’t quite remember the name of the function.\n\napropos(\"replace\")\n#> [1] \"%+replace%\"       \"replace\"          \"replace_na\"       \"setReplaceMethod\"\n#> [5] \"str_replace\"      \"str_replace_all\"  \"str_replace_na\"   \"theme_replace\"\n\n\n\ndir() lists all the files in a directory. The pattern argument takes a regular expression and only returns file names that match the pattern. For example, you can find all the R Markdown files in the current directory with:\n\nhead(dir(pattern = \"\\\\.Rmd$\"))\n#> character(0)\n\n(If you’re more comfortable with “globs” like *.Rmd, you can convert them to regular expressions with glob2rx())."
  },
  {
    "objectID": "regexps.html",
    "href": "regexps.html",
    "title": "17  Regular expressions",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is undergoing heavy restructuring and may be confusing or incomplete. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "regexps.html#introduction",
    "href": "regexps.html#introduction",
    "title": "17  Regular expressions",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nYou learned the basics of regular expressions in Chapter 16, but regular expressions are fairly rich language so it’s worth spending some extra time on the details.\nThe chapter starts by expanding your knowledge of patterns, to cover six important new topics (escaping, anchoring, character classes, shorthand classes, quantifiers, and alternation). Here we’ll focus mostly on the language itself, not the functions that use it. That means we’ll mostly work with toy character vectors, showing the results with str_view() and str_view_all(). You’ll need to take what you learn here and apply it to data frames with tidyr functions or by combining dplyr and stringr functions. We’ll then take what you’ve learned a show a few useful strategies when creating more complex patterns.\nNext we’ll talk about the important concepts of “grouping” and “capturing” which give you new ways to extract variables out of strings using tidyr::separate_group(). Grouping also allows you to use back references which allow you do things like match repeated patterns. We’ll finish by discussing the various “flags” that allow you to tweak the operation of regular expressions\n\n17.1.1 Prerequisites\nThis chapter will use regular expressions as provided by the stringr package.\n\nlibrary(tidyverse)\n\nIt’s worth noting that the regular expressions used by stringr are very slightly different to those of base R. That’s because stringr is built on top of the stringi package, which is in turn built on top of the ICU engine, whereas base R functions (like gsub() and grepl()) use either the TRE engine or the PCRE engine. Fortunately, the basics of regular expressions are so well established that you’ll encounter few variations when working with the patterns you’ll learn in this book (and we’ll point them out where important). You only need to be aware of the difference when you start to rely on advanced features like complex Unicode character ranges or special features that use the (?…) syntax. You can learn more about these advanced features in vignette(\"regular-expressions\", package = \"stringr\"). Another useful reference is https://www.regular-expressions.info/. It’s not R specific, but it covers the most advanced features and explains how regular expressions work under the hood.\n\n17.1.2 Exercises\n\nExplain why each of these strings don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\".\nHow would you match the sequence \"'\\?\nWhat patterns will the regular expression \\..\\..\\.. match? How would you represent it as a string?"
  },
  {
    "objectID": "regexps.html#pattern-language",
    "href": "regexps.html#pattern-language",
    "title": "17  Regular expressions",
    "section": "\n17.2 Pattern language",
    "text": "17.2 Pattern language\nYou learned the very basics of the regular expression pattern language in Chapter 16, and now its time to dig into more of the details. First, we’ll start with escaping, which allows you to match characters that the pattern language otherwise treats specially. Next you’ll learn about anchors, which allow you to match the start or end of the string. Then you’ll learn about character classes and their shortcuts, which allow you to match any character from a set. We’ll finish up with quantifiers, which control how many times a pattern can match, and alternation, which allows you to match either this or that.\nThe terms we use here are the technical names for each component. They’re not always the most evocative of their purpose, but it’s very helpful to know the correct terms if you later want to Google for more details.\nWe’ll concentrate on showing how these patterns work with str_view() and str_view_all() but remember that you can use them with any of the functions that you learned about in Chapter 16, i.e.:\n\n\nstr_detect(x, pattern) returns a logical vector the same length as x, indicating whether each element matches (TRUE) or doesn’t match (FALSE) the pattern.\n\nstr_count(x, pattern) returns the number of times pattern matches in each element of x.\n\nstr_replace_all(x, pattern, replacement) replaces every instance of pattern with replacement.\n\n\n17.2.1 Escaping\nIn Chapter 16, you’ll learned how to match a literal . by using fixed(\".\"). But what if you want to match a literal . as part of a bigger regular expression? You’ll need to use an escape, which tells the regular expression you want it to match exactly, not use its special behavior. Like strings, regexps use the backslash for escaping, so to match a ., you need the regexp \\.. Unfortunately this creates a problem. We use strings to represent regular expressions, and \\ is also used as an escape symbol in strings. So, as the following example shows, to create the regular expression \\. we need the string \"\\\\.\".\n\n# To create the regular expression \\., we need to use \\\\.\ndot <- \"\\\\.\"\n\n# But the expression itself only contains one \\\nstr_view(dot,\"\")\n\n\n\n\n\n# And this tells R to look for an explicit .\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n\n\n\n\n\nIn this book, we’ll write regular expression as \\. and strings that represent the regular expression as \"\\\\.\".\nIf \\ is used as an escape character in regular expressions, how do you match a literal \\? Well you need to escape it, creating the regular expression \\\\. To create that regular expression, you need to use a string, which also needs to escape \\. That means to match a literal \\ you need to write \"\\\\\\\\\" — you need four backslashes to match one!\n\nx <- \"a\\\\b\"\nstr_view(x,\"\")\n\n\n\n\nstr_view(x, \"\\\\\\\\\")\n\n\n\n\n\nAlternatively, you might find it easier to use the raw strings you learned about in Section 16.2.2). That lets you to avoid one layer of escaping:\n\nstr_view(x, r\"(\\\\)\")\n\n\n\n\n\nThe full set of characters with special meanings that need to be escaped is .^$\\|*+?{}[](). In general, look at punctuation characters with suspicion; if your regular expression isn’t matching what you think it should, check if you’ve used any of these characters.\n\n17.2.2 Anchors\nBy default, regular expressions will match any part of a string. If you want to match at the start of end you need to anchor the regular expression using ^ or $.\n\n\n^ to match the start of the string.\n\n$ to match the end of the string.\n\n\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_view(x, \"a\")  # match \"a\" anywhere\n\n\n\n\nstr_view(x, \"^a\") # match \"a\" at start\n\n\n\n\nstr_view(x, \"a$\") # match \"a\" at end\n\n\n\n\n\nTo remember which is which, try this mnemonic which Hadley learned from Evan Misshula: if you begin with power (^), you end up with money ($). It’s tempting to put $ at the start, because that’s how we write sums of money, but it’s not what regular expressions want.\nTo force a regular expression to only match the full string, anchor it with both ^ and $:\n\nx <- c(\"apple pie\", \"apple\", \"apple cake\")\nstr_view(x, \"apple\")\n\n\n\n\nstr_view(x, \"^apple$\")\n\n\n\n\n\nYou can also match the boundary between words (i.e. the start or end of a word) with \\b. This is not that useful in R code, but it can be handy when searching in RStudio. It’s useful to find the name of a function that’s a component of other functions. For example, if to find all uses of sum(), you can search for \\bsum\\b to avoid matching summarise, summary, rowsum and so on:\n\nx <- c(\"summary(x)\", \"summarise(df)\", \"rowsum(x)\", \"sum(x)\")\nstr_view(x, \"sum\")\n\n\n\n\nstr_view(x, \"\\\\bsum\\\\b\")\n\n\n\n\n\nWhen used alone these anchors will produce a zero-width match:\n\nstr_view_all(\"abc\", c(\"$\", \"^\", \"\\\\b\"))\n\n\n\n\n\n\n17.2.3 Character classes\nA character class, or character set, allows you to match any character in a set. The basic syntax lists each character you want to match inside of [], so [abc] will match a, b, or c. Inside of [] only -, ^, and \\ have special meanings:\n\n\n- defines a range. [a-z] matches any lower case letter and [0-9] matches any number.\n\n^ takes the inverse of the set. [^abc]: matches anything except a, b, or c.\n\n\\ escapes special characters so [\\^\\-\\]]: matches ^, -, or ].\n\n\nstr_view_all(\"abcd12345-!@#%.\", c(\"[abc]\", \"[a-z]\", \"[^a-z0-9]\"))\n\n\n\n\n\n# You need an escape to match characters that are otherwise\n# special inside of []\nstr_view_all(\"a-b-c\", \"[a\\\\-c]\")\n\n\n\n\n\nRemember that regular expressions are case sensitive so if you want to match any lowercase or uppercase letter, you’d need to write [a-zA-Z0-9].\n\n17.2.4 Shorthand character classes\nThere are a few character classes that are used so commonly that they get their own shortcut. You’ve already seen ., which matches any character apart from a newline. There are three other particularly useful pairs:\n\n\n\\d: matches any digit;\\D matches anything that isn’t a digit.\n\n\\s: matches any whitespace (e.g. space, tab, newline);\\S matches anything that isn’t whitespace.\n\n\\w matches any “word” character, i.e. letters and numbers;\\W, matches any non-word character.\n\nRemember, to create a regular expression containing \\d or \\s, you’ll need to escape the \\ for the string, so you’ll type \"\\\\d\" or \"\\\\s\". The following code demonstrates the different shortcuts with a selection of letters, numbers, and punctuation characters.\n\nstr_view_all(\"abcd12345!@#%. \", \"\\\\d+\")\n\n\n\n\nstr_view_all(\"abcd12345!@#%. \", \"\\\\D+\")\n\n\n\n\nstr_view_all(\"abcd12345!@#%. \", \"\\\\w+\")\n\n\n\n\nstr_view_all(\"abcd12345!@#%. \", \"\\\\W+\")\n\n\n\n\nstr_view_all(\"abcd12345!@#%. \", \"\\\\s+\")\n\n\n\n\nstr_view_all(\"abcd12345!@#%. \", \"\\\\S+\")\n\n\n\n\n\n\n17.2.5 Quantifiers\nThe quantifiers control how many times a pattern matches. In Chapter 16 you learned about ? (0 or 1 matches), + (1 or more matches), and * (0 or more matches). For example, colou?r will match American or British spelling, \\d+ will match one or more digits, and \\s? will optionally match a single whitespace.\nYou can also specify the number of matches precisely:\n\n\n{n}: exactly n\n\n{n,}: n or more\n\n{n,m}: between n and m\n\nThe following code shows how this works for a few simple examples using to \\b match the start or end of a word.\n\nx <- \" x xx xxx xxxx\"\nstr_view_all(x, \"\\\\bx{2}\")\n\n\n\n\nstr_view_all(x, \"\\\\bx{2,}\")\n\n\n\n\nstr_view_all(x, \"\\\\bx{1,3}\")\n\n\n\n\nstr_view_all(x, \"\\\\bx{2,3}\")\n\n\n\n\n\n\n17.2.6 Alternation\nYou can use alternation to pick between one or more alternative patterns. Here are a few examples:\n\nMatch apple, pear, or banana: apple|pear|banana.\nMatch three letters or two digits: \\w{3}|\\d{2}.\n\n17.2.7 Parentheses and operator precedence\nWhat does ab+ match? Does it match “a” followed by one or more “b”s, or does it match “ab” repeated any number of times? What does ^a|b$ match? Does it match the complete string a or the complete string b, or does it match a string starting with a or a string starting with “b”? The answer to these questions is determined by operator precedence, similar to the PEMDAS or BEDMAS rules you might have learned in school for what a + b * c.\nYou already know that a + b * c is equivalent to a + (b * c) not (a + b) * c because * has high precedence and + has lower precedence: you compute * before +. In regular expressions, quantifiers have high precedence and alternation has low precedence. That means ab+ is equivalent to a(b+), and ^a|b$ is equivalent to (^a)|(b$). Just like with algebra, you can use parentheses to override the usual order (because they have the highest precedence of all).\nTechnically the escape, character classes, and parentheses are all operators that also have precedence. But these tend to be less likely to cause confusion because they mostly behave how you expect: it’s unlikely that you’d think that \\(s|d) would mean (\\s)|(\\d).\n\n17.2.8 Exercises\n\nHow would you match the literal string \"$^$\"?\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\nStart with “y”.\nDon’t start with “y”.\nEnd with “x”.\nAre exactly three letters long. (Don’t cheat by using str_length()!)\nHave seven letters or more.\n\nSince words is long, you might want to use the match argument to str_view() to show only the matching or non-matching words.\n\nCreate regular expressions that match the British or American spellings of the following words: grey/gray, modelling/modeling, summarize/summarise, aluminium/aluminum, defence/defense, analog/analogue, center/centre, sceptic/skeptic, aeroplane/airplane, arse/ass, doughnut/donut.\nWhat strings will $a match?\nCreate a regular expression that will match telephone numbers as commonly written in your country.\nWrite the equivalents of ?, +, * in {m,n} form.\n\nDescribe in words what these regular expressions match: (read carefully to see if each entry is a regular expression or a string that defines a regular expression.)\n\n^.*$\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\n\n\nSolve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner."
  },
  {
    "objectID": "regexps.html#practice",
    "href": "regexps.html#practice",
    "title": "17  Regular expressions",
    "section": "\n17.3 Practice",
    "text": "17.3 Practice\nTo put these ideas in practice we’ll solve a few semi-authentic problems using the words and sentences datasets built into stringr. words is a list of common English words and sentences is a set of simple sentences originally used for testing voice transmission.\n\nstr_view(head(words),\"\")\n\n\n\n\nstr_view(head(sentences),\"\")\n\n\n\n\n\nThe following three sections help you practice the components of a pattern by discussing three general techniques: checking you work by creating simple positive and negative controls, combining regular expressions with Boolean algebra, and creating complex patterns using string manipulation.\n\n17.3.1 Check your work\nFirst, let’s find all sentences that start with “The”. Using the ^ anchor alone is not enough:\n\nstr_view(sentences, \"^The\", match = TRUE)\n\n\n\n\n\nBecause it all matches sentences starting with They or Those. We need to make sure that the “e” is the last letter in the word, which we can do by adding adding a word boundary:\n\nstr_view(sentences, \"^The\\\\b\", match = TRUE)\n\n\n\n\n\nWhat about finding all sentences that begin with a pronoun?\n\nstr_view(sentences, \"^She|He|It|They\\\\b\", match = TRUE)\n\n\n\n\n\nA quick inspection of the results shows that we’re getting some spurious matches. That’s because we’ve forgotten to use parentheses:\n\nstr_view(sentences, \"^(She|He|It|They)\\\\b\", match = TRUE)\n\n\n\n\n\nYou might wonder how you might spot such a mistake if it didn’t occur in the first few matches. A good technique is to create a few positive and negative matches and use them to test that you pattern works as expected.\n\npos <- c(\"He is a boy\", \"She had a good time\")\nneg <- c(\"Shells come from the sea\", \"Hadley said 'It's a great day'\")\n\npattern <- \"^(She|He|It|They)\\\\b\"\nstr_detect(pos, pattern)\n#> [1] TRUE TRUE\nstr_detect(neg, pattern)\n#> [1] FALSE FALSE\n\nIt’s typically much easier to come up with positive examples than negative examples, because it takes some time until you’re good enough with regular expressions to predict where your weaknesses are. Nevertheless they’re still useful; even if you don’t get them correct right away, you can slowly accumulate them as you work on your problem. If you you later get more into programming and learn about unit tests, you can then turn these examples into automated test that ensure you never you never make the same mistake twice.)\n\n17.3.2 Boolean operations\nImagine we want to find words that only contain consonants. One technique is to create a character class that contains all letters except for the vowels ([^aeiou]), then allow that to match any number of letters ([^aeiou]+), then force it to match the whole string by anchoring to the beginning and the end (^[^aeiou]+$):\n\nstr_view(words, \"^[^aeiou]+$\", match = TRUE)\n\n\n\n\n\nBut we can make this problem a bit easier by flipping the problem around. Instead of looking for words that contain only consonants, we could look for words that don’t contain any vowels:\n\nwords[!str_detect(words, \"[aeiou]\")]\n#> [1] \"by\"  \"dry\" \"fly\" \"mrs\" \"try\" \"why\"\n\nThis is a useful technique whenever you’re dealing with logical combinations, particularly those involving “and” or “not”. For example, imagine if you want to find all words that contain “a” and “b”. There’s no “and” operator built in to regular expressions so we have to tackle it by looking for all words that contain an “a” followed by a “b”, or a “b” followed by an “a”:\n\nwords[str_detect(words, \"a.*b|b.*a\")]\n#>  [1] \"able\"      \"about\"     \"absolute\"  \"available\" \"baby\"      \"back\"     \n#>  [7] \"bad\"       \"bag\"       \"balance\"   \"ball\"      \"bank\"      \"bar\"      \n#> [13] \"base\"      \"basis\"     \"bear\"      \"beat\"      \"beauty\"    \"because\"  \n#> [19] \"black\"     \"board\"     \"boat\"      \"break\"     \"brilliant\" \"britain\"  \n#> [25] \"debate\"    \"husband\"   \"labour\"    \"maybe\"     \"probable\"  \"table\"\n\nIts simpler to combine the results of two calls to str_detect():\n\nwords[str_detect(words, \"a\") & str_detect(words, \"b\")]\n#>  [1] \"able\"      \"about\"     \"absolute\"  \"available\" \"baby\"      \"back\"     \n#>  [7] \"bad\"       \"bag\"       \"balance\"   \"ball\"      \"bank\"      \"bar\"      \n#> [13] \"base\"      \"basis\"     \"bear\"      \"beat\"      \"beauty\"    \"because\"  \n#> [19] \"black\"     \"board\"     \"boat\"      \"break\"     \"brilliant\" \"britain\"  \n#> [25] \"debate\"    \"husband\"   \"labour\"    \"maybe\"     \"probable\"  \"table\"\n\nWhat if we wanted to see if there was a word that contains all vowels? If we did it with patterns we’d need to generate 5! (120) different patterns:\n\nwords[str_detect(words, \"a.*e.*i.*o.*u\")]\n#> character(0)\n# ...\nwords[str_detect(words, \"u.*o.*i.*e.*a\")]\n#> character(0)\n\nIt’s much simpler to combine six calls to str_detect():\n\nwords[\n  str_detect(words, \"a\") &\n  str_detect(words, \"e\") &\n  str_detect(words, \"i\") &\n  str_detect(words, \"o\") &\n  str_detect(words, \"u\")\n]\n#> character(0)\n\nIn general, if you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one.\n\n17.3.3 Creating a pattern with code\nWhat if we wanted to find all sentences that mention a color? The basic idea is simple: we just combine alternation with word boundaries.\n\nstr_view(sentences, \"\\\\b(red|green|blue)\\\\b\", match = TRUE)\n\n\n\n\n\nBut it would be tedious to construct this pattern by hand. Wouldn’t it be nice if we could store the colours in a vector?\n\nrgb <- c(\"red\", \"green\", \"blue\")\n\nWell, we can! We’d just need to create the pattern from the vector using str_c() and str_flatten()\n\nstr_c(\"\\\\b(\", str_flatten(rgb, \"|\"), \")\\\\b\")\n#> [1] \"\\\\b(red|green|blue)\\\\b\"\n\nWe could make this pattern more comprehensive if we had a good list of colors. One place we could start from is the list of built-in colours that R can use for plots:\n\ncolors()[1:27]\n#>  [1] \"white\"          \"aliceblue\"      \"antiquewhite\"   \"antiquewhite1\" \n#>  [5] \"antiquewhite2\"  \"antiquewhite3\"  \"antiquewhite4\"  \"aquamarine\"    \n#>  [9] \"aquamarine1\"    \"aquamarine2\"    \"aquamarine3\"    \"aquamarine4\"   \n#> [13] \"azure\"          \"azure1\"         \"azure2\"         \"azure3\"        \n#> [17] \"azure4\"         \"beige\"          \"bisque\"         \"bisque1\"       \n#> [21] \"bisque2\"        \"bisque3\"        \"bisque4\"        \"black\"         \n#> [25] \"blanchedalmond\" \"blue\"           \"blue1\"\n\nBut first lets element the numbered variants:\n\ncols <- colors()\ncols <- cols[!str_detect(cols, \"\\\\d\")]\ncols[1:27]\n#>  [1] \"white\"          \"aliceblue\"      \"antiquewhite\"   \"aquamarine\"    \n#>  [5] \"azure\"          \"beige\"          \"bisque\"         \"black\"         \n#>  [9] \"blanchedalmond\" \"blue\"           \"blueviolet\"     \"brown\"         \n#> [13] \"burlywood\"      \"cadetblue\"      \"chartreuse\"     \"chocolate\"     \n#> [17] \"coral\"          \"cornflowerblue\" \"cornsilk\"       \"cyan\"          \n#> [21] \"darkblue\"       \"darkcyan\"       \"darkgoldenrod\"  \"darkgray\"      \n#> [25] \"darkgreen\"      \"darkgrey\"       \"darkkhaki\"\n\nThen we can turn this into one giant pattern:\n\npattern <- str_c(\"\\\\b(\", str_flatten(cols, \"|\"), \")\\\\b\")\nstr_view(sentences, pattern, match = TRUE)\n\n\n\n\n\nIn this example cols only contains numbers and letters so you don’t need to worry about metacharacters. But in general, when creating patterns from existing strings it’s good practice to run through str_escape() which will automatically add \\ in front of otherwise special characters.\n\n17.3.4 Exercises\n\nConstruct patterns to find evidence for and against the rule “i before e except after c”?\n\ncolors() contains a number of modifiers like “lightgray” and “darkblue”. How could you automatically identify these modifiers? (Think about how you might detect and removed what is being modified).\nCreate a regular expression that finds any use of base R dataset. You can get a list of these datasets via a special use of the data() function: data(package = \"datasets\")$results[, \"Item\"]. Note that a number of old datasets are individual vectors; these contain the name of the grouping “data frame” in parentheses, so you’ll need to also strip these off."
  },
  {
    "objectID": "regexps.html#grouping-and-capturing",
    "href": "regexps.html#grouping-and-capturing",
    "title": "17  Regular expressions",
    "section": "\n17.4 Grouping and capturing",
    "text": "17.4 Grouping and capturing\nLike in algebra, parentheses are an important tool for controlling the order in which pattern operations are applied. But they also have an important additional effect: they create capturing groups that allow you to use to sub-components of the match. There are three main ways you can use them:\n\nTo match a repeated pattern.\nTo include a matched pattern in the replacement.\nTo extract individual components of the match.\n\nIf needed, there’s also a special form of parentheses that only affect operator precedence without creating capturing a group. All of these are these described below.\n\n17.4.1 Matching a repeated pattern\nYou can refer back to previously matched text inside parentheses by using back reference. Back references are usually numbered: \\1 refers to the match contained in the first parentheses, \\2 in the the second parentheses, and so on. For example, the following pattern finds all fruits that have a repeated pair of letters:\n\nstr_view(fruit, \"(..)\\\\1\", match = TRUE)\n\n\n\n\n\nAnd this one finds all words that start and end with the same pair of letters:\n\nstr_view(words, \"^(..).*\\\\1$\", match = TRUE)\n\n\n\n\n\n\n17.4.2 Replacing with the matched pattern\nYou can also use back references when replacing with str_replace() and str_replace_all(). The following code will switch the order of the second and third words:\n\nsentences |> \n  str_replace(\"(\\\\w+) (\\\\w+) (\\\\w+)\", \"\\\\1 \\\\3 \\\\2\") |> \n  head(5)\n#> [1] \"The canoe birch slid on the smooth planks.\" \n#> [2] \"Glue sheet the to the dark blue background.\"\n#> [3] \"It's to easy tell the depth of a well.\"     \n#> [4] \"These a days chicken leg is a rare dish.\"   \n#> [5] \"Rice often is served in round bowls.\"\n\nYou’ll sometimes see people using str_replace() to extract a single match:\n\npattern <- \"^.*the ([^ .,]+).*$\"\nsentences |> \n  str_subset(pattern) |> \n  str_replace(pattern, \"\\\\1\") |> \n  head(10)\n#>  [1] \"smooth\"  \"dark\"    \"depth\"   \"parked\"  \"sun\"     \"clear\"   \"ball\"   \n#>  [8] \"woman\"   \"evening\" \"man's\"\n\nBut you’re generally better off using str_match() or tidyr::separate_groups(), which you’ll learn about next.\n\n17.4.3 Extracting groups\nstringr provides a lower-level function for extract matches called str_match(). But it returns a matrix, so isn’t as easy to work with:\n\nsentences |> \n  str_match(\"the (\\\\w+) (\\\\w+)\") |> \n  head()\n#>      [,1]                [,2]     [,3]    \n#> [1,] \"the smooth planks\" \"smooth\" \"planks\"\n#> [2,] \"the sheet to\"      \"sheet\"  \"to\"    \n#> [3,] \"the depth of\"      \"depth\"  \"of\"    \n#> [4,] NA                  NA       NA      \n#> [5,] NA                  NA       NA      \n#> [6,] NA                  NA       NA\n\nInstead, we recommend using tidyr’s separate_groups() which creates a column for each capturing group.\n\n17.4.4 Named groups\nIf you have many groups, referring to them by position can get confusing. It’s possible to give them a name with (?<name>…). You can refer to it with \\k<name>.\n\nstr_view(words, \"^(?<first>.).*\\\\k<first>$\", match = TRUE)\n\n\n\n\n\nThis verbosity is a good fit with comments = TRUE:\n\npattern <- regex(\n  r\"(\n    ^           # start at the beginning of the string\n    (?<first>.) # and match the <first> letter\n    .*          # then match any other letters\n    \\k<first>$  # ensuring the last letter is the same as the <first>\n  )\", \n  comments = TRUE\n)\n\nYou can also use named groups as an alternative to the col_names argument to tidyr::separate_groups().\n\n17.4.5 Non-capturing groups\nOccasionally, you’ll want to use parentheses without creating matching groups. You can create a non-capturing group with (?:).\n\nx <- c(\"a gray cat\", \"a grey dog\")\nstr_match(x, \"(gr(e|a)y)\")\n#>      [,1]   [,2]   [,3]\n#> [1,] \"gray\" \"gray\" \"a\" \n#> [2,] \"grey\" \"grey\" \"e\"\nstr_match(x, \"(gr(?:e|a)y)\")\n#>      [,1]   [,2]  \n#> [1,] \"gray\" \"gray\"\n#> [2,] \"grey\" \"grey\"\n\nTypically, however, you’ll find it easier to just ignore that result by setting the col_name to NA:\n\n17.4.6 Exercises\n\n\nDescribe, in words, what these expressions will match:\n\n(.)\\1\\1\n\"(.)(.)\\\\2\\\\1\"\n(..)\\1\n\"(.).\\\\1.\\\\1\"\n\"(.)(.)(.).*\\\\3\\\\2\\\\1\"\n\n\n\nConstruct regular expressions to match words that:\n\nWho’s first letter is the same as the last letter, and the second letter is the same as the second to last letter.\nContain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.)"
  },
  {
    "objectID": "regexps.html#flags",
    "href": "regexps.html#flags",
    "title": "17  Regular expressions",
    "section": "\n17.5 Flags",
    "text": "17.5 Flags\nThe are a number of settings, often called flags in other programming languages, that you can use to control some of the details of the regex. In stringr, you can use these by wrapping the pattern in a call to regex():\n\n# The regular call:\nstr_view(fruit, \"nana\")\n# is shorthand for\nstr_view(fruit, regex(\"nana\"))\n\nThe most useful flag is probably ignore_case = TRUE because it allows characters to match either their uppercase or lowercase forms:\n\nbananas <- c(\"banana\", \"Banana\", \"BANANA\")\nstr_view(bananas, \"banana\")\n\n\n\n\nstr_view(bananas, regex(\"banana\", ignore_case = TRUE))\n\n\n\n\n\nIf you’re doing a lot of work with multiline strings (i.e. strings that contain \\n), multiline and dotall can also be useful. dotall = TRUE allows . to match everything, including \\n:\n\nx <- \"Line 1\\nLine 2\\nLine 3\"\nstr_view_all(x, \".L\")\n\n\n\n\nstr_view_all(x, regex(\".L\", dotall = TRUE))\n\n\n\n\n\nAnd multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string:\n\nx <- \"Line 1\\nLine 2\\nLine 3\"\nstr_view_all(x, \"^Line\")\n\n\n\n\nstr_view_all(x, regex(\"^Line\", multiline = TRUE))\n\n\n\n\n\nFinally, if you’re writing a complicated regular expression and you’re worried you might not understand it in the future, comments = TRUE can be extremely useful. It allows you to use comments and whitespace to make complex regular expressions more understandable. Spaces and new lines are ignored, as is everything after #. (Note that we use a raw string here to minimize the number of escapes needed)\n\nphone <- regex(r\"(\n  \\(?     # optional opening parens\n  (\\d{3}) # area code\n  [)\\ -]?  # optional closing parens, space, or dash\n  (\\d{3}) # another three numbers\n  [\\ -]?   # optional space or dash\n  (\\d{3}) # three more numbers\n  )\", comments = TRUE)\n\nstr_match(\"514-791-8141\", phone)\n#>      [,1]          [,2]  [,3]  [,4] \n#> [1,] \"514-791-814\" \"514\" \"791\" \"814\"\n\nIf you’re using comments and want to match a space, newline, or #, you’ll need to escape it:\n\nstr_view(\"x x #\", regex(\"x #\", comments = TRUE))\n\n\n\n\nstr_view(\"x x #\", regex(r\"(x\\ \\#)\", comments = TRUE))"
  },
  {
    "objectID": "factors.html",
    "href": "factors.html",
    "title": "18  Factors",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is largely complete and just needs final proof reading. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "factors.html#introduction",
    "href": "factors.html#introduction",
    "title": "18  Factors",
    "section": "\n18.1 Introduction",
    "text": "18.1 Introduction\nFactors are used for categorical variables, variables that have a fixed and known set of possible values. They are also useful when you want to display character vectors in a non-alphabetical order.\nIf you want to learn more about factors after reading this chapter, we recommend reading Amelia McNamara and Nicholas Horton’s paper, Wrangling categorical data in R. This paper lays out some of the history discussed in stringsAsFactors: An unauthorized biography and stringsAsFactors = <sigh>, and compares the tidy approaches to categorical data outlined in this book with base R methods. An early version of the paper helped motivate and scope the forcats package; thanks Amelia & Nick!\n\n18.1.1 Prerequisites\nBase R some basic tools for creating and manipulating factors. We’ll supplement these with the forcats package, which is part of the core tidyverse. It provides tools for dealing with categorical variables (and it’s an anagram of factors!) using a wide range of helpers for working with factors.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "factors.html#factor-basics",
    "href": "factors.html#factor-basics",
    "title": "18  Factors",
    "section": "\n18.2 Factor basics",
    "text": "18.2 Factor basics\nImagine that you have a variable that records month:\n\nx1 <- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\n\nUsing a string to record this variable has two problems:\n\n\nThere are only twelve possible months, and there’s nothing saving you from typos:\n\nx2 <- c(\"Dec\", \"Apr\", \"Jam\", \"Mar\")\n\n\n\nIt doesn’t sort in a useful way:\n\nsort(x1)\n#> [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\n\n\n\nYou can fix both of these problems with a factor. To create a factor you must start by creating a list of the valid levels:\n\nmonth_levels <- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\n\nNow you can create a factor:\n\ny1 <- factor(x1, levels = month_levels)\ny1\n#> [1] Dec Apr Jan Mar\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\nsort(y1)\n#> [1] Jan Mar Apr Dec\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nAnd any values not in the level will be silently converted to NA:\n\ny2 <- factor(x2, levels = month_levels)\ny2\n#> [1] Dec  Apr  <NA> Mar \n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nIf you want a warning, you can use readr::parse_factor():\n\ny2 <- parse_factor(x2, levels = month_levels)\n#> Warning: 1 parsing failure.\n#> row col           expected actual\n#>   3  -- value in level set    Jam\n\nIf you omit the levels, they’ll be taken from the data in alphabetical order:\n\nfactor(x1)\n#> [1] Dec Apr Jan Mar\n#> Levels: Apr Dec Jan Mar\n\nSometimes you’d prefer that the order of the levels match the order of the first appearance in the data. You can do that when creating the factor by setting levels to unique(x), or after the fact, with fct_inorder():\n\nf1 <- factor(x1, levels = unique(x1))\nf1\n#> [1] Dec Apr Jan Mar\n#> Levels: Dec Apr Jan Mar\n\nf2 <- x1 |> factor() |> fct_inorder()\nf2\n#> [1] Dec Apr Jan Mar\n#> Levels: Dec Apr Jan Mar\n\nIf you ever need to access the set of valid levels directly, you can do so with levels():\n\nlevels(f2)\n#> [1] \"Dec\" \"Apr\" \"Jan\" \"Mar\""
  },
  {
    "objectID": "factors.html#general-social-survey",
    "href": "factors.html#general-social-survey",
    "title": "18  Factors",
    "section": "\n18.3 General Social Survey",
    "text": "18.3 General Social Survey\nFor the rest of this chapter, we’re going to use forcats::gss_cat. It’s a sample of data from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago. The survey has thousands of questions, so in gss_cat Hadley selected a handful that will illustrate some common challenges you’ll encounter when working with factors.\n\ngss_cat\n#> # A tibble: 21,483 × 9\n#>    year marital         age race  rincome        partyid     relig denom tvhours\n#>   <int> <fct>         <int> <fct> <fct>          <fct>       <fct> <fct>   <int>\n#> 1  2000 Never married    26 White $8000 to 9999  Ind,near r… Prot… Sout…      12\n#> 2  2000 Divorced         48 White $8000 to 9999  Not str re… Prot… Bapt…      NA\n#> 3  2000 Widowed          67 White Not applicable Independent Prot… No d…       2\n#> 4  2000 Never married    39 White Not applicable Ind,near r… Orth… Not …       4\n#> 5  2000 Divorced         25 White Not applicable Not str de… None  Not …       1\n#> 6  2000 Married          25 White $20000 - 24999 Strong dem… Prot… Sout…      NA\n#> # … with 21,477 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n(Remember, since this dataset is provided by a package, you can get more information about the variables with ?gss_cat.)\nWhen factors are stored in a tibble, you can’t see their levels so easily. One way to view them is with count():\n\ngss_cat |>\n  count(race)\n#> # A tibble: 3 × 2\n#>   race      n\n#>   <fct> <int>\n#> 1 Other  1959\n#> 2 Black  3129\n#> 3 White 16395\n\nOr with a bar chart:\n\nggplot(gss_cat, aes(race)) +\n  geom_bar()\n\n\n\n\nWhen working with factors, the two most common operations are changing the order of the levels, and changing the values of the levels. Those operations are described in the sections below.\n\n18.3.1 Exercise\n\nExplore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?\nWhat is the most common relig in this survey? What’s the most common partyid?\nWhich relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?"
  },
  {
    "objectID": "factors.html#modifying-factor-order",
    "href": "factors.html#modifying-factor-order",
    "title": "18  Factors",
    "section": "\n18.4 Modifying factor order",
    "text": "18.4 Modifying factor order\nIt’s often useful to change the order of the factor levels in a visualization. For example, imagine you want to explore the average number of hours spent watching TV per day across religions:\n\nrelig_summary <- gss_cat |>\n  group_by(relig) |>\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(relig_summary, aes(tvhours, relig)) + \n  geom_point()\n\n\n\n\nIt is hard to read this plot because there’s no overall pattern. We can improve it by reordering the levels of relig using fct_reorder(). fct_reorder() takes three arguments:\n\n\nf, the factor whose levels you want to modify.\n\nx, a numeric vector that you want to use to reorder the levels.\nOptionally, fun, a function that’s used if there are multiple values of x for each value of f. The default value is median.\n\n\nggplot(relig_summary, aes(tvhours, fct_reorder(relig, tvhours))) +\n  geom_point()\n\n\n\n\nReordering religion makes it much easier to see that people in the “Don’t know” category watch much more TV, and Hinduism & Other Eastern religions watch much less.\nAs you start making more complicated transformations, we recommend moving them out of aes() and into a separate mutate() step. For example, you could rewrite the plot above as:\n\nrelig_summary |>\n  mutate(\n    relig = fct_reorder(relig, tvhours)\n  ) |>\n  ggplot(aes(tvhours, relig)) +\n  geom_point()\n\nWhat if we create a similar plot looking at how average age varies across reported income level?\n\nrincome_summary <- gss_cat |>\n  group_by(rincome) |>\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(rincome_summary, aes(age, fct_reorder(rincome, age))) + \n  geom_point()\n\n\n\n\nHere, arbitrarily reordering the levels isn’t a good idea! That’s because rincome already has a principled order that we shouldn’t mess with. Reserve fct_reorder() for factors whose levels are arbitrarily ordered.\nHowever, it does make sense to pull “Not applicable” to the front with the other special levels. You can use fct_relevel(). It takes a factor, f, and then any number of levels that you want to move to the front of the line.\n\nggplot(rincome_summary, aes(age, fct_relevel(rincome, \"Not applicable\"))) +\n  geom_point()\n\n\n\n\nWhy do you think the average age for “Not applicable” is so high?\nAnother type of reordering is useful when you are coloring the lines on a plot. fct_reorder2(f, x, y) reorders the factor f by the y values associated with the largest x values. This makes the plot easier to read because the colors of the line at the far right of the plot will line up with the legend.\n\nby_age <- gss_cat |>\n  filter(!is.na(age)) |>\n  count(age, marital) |>\n  group_by(age) |>\n  mutate(\n    prop = n / sum(n)\n  )\n\nggplot(by_age, aes(age, prop, colour = marital)) +\n  geom_line(na.rm = TRUE)\n\nggplot(by_age, aes(age, prop, colour = fct_reorder2(marital, age, prop))) +\n  geom_line() +\n  labs(colour = \"marital\")\n\n\n\n\n\n\n\n\n\n\n\nFinally, for bar plots, you can use fct_infreq() to order levels in decreasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables. Combine it with fct_rev() if you want them in increasing frequency so that in the bar plot largest values are on the right, not the left.\n\ngss_cat |>\n  mutate(marital = marital |> fct_infreq() |> fct_rev()) |>\n  ggplot(aes(marital)) +\n  geom_bar()\n\n\n\n\n\n18.4.1 Exercises\n\nThere are some suspiciously high numbers in tvhours. Is the mean a good summary?\nFor each factor in gss_cat identify whether the order of the levels is arbitrary or principled.\nWhy did moving “Not applicable” to the front of the levels move it to the bottom of the plot?"
  },
  {
    "objectID": "factors.html#modifying-factor-levels",
    "href": "factors.html#modifying-factor-levels",
    "title": "18  Factors",
    "section": "\n18.5 Modifying factor levels",
    "text": "18.5 Modifying factor levels\nMore powerful than changing the orders of the levels is changing their values. This allows you to clarify labels for publication, and collapse levels for high-level displays. The most general and powerful tool is fct_recode(). It allows you to recode, or change, the value of each level. For example, take the gss_cat$partyid:\n\ngss_cat |> count(partyid)\n#> # A tibble: 10 × 2\n#>   partyid                n\n#>   <fct>              <int>\n#> 1 No answer            154\n#> 2 Don't know             1\n#> 3 Other party          393\n#> 4 Strong republican   2314\n#> 5 Not str republican  3032\n#> 6 Ind,near rep        1791\n#> # … with 4 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThe levels are terse and inconsistent. Let’s tweak them to be longer and use a parallel construction. Like most rename and recoding functions in the tidyverse, the new values go on the left and the old values go on the right:\n\ngss_cat |>\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\"\n    )\n  ) |>\n  count(partyid)\n#> # A tibble: 10 × 2\n#>   partyid                   n\n#>   <fct>                 <int>\n#> 1 No answer               154\n#> 2 Don't know                1\n#> 3 Other party             393\n#> 4 Republican, strong     2314\n#> 5 Republican, weak       3032\n#> 6 Independent, near rep  1791\n#> # … with 4 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nfct_recode() will the leave levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.\nTo combine groups, you can assign multiple old levels to the same new level:\n\ngss_cat |>\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\",\n      \"Other\"                 = \"No answer\",\n      \"Other\"                 = \"Don't know\",\n      \"Other\"                 = \"Other party\"\n    )\n  ) |>\n  count(partyid)\n#> # A tibble: 8 × 2\n#>   partyid                   n\n#>   <fct>                 <int>\n#> 1 Other                   548\n#> 2 Republican, strong     2314\n#> 3 Republican, weak       3032\n#> 4 Independent, near rep  1791\n#> 5 Independent            4119\n#> 6 Independent, near dem  2499\n#> # … with 2 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nUse this technique with care: if you group together categories that are truly different you will end up with misleading results.\nIf you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode(). For each new variable, you can provide a vector of old levels:\n\ngss_cat |>\n  mutate(\n    partyid = fct_collapse(partyid,\n      \"other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n      \"rep\" = c(\"Strong republican\", \"Not str republican\"),\n      \"ind\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n      \"dem\" = c(\"Not str democrat\", \"Strong democrat\")\n    )\n  ) |>\n  count(partyid)\n#> # A tibble: 4 × 2\n#>   partyid     n\n#>   <fct>   <int>\n#> 1 other     548\n#> 2 rep      5346\n#> 3 ind      8409\n#> 4 dem      7180\n\nSometimes you just want to lump together the small groups to make a plot or table simpler. That’s the job of the fct_lump_*() family of functions. fct_lump_lowfreq() is a simple starting point that progressively lumps the smallest groups categories into “Other”, always keeping “Other” as the smallest category.\n\ngss_cat |>\n  mutate(relig = fct_lump_lowfreq(relig)) |>\n  count(relig)\n#> # A tibble: 2 × 2\n#>   relig          n\n#>   <fct>      <int>\n#> 1 Protestant 10846\n#> 2 Other      10637\n\nIn this case it’s not very helpful: it is true that the majority of Americans in this survey are Protestant, but we’d probably like to see some more details! Instead, we can use the fct_lump_n() to specify that we want exactly 10 groups:\n\ngss_cat |>\n  mutate(relig = fct_lump_n(relig, n = 10)) |>\n  count(relig, sort = TRUE) |>\n  print(n = Inf)\n#> # A tibble: 10 × 2\n#>    relig                       n\n#>    <fct>                   <int>\n#>  1 Protestant              10846\n#>  2 Catholic                 5124\n#>  3 None                     3523\n#>  4 Christian                 689\n#>  5 Other                     458\n#>  6 Jewish                    388\n#>  7 Buddhism                  147\n#>  8 Inter-nondenominational   109\n#>  9 Moslem/islam              104\n#> 10 Orthodox-christian         95\n\nRead the documentation to learn about fct_lump_min() and fct_lump_prop() which are useful in other cases.\n\n18.5.1 Exercises\n\nHow have the proportions of people identifying as Democrat, Republican, and Independent changed over time?\nHow could you collapse rincome into a small set of categories?\nNotice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)"
  },
  {
    "objectID": "factors.html#ordered-factors",
    "href": "factors.html#ordered-factors",
    "title": "18  Factors",
    "section": "\n18.6 Ordered factors",
    "text": "18.6 Ordered factors\nBefore we go on, there’s a special type of factor that needs to be mentioned briefly: ordered factors. Ordered factors, created with ordered(), imply a strict ordering and equal distance between levels: the first level is “less than” the second level by the same amount that the second level is “less than” the third level, and so on.. You can recognize them when printing because they use < between the factor levels:\n\nordered(c(\"a\", \"b\", \"c\"))\n#> [1] a b c\n#> Levels: a < b < c\n\nIn practice, ordered() factors behave very similarly to regular factors. There are only two places where you might notice different behavior:\n\nIf you map an ordered factor to color or fill in ggplot2, it will default to scale_color_viridis()/scale_fill_viridis(), a color scale that implies a ranking.\nIf you use an ordered function in a linear model, it will use “polygonal contrasts”. These are mildly useful, but you are unlikely to have heard of them unless you have a PhD in Statistics, and even then you probably don’t routinely interpret them. If you want to learn more, we recommend vignette(\"contrasts\", package = \"faux\") by Lisa DeBruine.\n\nGiven the arguable utility of these differences, we don’t generally recommend using ordered factors."
  },
  {
    "objectID": "datetimes.html",
    "href": "datetimes.html",
    "title": "19  Dates and times",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "datetimes.html#introduction",
    "href": "datetimes.html#introduction",
    "title": "19  Dates and times",
    "section": "\n19.1 Introduction",
    "text": "19.1 Introduction\nThis chapter will show you how to work with dates and times in R. At first glance, dates and times seem simple. You use them all the time in your regular life, and they don’t seem to cause much confusion. However, the more you learn about dates and times, the more complicated they seem to get. To warm up think about how many days there are in a year, and how many hours there are in a day.\nYou probably remembered that most years have 365 days, but leap years have 366. Do you know the full rule for determining if a year is a leap year1? The number of hours in a day is a little less obvious: most days have 24 hours, but if you use daylight saving time (DST), one day each year has 23 hours and another has 25.\nDates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones, and DST. This chapter won’t teach you every last detail about dates and times, but it will give you a solid grounding of practical skills that will help you with common data analysis challenges.\n\n19.1.1 Prerequisites\nThis chapter will focus on the lubridate package, which makes it easier to work with dates and times in R. lubridate is not part of core tidyverse because you only need it when you’re working with dates/times. We will also need nycflights13 for practice data.\n\nlibrary(tidyverse)\n\nlibrary(lubridate)\nlibrary(nycflights13)"
  },
  {
    "objectID": "datetimes.html#creating-datetimes",
    "href": "datetimes.html#creating-datetimes",
    "title": "19  Dates and times",
    "section": "\n19.2 Creating date/times",
    "text": "19.2 Creating date/times\nThere are three types of date/time data that refer to an instant in time:\n\nA date. Tibbles print this as <date>.\nA time within a day. Tibbles print this as <time>.\nA date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). Tibbles print this as <dttm>. Base R calls these POSIXct, but doesn’t exactly trip off the tongue.\n\nIn this chapter we are going to focus on dates and date-times as R doesn’t have a native class for storing times. If you need one, you can use the hms package.\nYou should always use the simplest possible data type that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we’ll come back to at the end of the chapter.\nTo get the current date or date-time you can use today() or now():\n\ntoday()\n#> [1] \"2022-08-14\"\nnow()\n#> [1] \"2022-08-14 18:37:24 CEST\"\n\nOtherwise, there are three ways you’re likely to create a date/time:\n\nFrom a string.\nFrom individual date-time components.\nFrom an existing date/time object.\n\nThey work as follows.\n\n19.2.1 From strings\nDate/time data often comes as strings. You’ve seen one approach to parsing strings into date-times in date-times. Another approach is to use the helpers provided by lubridate. They automatically work out the format once you specify the order of the component. To use them, identify the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” in the same order. That gives you the name of the lubridate function that will parse your date. For example:\n\nymd(\"2017-01-31\")\n#> [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")\n#> [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")\n#> [1] \"2017-01-31\"\n\nymd() and friends create dates. To create a date-time, add an underscore and one or more of “h”, “m”, and “s” to the name of the parsing function:\n\nymd_hms(\"2017-01-31 20:11:59\")\n#> [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")\n#> [1] \"2017-01-31 08:01:00 UTC\"\n\nYou can also force the creation of a date-time from a date by supplying a timezone:\n\nymd(\"2017-01-31\", tz = \"UTC\")\n#> [1] \"2017-01-31 UTC\"\n\n\n19.2.2 From individual components\nInstead of a single string, sometimes you’ll have the individual components of the date-time spread across multiple columns. This is what we have in the flights data:\n\nflights |> \n  select(year, month, day, hour, minute)\n#> # A tibble: 336,776 × 5\n#>    year month   day  hour minute\n#>   <int> <int> <int> <dbl>  <dbl>\n#> 1  2013     1     1     5     15\n#> 2  2013     1     1     5     29\n#> 3  2013     1     1     5     40\n#> 4  2013     1     1     5     45\n#> 5  2013     1     1     6      0\n#> 6  2013     1     1     5     58\n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nTo create a date/time from this sort of input, use make_date() for dates, or make_datetime() for date-times:\n\nflights |> \n  select(year, month, day, hour, minute) |> \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n#> # A tibble: 336,776 × 6\n#>    year month   day  hour minute departure          \n#>   <int> <int> <int> <dbl>  <dbl> <dttm>             \n#> 1  2013     1     1     5     15 2013-01-01 05:15:00\n#> 2  2013     1     1     5     29 2013-01-01 05:29:00\n#> 3  2013     1     1     5     40 2013-01-01 05:40:00\n#> 4  2013     1     1     5     45 2013-01-01 05:45:00\n#> 5  2013     1     1     6      0 2013-01-01 06:00:00\n#> 6  2013     1     1     5     58 2013-01-01 05:58:00\n#> # … with 336,770 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nLet’s do the same thing for each of the four time columns in flights. The times are represented in a slightly odd format, so we use modulus arithmetic to pull out the hour and minute components. Once we’ve created the date-time variables, we focus in on the variables we’ll explore in the rest of the chapter.\n\nmake_datetime_100 <- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt <- flights |> \n  filter(!is.na(dep_time), !is.na(arr_time)) |> \n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) |> \n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n#> # A tibble: 328,063 × 9\n#>   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#>   <chr>  <chr>     <dbl>     <dbl> <dttm>              <dttm>             \n#> 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n#> 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n#> 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n#> 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n#> 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n#> 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n#> # … with 328,057 more rows, and 3 more variables: arr_time <dttm>,\n#> #   sched_arr_time <dttm>, air_time <dbl>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nWith this data, we can visualize the distribution of departure times across the year:\n\nflights_dt |> \n  ggplot(aes(dep_time)) + \n  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day\n\n\n\n\nOr within a single day:\n\nflights_dt |> \n  filter(dep_time < ymd(20130102)) |> \n  ggplot(aes(dep_time)) + \n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes\n\n\n\n\nNote that when you use date-times in a numeric context (like in a histogram), 1 means 1 second, so a binwidth of 86400 means one day. For dates, 1 means 1 day.\n\n19.2.3 From other types\nYou may want to switch between a date-time and a date. That’s the job of as_datetime() and as_date():\n\nas_datetime(today())\n#> [1] \"2022-08-14 UTC\"\nas_date(now())\n#> [1] \"2022-08-14\"\n\nSometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date().\n\nas_datetime(60 * 60 * 10)\n#> [1] \"1970-01-01 10:00:00 UTC\"\nas_date(365 * 10 + 2)\n#> [1] \"1980-01-01\"\n\n\n19.2.4 Exercises\n\n\nWhat happens if you parse a string that contains invalid dates?\n\nymd(c(\"2010-10-10\", \"bananas\"))\n\n\nWhat does the tzone argument to today() do? Why is it important?\n\nUse the appropriate lubridate function to parse each of the following dates:\n\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 2014"
  },
  {
    "objectID": "datetimes.html#date-time-components",
    "href": "datetimes.html#date-time-components",
    "title": "19  Dates and times",
    "section": "\n19.3 Date-time components",
    "text": "19.3 Date-time components\nNow that you know how to get date-time data into R’s date-time data structures, let’s explore what you can do with them. This section will focus on the accessor functions that let you get and set individual components. The next section will look at how arithmetic works with date-times.\n\n19.3.1 Getting components\nYou can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second().\n\ndatetime <- ymd_hms(\"2026-07-08 12:34:56\")\n\nyear(datetime)\n#> [1] 2026\nmonth(datetime)\n#> [1] 7\nmday(datetime)\n#> [1] 8\n\nyday(datetime)\n#> [1] 189\nwday(datetime)\n#> [1] 4\n\nFor month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.\n\nmonth(datetime, label = TRUE)\n#> [1] Jul\n#> 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nwday(datetime, label = TRUE, abbr = FALSE)\n#> [1] Wednesday\n#> 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\n\nWe can use wday() to see that more flights depart during the week than on the weekend:\n\nflights_dt |> \n  mutate(wday = wday(dep_time, label = TRUE)) |> \n  ggplot(aes(x = wday)) +\n    geom_bar()\n\n\n\n\nThere’s an interesting pattern if we look at the average departure delay by minute within the hour. It looks like flights leaving in minutes 20-30 and 50-60 have much lower delays than the rest of the hour!\n\nflights_dt |> \n  mutate(minute = minute(dep_time)) |> \n  group_by(minute) |> \n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    n = n()) |> \n  ggplot(aes(minute, avg_delay)) +\n    geom_line()\n\n\n\n\nInterestingly, if we look at the scheduled departure time we don’t see such a strong pattern:\n\nsched_dep <- flights_dt |> \n  mutate(minute = minute(sched_dep_time)) |> \n  group_by(minute) |> \n  summarise(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n())\n\nggplot(sched_dep, aes(minute, avg_delay)) +\n  geom_line()\n\n\n\n\nSo why do we see that pattern with the actual departure times? Well, like much data collected by humans, there’s a strong bias towards flights leaving at “nice” departure times. Always be alert for this sort of pattern whenever you work with data that involves human judgement!\n\nggplot(sched_dep, aes(minute, n)) +\n  geom_line()\n\n\n\n\n\n19.3.2 Rounding\nAn alternative approach to plotting individual components is to round the date to a nearby unit of time, with floor_date(), round_date(), and ceiling_date(). Each function takes a vector of dates to adjust and then the name of the unit round down (floor), round up (ceiling), or round to. This, for example, allows us to plot the number of flights per week:\n\nflights_dt |> \n  count(week = floor_date(dep_time, \"week\")) |> \n  ggplot(aes(week, n)) +\n  geom_line() + \n  geom_point()\n\n\n\n\nYou can use rounding to show the distribution of flights across the course of a day by computing the difference between dep_time and the earliest instant of that day:\n\nflights_dt |> \n  mutate(dep_hour = dep_time - floor_date(dep_time, \"day\")) |> \n  ggplot(aes(dep_hour)) +\n    geom_freqpoly(binwidth = 60 * 30)\n#> Don't know how to automatically pick scale for object of type difftime. Defaulting to continuous.\n\n\n\n\nComputing the difference between a pair of date-times yields a difftime (more on that in Section 19.4.3). We can convert that to an hms object to get a more useful x-axis:\n\nflights_dt |> \n  mutate(dep_hour = hms::as_hms(dep_time - floor_date(dep_time, \"day\"))) |> \n  ggplot(aes(dep_hour)) +\n    geom_freqpoly(binwidth = 60 * 30)\n\n\n\n\n\n19.3.3 Modifying components\nYou can also use each accessor function to modify the components of a date/time:\n\n(datetime <- ymd_hms(\"2026-07-08 12:34:56\"))\n#> [1] \"2026-07-08 12:34:56 UTC\"\n\nyear(datetime) <- 2030\ndatetime\n#> [1] \"2030-07-08 12:34:56 UTC\"\nmonth(datetime) <- 01\ndatetime\n#> [1] \"2030-01-08 12:34:56 UTC\"\nhour(datetime) <- hour(datetime) + 1\ndatetime\n#> [1] \"2030-01-08 13:34:56 UTC\"\n\nAlternatively, rather than modifying an existing variabke, you can create a new date-time with update(). This also allows you to set multiple values in one step:\n\nupdate(datetime, year = 2030, month = 2, mday = 2, hour = 2)\n#> [1] \"2030-02-02 02:34:56 UTC\"\n\nIf values are too big, they will roll-over:\n\nupdate(ymd(\"2023-02-01\"), mday = 30)\n#> [1] \"2023-03-02\"\nupdate(ymd(\"2023-02-01\"), hour = 400)\n#> [1] \"2023-02-17 16:00:00 UTC\"\n\n\n19.3.4 Exercises\n\nHow does the distribution of flight times within a day change over the course of the year?\nCompare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.\nCompare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.)\nHow does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why?\nOn what day of the week should you leave if you want to minimise the chance of a delay?\nWhat makes the distribution of diamonds$carat and flights$sched_dep_time similar?\nConfirm my hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed."
  },
  {
    "objectID": "datetimes.html#time-spans",
    "href": "datetimes.html#time-spans",
    "title": "19  Dates and times",
    "section": "\n19.4 Time spans",
    "text": "19.4 Time spans\nNext you’ll learn about how arithmetic with dates works, including subtraction, addition, and division. Along the way, you’ll learn about three important classes that represent time spans:\n\n\ndurations, which represent an exact number of seconds.\n\nperiods, which represent human units like weeks and months.\n\nintervals, which represent a starting and ending point.\n\n\n19.4.1 Durations\nIn R, when you subtract two dates, you get a difftime object:\n\n# How old is Hadley?\nh_age <- today() - ymd(\"1979-10-14\")\nh_age\n#> Time difference of 15645 days\n\nA difftime class object records a time span of seconds, minutes, hours, days, or weeks. This ambiguity can make difftimes a little painful to work with, so lubridate provides an alternative which always uses seconds: the duration.\n\nas.duration(h_age)\n#> [1] \"1351728000s (~42.83 years)\"\n\nDurations come with a bunch of convenient constructors:\n\ndseconds(15)\n#> [1] \"15s\"\ndminutes(10)\n#> [1] \"600s (~10 minutes)\"\ndhours(c(12, 24))\n#> [1] \"43200s (~12 hours)\" \"86400s (~1 days)\"\nddays(0:5)\n#> [1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n#> [4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\ndweeks(3)\n#> [1] \"1814400s (~3 weeks)\"\ndyears(1)\n#> [1] \"31557600s (~1 years)\"\n\nDurations always record the time span in seconds. Larger units are created by converting minutes, hours, days, weeks, and years to seconds: 60 seconds in a minute, 60 minutes in an hour, 24 hours in a day, and 7 days in a week. Larger time units are more problematic. A year is uses the “average” number of days in a year, i.e. 365.25. There’s no way to convert a month to a duration, because there’s just too much variation.\nYou can add and multiply durations:\n\n2 * dyears(1)\n#> [1] \"63115200s (~2 years)\"\ndyears(1) + dweeks(12) + dhours(15)\n#> [1] \"38869200s (~1.23 years)\"\n\nYou can add and subtract durations to and from days:\n\ntomorrow <- today() + ddays(1)\nlast_year <- today() - dyears(1)\n\nHowever, because durations represent an exact number of seconds, sometimes you might get an unexpected result:\n\none_pm <- ymd_hms(\"2026-03-12 13:00:00\", tz = \"America/New_York\")\n\none_pm\n#> [1] \"2026-03-12 13:00:00 EDT\"\none_pm + ddays(1)\n#> [1] \"2026-03-13 13:00:00 EDT\"\n\nWhy is one day after 1pm March 12, 2pm March 13? If you look carefully at the date you might also notice that the time zones have changed. March 12 only has 23 hours because it’s when DST starts, so if we add a full days worth of seconds we end up with a different time.\n\n19.4.2 Periods\nTo solve this problem, lubridate provides periods. Periods are time spans but don’t have a fixed length in seconds, instead they work with “human” times, like days and months. That allows them to work in a more intuitive way:\n\none_pm\n#> [1] \"2026-03-12 13:00:00 EDT\"\none_pm + days(1)\n#> [1] \"2026-03-13 13:00:00 EDT\"\n\nLike durations, periods can be created with a number of friendly constructor functions.\n\nhours(c(12, 24))\n#> [1] \"12H 0M 0S\" \"24H 0M 0S\"\ndays(7)\n#> [1] \"7d 0H 0M 0S\"\nmonths(1:6)\n#> [1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n#> [5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\nYou can add and multiply periods:\n\n10 * (months(6) + days(1))\n#> [1] \"60m 10d 0H 0M 0S\"\ndays(50) + hours(25) + minutes(2)\n#> [1] \"50d 25H 2M 0S\"\n\nAnd of course, add them to dates. Compared to durations, periods are more likely to do what you expect:\n\n# A leap year\nymd(\"2024-01-01\") + dyears(1)\n#> [1] \"2024-12-31 06:00:00 UTC\"\nymd(\"2024-01-01\") + years(1)\n#> [1] \"2025-01-01\"\n\n# Daylight Savings Time\none_pm + ddays(1)\n#> [1] \"2026-03-13 13:00:00 EDT\"\none_pm + days(1)\n#> [1] \"2026-03-13 13:00:00 EDT\"\n\nLet’s use periods to fix an oddity related to our flight dates. Some planes appear to have arrived at their destination before they departed from New York City.\n\nflights_dt |> \n  filter(arr_time < dep_time) \n#> # A tibble: 10,633 × 9\n#>   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#>   <chr>  <chr>     <dbl>     <dbl> <dttm>              <dttm>             \n#> 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n#> 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n#> 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n#> 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n#> 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n#> 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n#> # … with 10,627 more rows, and 3 more variables: arr_time <dttm>,\n#> #   sched_arr_time <dttm>, air_time <dbl>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThese are overnight flights. We used the same date information for both the departure and the arrival times, but these flights arrived on the following day. We can fix this by adding days(1) to the arrival time of each overnight flight.\n\nflights_dt <- flights_dt |> \n  mutate(\n    overnight = arr_time < dep_time,\n    arr_time = arr_time + days(if_else(overnight, 0, 1)),\n    sched_arr_time = sched_arr_time + days(overnight * 1)\n  )\n\nNow all of our flights obey the laws of physics.\n\nflights_dt |> \n  filter(overnight, arr_time < dep_time) \n#> # A tibble: 10,633 × 10\n#>   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#>   <chr>  <chr>     <dbl>     <dbl> <dttm>              <dttm>             \n#> 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n#> 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n#> 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n#> 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n#> 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n#> 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n#> # … with 10,627 more rows, and 4 more variables: arr_time <dttm>,\n#> #   sched_arr_time <dttm>, air_time <dbl>, overnight <lgl>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n19.4.3 Intervals\nIt’s obvious what dyears(1) / ddays(365) should return: one, because durations are always represented by a number of seconds, and a duration of a year is defined as 365 days worth of seconds.\nWhat should years(1) / days(1) return? Well, if the year was 2015 it should return 365, but if it was 2016, it should return 366! There’s not quite enough information for lubridate to give a single clear answer. What it does instead is give an estimate:\n\nyears(1) / days(1)\n#> [1] 365.25\n\nIf you want a more accurate measurement, you’ll have to use an interval. An interval is a pair of starting and ending date times, or you can think of it as a duration with a starting point.\nYou can create an interval by writing start %--% end:\n\ny2023 <- ymd(\"2023-01-01\") %--% ymd(\"2024-01-01\")\ny2024 <- ymd(\"2024-01-01\") %--% ymd(\"2025-01-01\")\n\ny2023\n#> [1] 2023-01-01 UTC--2024-01-01 UTC\ny2024\n#> [1] 2024-01-01 UTC--2025-01-01 UTC\n\nYou could then divide it by days() to find out how many days fit in the year:\n\ny2023 / days(1)\n#> [1] 365\ny2024 / days(1)\n#> [1] 366\n\n\n19.4.4 Summary\nHow do you pick between duration, periods, and intervals? As always, pick the simplest data structure that solves your problem. If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.\n\n19.4.5 Exercises\n\nExplain days(overnight * 1) to someone who has just started learning R. How does it work?\nCreate a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.\nWrite a function that given your birthday (as a date), returns how old you are in years.\nWhy can’t (today() %--% (today() + years(1))) / months(1) work?"
  },
  {
    "objectID": "datetimes.html#time-zones",
    "href": "datetimes.html#time-zones",
    "title": "19  Dates and times",
    "section": "\n19.5 Time zones",
    "text": "19.5 Time zones\nTime zones are an enormously complicated topic because of their interaction with geopolitical entities. Fortunately we don’t need to dig into all the details as they’re not all important for data analysis, but there are a few challenges we’ll need to tackle head on.\n\nThe first challenge is that everyday names of time zones tend to be ambiguous. For example, if you’re American you’re probably familiar with EST, or Eastern Standard Time. However, both Australia and Canada also have EST! To avoid confusion, R uses the international standard IANA time zones. These use a consistent naming scheme {area}/{location}, typically in the form {continent}/{city} or {ocean}/{city}. Examples include “America/New_York”, “Europe/Paris”, and “Pacific/Auckland”.\nYou might wonder why the time zone uses a city, when typically you think of time zones as associated with a country or region within a country. This is because the IANA database has to record decades worth of time zone rules. Over the course of decades, countries change names (or break apart) fairly frequently, but city names tend to stay the same. Another problem is that the name needs to reflect not only the current behavior, but also the complete history. For example, there are time zones for both “America/New_York” and “America/Detroit”. These cities both currently use Eastern Standard Time but in 1969-1972 Michigan (the state in which Detroit is located), did not follow DST, so it needs a different name. It’s worth reading the raw time zone database (available at http://www.iana.org/time-zones) just to read some of these stories!\nYou can find out what R thinks your current time zone is with Sys.timezone():\n\nSys.timezone()\n#> [1] \"Europe/Berlin\"\n\n(If R doesn’t know, you’ll get an NA.)\nAnd see the complete list of all time zone names with OlsonNames():\n\nlength(OlsonNames())\n#> [1] 594\nhead(OlsonNames())\n#> [1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\"\n#> [4] \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"\n\nIn R, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time:\n\nx1 <- ymd_hms(\"2024-06-01 12:00:00\", tz = \"America/New_York\")\nx1\n#> [1] \"2024-06-01 12:00:00 EDT\"\n\nx2 <- ymd_hms(\"2024-06-01 18:00:00\", tz = \"Europe/Copenhagen\")\nx2\n#> [1] \"2024-06-01 18:00:00 CEST\"\n\nx3 <- ymd_hms(\"2024-06-02 04:00:00\", tz = \"Pacific/Auckland\")\nx3\n#> [1] \"2024-06-02 04:00:00 NZST\"\n\nYou can verify that they’re the same time using subtraction:\n\nx1 - x2\n#> Time difference of 0 secs\nx1 - x3\n#> Time difference of 0 secs\n\nUnless otherwise specified, lubridate always uses UTC. UTC (Coordinated Universal Time) is the standard time zone used by the scientific community and is roughly equivalent to GMT (Greenwich Mean Time). It does not have DST, which makes a convenient representation for computation. Operations that combine date-times, like c(), will often drop the time zone. In that case, the date-times will display in your local time zone:\n\nx4 <- c(x1, x2, x3)\nx4\n#> [1] \"2024-06-01 12:00:00 EDT\" \"2024-06-01 12:00:00 EDT\"\n#> [3] \"2024-06-01 12:00:00 EDT\"\n\nYou can change the time zone in two ways:\n\n\nKeep the instant in time the same, and change how it’s displayed. Use this when the instant is correct, but you want a more natural display.\n\nx4a <- with_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4a\n#> [1] \"2024-06-02 02:30:00 +1030\" \"2024-06-02 02:30:00 +1030\"\n#> [3] \"2024-06-02 02:30:00 +1030\"\nx4a - x4\n#> Time differences in secs\n#> [1] 0 0 0\n\n(This also illustrates another challenge of times zones: they’re not all integer hour offsets!)\n\n\nChange the underlying instant in time. Use this when you have an instant that has been labelled with the incorrect time zone, and you need to fix it.\n\nx4b <- force_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4b\n#> [1] \"2024-06-01 12:00:00 +1030\" \"2024-06-01 12:00:00 +1030\"\n#> [3] \"2024-06-01 12:00:00 +1030\"\nx4b - x4\n#> Time differences in hours\n#> [1] -14.5 -14.5 -14.5"
  },
  {
    "objectID": "missing-values.html",
    "href": "missing-values.html",
    "title": "20  Missing values",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "missing-values.html#introduction",
    "href": "missing-values.html#introduction",
    "title": "20  Missing values",
    "section": "\n20.1 Introduction",
    "text": "20.1 Introduction\nYou’ve already learned the basics of missing values earlier in the book. You first saw them in Section 4.4.2 where they interfered with computing summary statistics, and you learned about their infectious nature and how to check for their presence in Section 14.2.2. Now we’ll come back to them in more depth, so you can learn more of the details.\nWe’ll start by discussing some general tools for working with missing values recorded as NAs. We’ll then explore the idea of implicitly missing values, values are that are simply absent from your data, and show some tools you can use to make them explicit. We’ll finish off with a related discussion of empty groups, caused by factor levels that don’t appear in the data.\n\n20.1.1 Prerequisites\nThe functions for working with missing data mostly come from dplyr and tidyr, which are core members of the tidyverse.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "missing-values.html#explicit-missing-values",
    "href": "missing-values.html#explicit-missing-values",
    "title": "20  Missing values",
    "section": "\n20.2 Explicit missing values",
    "text": "20.2 Explicit missing values\nTo begin, let’s explore a few handy tools for creating or eliminating missing explicit values, i.e. cells where you see an NA.\n\n20.2.1 Last observation carried forward\nA common use for missing values is as a data entry convenience. Sometimes data that has been entered by hand, missing values indicate that the value in the previous row has been repeated:\n\ntreatment <- tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         NA,\n  \"Katherine Burke\",  1,         4\n)\n\nYou can fill in these missing values with tidyr::fill(). It works like select(), taking a set of columns:\n\ntreatment |>\n  fill(everything())\n#> # A tibble: 4 × 3\n#>   person           treatment response\n#>   <chr>                <dbl>    <dbl>\n#> 1 Derrick Whitmore         1        7\n#> 2 Derrick Whitmore         2       10\n#> 3 Derrick Whitmore         3       10\n#> 4 Katherine Burke          1        4\n\nThis treatment is sometimes called “last observation carried forward”, or locf for short. You can use the .direction argument to fill in missing values that have been generated in more exotic ways.\n\n20.2.2 Fixed values\nSome times missing values represent some fixed and known value, mostly commonly 0. You can use dplyr::coalesce() to replace them:\n\nx <- c(1, 4, 5, 7, NA)\ncoalesce(x, 0)\n#> [1] 1 4 5 7 0\n\nYou could use mutate() together with across() to apply this treatment to (say) every numeric column in a data frame:\n\ndf |> \n  mutate(across(where(is.numeric), coalesce, 0))\n\n\n20.2.3 Sentinel values\nSometimes you’ll hit the opposite problem where some concrete value actually represents a missing value. This typically arises in data generated by older software that doesn’t have a proper way to represent missing values, so it must instead use some special value like 99 or -999.\nIf possible, handle this when reading in the data, for example, by using the na argument to readr::read_csv(). If you discover the problem later, or your data source doesn’t provide a way to handle on it read, you can use dplyr::na_if():\n\nx <- c(1, 4, 5, 7, -99)\nna_if(x, -99)\n#> [1]  1  4  5  7 NA\n\nYou could apply this transformation to every numeric column in a data frame with the following code.\n\ndf |> \n  mutate(across(where(is.numeric), na_if, -99))\n\n\n20.2.4 NaN\nBefore we continue, there’s one special type of missing value that you’ll encounter from time to time: a NaN (pronounced “nan”), or not a number. It’s not that important to know about because it generally behaves just like NA:\n\nx <- c(NA, NaN)\nx * 10\n#> [1]  NA NaN\nx == 1\n#> [1] NA NA\nis.na(x)\n#> [1] TRUE TRUE\n\nIn the rare case you need to distinguish an NA from a NaN, you can use is.nan(x).\nYou’ll generally encounter a NaN when you perform a mathematical operation that has an indeterminate result:\n\n0 / 0 \n#> [1] NaN\n0 * Inf\n#> [1] NaN\nInf - Inf\n#> [1] NaN\nsqrt(-1)\n#> Warning in sqrt(-1): NaNs produced\n#> [1] NaN"
  },
  {
    "objectID": "missing-values.html#implicit-missing-values",
    "href": "missing-values.html#implicit-missing-values",
    "title": "20  Missing values",
    "section": "\n20.3 Implicit missing values",
    "text": "20.3 Implicit missing values\nSo far we’ve talked about missing values that are explicitly missing, i.e. you can see an NA in your data. But missing values can also be implicitly missing, if an entire row of data is simply absent from the data. Let’s illustrate the difference with a simple data set that records the price of some stock each quarter:\n\nstocks <- tibble(\n  year  = c(2020, 2020, 2020, 2020, 2021, 2021, 2021),\n  qtr   = c(   1,    2,    3,    4,    2,    3,    4),\n  price = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\n\nThis dataset has two missing observations:\n\nThe price in the fourth quarter of 2020 is explicitly missing, because its value is NA.\nThe price for the first quarter of 2021 is implicitly missing, because it simply does not appear in the dataset.\n\nOne way to think about the difference is with this Zen-like koan:\n\nAn explicit missing value is the presence of an absence.\nAn implicit missing value is the absence of a presence.\n\nSometimes you want to make implicit missings explicit in order to have something physical to work with. In other cases, explicit missings are forced upon you by the structure of the data and you want to get rid of them. The following sections discuss some tools for moving between implicit and explicit missingness.\n\n20.3.1 Pivoting\nYou’ve already seen one tool that can make implicit missings explicit and vice versa: pivoting. Making data wider can make implicit missing values explicit because every combination of the rows and new columns must have some value. For example, if we pivot stocks to put the quarter in the columns, both missing values become explicit:\n\nstocks |>\n  pivot_wider(\n    names_from = qtr, \n    values_from = price\n  )\n#> # A tibble: 2 × 5\n#>    year   `1`   `2`   `3`   `4`\n#>   <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  2020  1.88  0.59  0.35 NA   \n#> 2  2021 NA     0.92  0.17  2.66\n\nBy default, making data longer preserves explicit missing values, but if they are structurally missing values that only exist because the data is not tidy, you can drop them (make them implicit) by setting values_drop_na = TRUE. See the examples in Section 6.2 for more details.\n\n20.3.2 Complete\ntidyr::complete() allows you to generate explicit missing values by providing a set of variables that define the combination of rows that should exist. For example, we know that all combinations of year and qtr should exist in the stocks data:\n\nstocks |>\n  complete(year, qtr)\n#> # A tibble: 8 × 3\n#>    year   qtr price\n#>   <dbl> <dbl> <dbl>\n#> 1  2020     1  1.88\n#> 2  2020     2  0.59\n#> 3  2020     3  0.35\n#> 4  2020     4 NA   \n#> 5  2021     1 NA   \n#> 6  2021     2  0.92\n#> # … with 2 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nTypically, you’ll call complete() with names of existing variables, filling in the missing combinations. However, sometimes the individual variables are themselves incomplete, so you can instead provide your own data. For example, you might know that the stocks dataset is supposed to run from 2019 to 2021, so you could explicitly supply those values for year:\n\nstocks |>\n  complete(year = 2019:2021, qtr)\n#> # A tibble: 12 × 3\n#>    year   qtr price\n#>   <dbl> <dbl> <dbl>\n#> 1  2019     1 NA   \n#> 2  2019     2 NA   \n#> 3  2019     3 NA   \n#> 4  2019     4 NA   \n#> 5  2020     1  1.88\n#> 6  2020     2  0.59\n#> # … with 6 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nIf the range of a variable is correct, but not all values are present, you could use full_seq(x, 1) to generate all values from min(x) to max(x) spaced out by 1.\nIn some cases, the complete set of observations can’t be generated by a simple combination of variables. In that case, you can do manually what complete() does for you: create a data frame that contains all the rows that should exist (using whatever combination of techniques you need), then combine it with your original dataset with dplyr::full_join().\n\n20.3.3 Joins\nThis brings us to another important way of revealing implicitly missing observations: joins. Often you can only know that values are missing from one dataset when you go to join it to another. dplyr::anti_join() is particularly useful at revealing these values. The following example shows how two anti_join()s reveal that we’re missing information for four airports and 722 planes.\n\nlibrary(nycflights13)\n\nflights |> \n  distinct(faa = dest) |> \n  anti_join(airports)\n#> Joining, by = \"faa\"\n#> # A tibble: 4 × 1\n#>   faa  \n#>   <chr>\n#> 1 BQN  \n#> 2 SJU  \n#> 3 STT  \n#> 4 PSE\n\nflights |> \n  distinct(tailnum) |> \n  anti_join(planes)\n#> Joining, by = \"tailnum\"\n#> # A tibble: 722 × 1\n#>   tailnum\n#>   <chr>  \n#> 1 N3ALAA \n#> 2 N3DUAA \n#> 3 N542MQ \n#> 4 N730MQ \n#> 5 N9EAMQ \n#> 6 N532UA \n#> # … with 716 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThe default behavior of joins is to succeed if observations in x don’t have a match in y. If you’re worried about this, and you have dplyr 1.1.0 or newer, you can use the new unmatched = \"error\" argument to tell joins to error if any rows in x don’t have a match in y.\n\n20.3.4 Exercises\n\nCan you find any relationship between the carrier and the rows that appear to be missing from planes?"
  },
  {
    "objectID": "missing-values.html#factors-and-empty-groups",
    "href": "missing-values.html#factors-and-empty-groups",
    "title": "20  Missing values",
    "section": "\n20.4 Factors and empty groups",
    "text": "20.4 Factors and empty groups\nA final type of missingness is the empty group, a group that doesn’t contain any observations, which can arise when working with factors. For example, imagine we have a dataset that contains some health information about people:\n\nhealth <- tibble(\n  name = c(\"Ikaia\", \"Oletta\", \"Leriah\", \"Dashay\", \"Tresaun\"),\n  smoker = factor(c(\"no\", \"no\", \"no\", \"no\", \"no\"), levels = c(\"yes\", \"no\")),\n  age = c(34L, 88L, 75L, 47L, 56L),\n)\n\nAnd we want to count the number of smokers with dplyr::count():\n\nhealth |> count(smoker)\n#> # A tibble: 1 × 2\n#>   smoker     n\n#>   <fct>  <int>\n#> 1 no         5\n\nThis dataset only contains non-smokers, but we know that smokers exist; the group of non-smoker is empty. We can request count() to keep all the groups, even those not seen in the data by using .drop = FALSE:\n\nhealth |> count(smoker, .drop = FALSE)\n#> # A tibble: 2 × 2\n#>   smoker     n\n#>   <fct>  <int>\n#> 1 yes        0\n#> 2 no         5\n\nThe same principle applies to ggplot2’s discrete axes, which will also drop levels that don’t have any values. You can force them to display by supplying drop = FALSE to the appropriate discrete axis:\n\nggplot(health, aes(smoker)) +\n  geom_bar() +\n  scale_x_discrete()\n\nggplot(health, aes(smoker)) +\n  geom_bar() +\n  scale_x_discrete(drop = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nThe same problem comes up more generally with dplyr::group_by(). And again you can use .drop = FALSE to preserve all factor levels:\n\nhealth |> \n  group_by(smoker, .drop = FALSE) |> \n  summarise(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  )\n#> Warning in min(age): no non-missing arguments to min; returning Inf\n#> Warning in max(age): no non-missing arguments to max; returning -Inf\n#> # A tibble: 2 × 6\n#>   smoker     n mean_age min_age max_age sd_age\n#>   <fct>  <int>    <dbl>   <dbl>   <dbl>  <dbl>\n#> 1 yes        0      NaN     Inf    -Inf   NA  \n#> 2 no         5       60      34      88   21.6\n\nWe get some interesting results here because when summarizing an empty group, the summary functions are applied to zero-length vectors. There’s an important distinction between empty vectors, which have length 0, and missing values, each of which has length 1.\n\n# A vector containing two missing values\nx1 <- c(NA, NA)\nlength(x1)\n#> [1] 2\n\n# A vector containing nothing\nx2 <- numeric()\nlength(x2)\n#> [1] 0\n\nAll summary functions work with zero-length vectors, but they may return results that are surprising at first glance. Here we see mean(age) returning NaN because mean(age) = sum(age)/length(age) which here is 0/0. max() and min() return -Inf and Inf for empty vectors so if you combine the results with a non-empty vector of new data and recompute you’ll get the minimum or maximum of the new data1.\nSometimes a simpler approach is to perform the summary and then make the implicit missings explicit with complete().\n\nhealth |> \n  group_by(smoker) |> \n  summarise(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  ) |> \n  complete(smoker)\n#> # A tibble: 2 × 6\n#>   smoker     n mean_age min_age max_age sd_age\n#>   <fct>  <int>    <dbl>   <int>   <int>  <dbl>\n#> 1 yes       NA       NA      NA      NA   NA  \n#> 2 no         5       60      34      88   21.6\n\nThe main drawback of this approach is that you get an NA for the count, even though you know that it should be zero."
  },
  {
    "objectID": "column-wise.html",
    "href": "column-wise.html",
    "title": "21  Column-wise operations",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "column-wise.html#introduction",
    "href": "column-wise.html#introduction",
    "title": "21  Column-wise operations",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction\n\n\n21.1.1 Prerequisites\nIn this chapter we’ll continue using dplyr. dplyr is a member of the core tidyverse.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "wrangle.html",
    "href": "wrangle.html",
    "title": "Wrangle",
    "section": "",
    "text": "This part of the book proceeds as follows:\n\nIn Chapter 25, you’ll learn how to get plain-text data in rectangular formats from disk and into R.\nIn Chapter 23, you’ll learn how to get data from Excel spreadsheets and Google Sheets into R.\nIn Chapter 24, you’ll learn about getting data into R from databases.\nIn Chapter 25, you’ll learn how to work with hierarchical data that includes deeply nested lists, as is often created we your raw data is in JSON.\nIn Chapter 26, you’ll learn about harvesting data off the web and getting it into R.\n\nSome other types of data are not covered in this book:\n\nhaven reads SPSS, Stata, and SAS files.\nxml2 for xml2 for XML\n\nFor other file types, try the R data import/export manual and the rio package."
  },
  {
    "objectID": "parsing.html",
    "href": "parsing.html",
    "title": "22  Parsing",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it. You can find the complete first edition at https://r4ds.had.co.nz.\nThings that should be mentioned in this chapter:"
  },
  {
    "objectID": "parsing.html#parsing-a-vector",
    "href": "parsing.html#parsing-a-vector",
    "title": "22  Parsing",
    "section": "\n22.1 Parsing a vector",
    "text": "22.1 Parsing a vector\nBefore we get into the details of how readr reads files from disk, we need to take a little detour to talk about the parse_*() functions. These functions take a character vector and return a more specialised vector like a logical, integer, or date:\n\nstr(parse_logical(c(\"TRUE\", \"FALSE\", \"NA\")))\n#>  logi [1:3] TRUE FALSE NA\nstr(parse_integer(c(\"1\", \"2\", \"3\")))\n#>  int [1:3] 1 2 3\nstr(parse_date(c(\"2010-01-01\", \"1979-10-14\")))\n#>  Date[1:2], format: \"2010-01-01\" \"1979-10-14\"\n\nThese functions are useful in their own right, but are also an important building block for readr. Once you’ve learned how the individual parsers work in this section, we’ll circle back and see how they fit together to parse a complete file in the next section.\nLike all functions in the tidyverse, the parse_*() functions are uniform: the first argument is a character vector to parse, and the na argument specifies which strings should be treated as missing:\n\nparse_integer(c(\"1\", \"231\", \".\", \"456\"), na = \".\")\n#> [1]   1 231  NA 456\n\nIf parsing fails, you’ll get a warning:\n\nx <- parse_integer(c(\"123\", \"345\", \"abc\", \"123.45\"))\n#> Warning: 2 parsing failures.\n#> row col               expected actual\n#>   3  -- no trailing characters abc   \n#>   4  -- no trailing characters 123.45\n\nAnd the failures will be missing in the output:\n\nx\n#> [1] 123 345  NA  NA\n#> attr(,\"problems\")\n#> # A tibble: 2 × 4\n#>     row   col expected               actual\n#>   <int> <int> <chr>                  <chr> \n#> 1     3    NA no trailing characters abc   \n#> 2     4    NA no trailing characters 123.45\n\nIf there are many parsing failures, you’ll need to use problems() to get the complete set. This returns a tibble, which you can then manipulate with dplyr.\n\nproblems(x)\n#> # A tibble: 2 × 4\n#>     row   col expected               actual\n#>   <int> <int> <chr>                  <chr> \n#> 1     3    NA no trailing characters abc   \n#> 2     4    NA no trailing characters 123.45\n\nUsing parsers is mostly a matter of understanding what’s available and how they deal with different types of input. There are eight particularly important parsers:\n\nparse_logical() and parse_integer() parse logicals and integers respectively. There’s basically nothing that can go wrong with these parsers so we won’t describe them here further.\nparse_double() is a strict numeric parser, and parse_number() is a flexible numeric parser. These are more complicated than you might expect because different parts of the world write numbers in different ways.\nparse_character() seems so simple that it shouldn’t be necessary. But one complication makes it quite important: character encodings.\nparse_factor() create factors, the data structure that R uses to represent categorical variables with fixed and known values.\nparse_datetime(), parse_date(), and parse_time() allow you to parse various date & time specifications. These are the most complicated because there are so many different ways of writing dates.\n\nThe following sections describe these parsers in more detail.\n\n22.1.1 Numbers\nIt seems like it should be straightforward to parse a number, but three problems make it tricky:\n\nPeople write numbers differently in different parts of the world. For example, some countries use . in between the integer and fractional parts of a real number, while others use ,.\nNumbers are often surrounded by other characters that provide some context, like “$1000” or “10%”.\nNumbers often contain “grouping” characters to make them easier to read, like “1,000,000”, and these grouping characters vary around the world.\n\nTo address the first problem, readr has the notion of a “locale”, an object that specifies parsing options that differ from place to place. When parsing numbers, the most important option is the character you use for the decimal mark. You can override the default value of . by creating a new locale and setting the decimal_mark argument:\n\nparse_double(\"1.23\")\n#> [1] 1.23\nparse_double(\"1,23\", locale = locale(decimal_mark = \",\"))\n#> [1] 1.23\n\nreadr’s default locale is US-centric, because generally R is US-centric (i.e. the documentation of base R is written in American English). An alternative approach would be to try and guess the defaults from your operating system. This is hard to do well, and, more importantly, makes your code fragile: even if it works on your computer, it might fail when you email it to a colleague in another country.\nparse_number() addresses the second problem: it ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text.\n\nparse_number(\"$100\")\n#> [1] 100\nparse_number(\"20%\")\n#> [1] 20\nparse_number(\"It cost $123.45\")\n#> [1] 123.45\n\nThe final problem is addressed by the combination of parse_number() and the locale as parse_number() will ignore the “grouping mark”:\n\n# Used in America\nparse_number(\"$123,456,789\")\n#> [1] 123456789\n\n# Used in many parts of Europe\nparse_number(\"123.456.789\", locale = locale(grouping_mark = \".\"))\n#> [1] 123456789\n\n# Used in Switzerland\nparse_number(\"123'456'789\", locale = locale(grouping_mark = \"'\"))\n#> [1] 123456789\n\n\n22.1.2 Strings\nIt seems like parse_character() should be really simple — it could just return its input. Unfortunately life isn’t so simple, as there are multiple ways to represent the same string. To understand what’s going on, we need to dive into the details of how computers represent strings. In R, we can get at the underlying representation of a string using charToRaw():\n\ncharToRaw(\"Hadley\")\n#> [1] 48 61 64 6c 65 79\n\nEach hexadecimal number represents a byte of information: 48 is H, 61 is a, and so on. The mapping from hexadecimal number to character is called the encoding, and in this case the encoding is called ASCII. ASCII does a great job of representing English characters, because it’s the American Standard Code for Information Interchange.\nThings get more complicated for languages other than English. In the early days of computing there were many competing standards for encoding non-English characters, and to correctly interpret a string you needed to know both the values and the encoding. For example, two common encodings are Latin1 (aka ISO-8859-1, used for Western European languages) and Latin2 (aka ISO-8859-2, used for Eastern European languages). In Latin1, the byte b1 is “±”, but in Latin2, it’s “ą”! Fortunately, today there is one standard that is supported almost everywhere: UTF-8. UTF-8 can encode just about every character used by humans today, as well as many extra symbols (like emoji!).\nreadr uses UTF-8 everywhere: it assumes your data is UTF-8 encoded when you read it, and always uses it when writing. This is a good default, but will fail for data produced by older systems that don’t understand UTF-8. If this happens to you, your strings will look weird when you print them. Sometimes just one or two characters might be messed up; other times you’ll get complete gibberish. For example:\n\nx1 <- \"El Ni\\xf1o was particularly bad this year\"\nx2 <- \"\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd\"\n\nx1\n#> [1] \"El Ni\\xf1o was particularly bad this year\"\nx2\n#> [1] \"\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd\"\n\nTo fix the problem you need to specify the encoding in parse_character():\n\nparse_character(x1, locale = locale(encoding = \"Latin1\"))\n#> [1] \"El Niño was particularly bad this year\"\nparse_character(x2, locale = locale(encoding = \"Shift-JIS\"))\n#> [1] \"こんにちは\"\n\nHow do you find the correct encoding? If you’re lucky, it’ll be included somewhere in the data documentation. Unfortunately, that’s rarely the case, so readr provides guess_encoding() to help you figure it out. It’s not foolproof, and it works better when you have lots of text (unlike here), but it’s a reasonable place to start. Expect to try a few different encodings before you find the right one.\n\nguess_encoding(charToRaw(x1))\n#> # A tibble: 2 × 2\n#>   encoding   confidence\n#>   <chr>           <dbl>\n#> 1 ISO-8859-1       0.46\n#> 2 ISO-8859-9       0.23\nguess_encoding(charToRaw(x2))\n#> # A tibble: 1 × 2\n#>   encoding confidence\n#>   <chr>         <dbl>\n#> 1 KOI8-R         0.42\n\nThe first argument to guess_encoding() can either be a path to a file, or, as in this case, a raw vector (useful if the strings are already in R).\nEncodings are a rich and complex topic, and we’ve only scratched the surface here. If you’d like to learn more we recommend reading the detailed explanation at http://kunststube.net/encoding/.\n\n22.1.3 Factors\nR uses factors to represent categorical variables that have a known set of possible values. Give parse_factor() a vector of known levels to generate a warning whenever an unexpected value is present:\n\nfruit <- c(\"apple\", \"banana\")\nparse_factor(c(\"apple\", \"banana\", \"bananana\"), levels = fruit)\n#> Warning: 1 parsing failure.\n#> row col           expected   actual\n#>   3  -- value in level set bananana\n#> [1] apple  banana <NA>  \n#> attr(,\"problems\")\n#> # A tibble: 1 × 4\n#>     row   col expected           actual  \n#>   <int> <int> <chr>              <chr>   \n#> 1     3    NA value in level set bananana\n#> Levels: apple banana\n\nBut if you have many problematic entries, it’s often easier to leave them as character vectors and then use the tools you’ll learn about in strings and factors to clean them up.\n\n22.1.4 Dates, date-times, and times\nYou pick between three parsers depending on whether you want a date (the number of days since 1970-01-01), a date-time (the number of seconds since midnight 1970-01-01), or a time (the number of seconds since midnight). When called without any additional arguments:\n\n\nparse_datetime() expects an ISO8601 date-time. ISO8601 is an international standard in which the components of a date are organised from biggest to smallest: year, month, day, hour, minute, second.\n\nparse_datetime(\"2010-10-01T2010\")\n#> [1] \"2010-10-01 20:10:00 UTC\"\n# If time is omitted, it will be set to midnight\nparse_datetime(\"20101010\")\n#> [1] \"2010-10-10 UTC\"\n\nThis is the most important date/time standard, and if you work with dates and times frequently, we recommend reading https://en.wikipedia.org/wiki/ISO_8601\n\n\nparse_date() expects a four digit year, a - or /, the month, a - or /, then the day:\n\nparse_date(\"2010-10-01\")\n#> [1] \"2010-10-01\"\n\n\n\nparse_time() expects the hour, :, minutes, optionally : and seconds, and an optional am/pm specifier:\n\nlibrary(hms)\nparse_time(\"01:10 am\")\n#> 01:10:00\nparse_time(\"20:10:01\")\n#> 20:10:01\n\nBase R doesn’t have a great built in class for time data, so we use the one provided in the hms package.\n\n\nIf these defaults don’t work for your data you can supply your own date-time format, built up of the following pieces:\n\nYear\n\n%Y (4 digits).\n\n\n%y (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999.\n\nMonth\n\n%m (2 digits).\n\n\n%b (abbreviated name, like “Jan”).\n\n\n%B (full name, “January”).\n\nDay\n\n%d (2 digits).\n\n\n%e (optional leading space).\n\nTime\n\n%H 0-23 hour.\n\n\n%I 0-12, must be used with %p.\n\n\n%p AM/PM indicator.\n\n\n%M minutes.\n\n\n%S integer seconds.\n\n\n%OS real seconds.\n\n\n%Z Time zone (as name, e.g. America/Chicago). Beware of abbreviations: if you’re American, note that “EST” is a Canadian time zone that does not have daylight savings time. It is not Eastern Standard Time! We’ll come back to this [time zones].\n\n\n%z (as offset from UTC, e.g. +0800).\n\nNon-digits\n\n%. skips one non-digit character.\n\n\n%* skips any number of non-digits.\n\n\nThe best way to figure out the correct format is to create a few examples in a character vector, and test with one of the parsing functions. For example:\n\nparse_date(\"01/02/15\", \"%m/%d/%y\")\n#> [1] \"2015-01-02\"\nparse_date(\"01/02/15\", \"%d/%m/%y\")\n#> [1] \"2015-02-01\"\nparse_date(\"01/02/15\", \"%y/%m/%d\")\n#> [1] \"2001-02-15\"\n\nIf you’re using %b or %B with non-English month names, you’ll need to set the lang argument to locale(). See the list of built-in languages in date_names_langs(), or if your language is not already included, create your own with date_names().\n\nparse_date(\"1 janvier 2015\", \"%d %B %Y\", locale = locale(\"fr\"))\n#> [1] \"2015-01-01\"\n\n\n22.1.5 Exercises\n\nWhat are the most important arguments to locale()?\nWhat happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”?\nWe didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful.\nIf you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly.\nWhat’s the difference between read_csv() and read_csv2()?\nWhat are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out.\n\nGenerate the correct format string to parse each of the following dates and times:\n\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 2014\nt1 <- \"1705\"\nt2 <- \"11:15:10.12 PM\""
  },
  {
    "objectID": "parsing.html#sec-parsing-a-file",
    "href": "parsing.html#sec-parsing-a-file",
    "title": "22  Parsing",
    "section": "\n22.2 Parsing a file",
    "text": "22.2 Parsing a file\nNow that you’ve learned how to parse an individual vector, it’s time to return to the beginning and explore how readr parses a file. There are two new things that you’ll learn about in this section:\n\nHow readr automatically guesses the type of each column.\nHow to override the default specification.\n\n\n22.2.1 Strategy\nreadr uses a heuristic to figure out the type of each column: it reads the first 1000 rows and uses some (moderately conservative) heuristics to figure out the type of each column. You can emulate this process with a character vector using guess_parser(), which returns readr’s best guess, and parse_guess() which uses that guess to parse the column:\n\nguess_parser(\"2010-10-01\")\n#> [1] \"date\"\nguess_parser(\"15:01\")\n#> [1] \"time\"\nguess_parser(c(\"TRUE\", \"FALSE\"))\n#> [1] \"logical\"\nguess_parser(c(\"1\", \"5\", \"9\"))\n#> [1] \"double\"\nguess_parser(c(\"12,352,561\"))\n#> [1] \"number\"\n\nstr(parse_guess(\"2010-10-10\"))\n#>  Date[1:1], format: \"2010-10-10\"\n\nThe heuristic tries each of the following types, stopping when it finds a match:\n\nlogical: contains only “F”, “T”, “FALSE”, or “TRUE”.\ninteger: contains only numeric characters (and -).\ndouble: contains only valid doubles (including numbers like 4.5e-5).\nnumber: contains valid doubles with the grouping mark inside.\ntime: matches the default time_format.\ndate: matches the default date_format.\ndate-time: any ISO8601 date.\n\nIf none of these rules apply, then the column will stay as a vector of strings.\n\n22.2.2 Problems\nThese defaults don’t always work for larger files. There are two basic problems:\n\nThe first thousand rows might be a special case, and readr guesses a type that is not sufficiently general. For example, you might have a column of doubles that only contains integers in the first 1000 rows.\nThe column might contain a lot of missing values. If the first 1000 rows contain only NAs, readr will guess that it’s a logical vector, whereas you probably want to parse it as something more specific.\n\nreadr contains a challenging CSV that illustrates both of these problems:\n\nchallenge <- read_csv(readr_example(\"challenge.csv\"))\n#> Rows: 2000 Columns: 2\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl  (1): x\n#> date (1): y\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n(Note the use of readr_example() which finds the path to one of the files included with the package)\nThere are two printed outputs: the column specification generated by looking at the first 1000 rows, and the first five parsing failures. It’s always a good idea to explicitly pull out the problems(), so you can explore them in more depth:\n\nproblems(challenge)\n#> # A tibble: 0 × 5\n#> # … with 5 variables: row <int>, col <int>, expected <chr>, actual <chr>,\n#> #   file <chr>\n#> # ℹ Use `colnames()` to see all variable names\n\nA good strategy is to work column by column until there are no problems remaining. Here we can see that there are a lot of parsing problems with the y column. If we look at the last few rows, you’ll see that they’re dates stored in a character vector:\n\ntail(challenge)\n#> # A tibble: 6 × 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1 0.805 2019-11-21\n#> 2 0.164 2018-03-29\n#> 3 0.472 2014-08-04\n#> 4 0.718 2015-08-16\n#> 5 0.270 2020-02-04\n#> 6 0.608 2019-01-06\n\nThat suggests we need to use a date parser instead. To fix the call, start by copying and pasting the column specification into your original call:\n\nchallenge <- read_csv(\n  readr_example(\"challenge.csv\"), \n  col_types = cols(\n    x = col_double(),\n    y = col_logical()\n  )\n)\n\nThen you can fix the type of the y column by specifying that y is a date column:\n\nchallenge <- read_csv(\n  readr_example(\"challenge.csv\"), \n  col_types = cols(\n    x = col_double(),\n    y = col_date()\n  )\n)\ntail(challenge)\n#> # A tibble: 6 × 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1 0.805 2019-11-21\n#> 2 0.164 2018-03-29\n#> 3 0.472 2014-08-04\n#> 4 0.718 2015-08-16\n#> 5 0.270 2020-02-04\n#> 6 0.608 2019-01-06\n\nEvery parse_xyz() function has a corresponding col_xyz() function. You use parse_xyz() when the data is in a character vector in R already; you use col_xyz() when you want to tell readr how to load the data.\nWe highly recommend always supplying col_types, building up from the print-out provided by readr. This ensures that you have a consistent and reproducible data import script. If you rely on the default guesses and your data changes, readr will continue to read it in. If you want to be really strict, use stop_for_problems(): that will throw an error and stop your script if there are any parsing problems.\n\n22.2.3 Other strategies\nThere are a few other general strategies to help you parse files:\n\n\nIn the previous example, we just got unlucky: if we look at just one more row than the default, we can correctly parse in one shot:\n\nchallenge2 <- read_csv(readr_example(\"challenge.csv\"), guess_max = 1001)\n#> Rows: 2000 Columns: 2\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl  (1): x\n#> date (1): y\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nchallenge2\n#> # A tibble: 2,000 × 2\n#>       x y     \n#>   <dbl> <date>\n#> 1   404 NA    \n#> 2  4172 NA    \n#> 3  3004 NA    \n#> 4   787 NA    \n#> 5    37 NA    \n#> 6  2332 NA    \n#> # … with 1,994 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\nSometimes it’s easier to diagnose problems if you just read in all the columns as character vectors:\n\nchallenge2 <- read_csv(readr_example(\"challenge.csv\"), \n  col_types = cols(.default = col_character())\n)\n\nThis is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.\n\ndf <- tribble(\n  ~x,  ~y,\n  \"1\", \"1.21\",\n  \"2\", \"2.32\",\n  \"3\", \"4.56\"\n)\ndf\n#> # A tibble: 3 × 2\n#>   x     y    \n#>   <chr> <chr>\n#> 1 1     1.21 \n#> 2 2     2.32 \n#> 3 3     4.56\n\n# Note the column types\ntype_convert(df)\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_double()\n#> )\n#> # A tibble: 3 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1  1.21\n#> 2     2  2.32\n#> 3     3  4.56\n\n\nIf you’re reading a very large file, you might want to set n_max to a smallish number like 10,000 or 100,000. That will accelerate your iterations while you eliminate common problems.\nIf you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats."
  },
  {
    "objectID": "spreadsheets.html",
    "href": "spreadsheets.html",
    "title": "23  Spreadsheets",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "spreadsheets.html#introduction",
    "href": "spreadsheets.html#introduction",
    "title": "23  Spreadsheets",
    "section": "\n23.1 Introduction",
    "text": "23.1 Introduction\nSo far you have learned about importing data from plain text files, e.g. .csv and .tsv files. Sometimes you need to analyze data that lives in a spreadsheet. In this chapter we will introduce you to tools for working with data in Excel spreadsheets and Google Sheets. This will build on much of what you’ve learned in Chapter 8 and Chapter 22, but we will also discuss additional considerations and complexities when working with data from spreadsheets.\nIf you or your collaborators are using spreadsheets for organizing data, we strongly recommend reading the paper “Data Organization in Spreadsheets” by Karl Broman and Kara Woo: https://doi.org/10.1080/00031305.2017.1375989. The best practices presented in this paper will save you much headache down the line when you import the data from a spreadsheet into R to analyse and visualise."
  },
  {
    "objectID": "spreadsheets.html#excel",
    "href": "spreadsheets.html#excel",
    "title": "23  Spreadsheets",
    "section": "\n23.2 Excel",
    "text": "23.2 Excel\n\n23.2.1 Prerequisites\nIn this chapter, you’ll learn how to load data from Excel spreadsheets in R with the readxl package. This package is non-core tidyverse, so you need to load it explicitly but it is installed automatically when you install the tidyverse package.\n\nlibrary(readxl)\nlibrary(tidyverse)\n\nxlsx and XLConnect can be used for reading data from and writing data to Excel spreadsheets. However, these two packages require Java installed on your machine and the rJava package. Due to potential challenges with installation, we recommend using alternative packages we’ve introduced in this chapter.\n\n23.2.2 Getting started\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\n\nread_xls() reads Excel files with xls format.\n\nread_xlsx() read Excel files with xlsx format.\n\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nThese functions all have similar syntax just like other functions we have previously introduced for reading other types of files, e.g. read_csv(), read_table(), etc. For the rest of the chapter we will focus on using read_excel().\n\n23.2.3 Reading spreadsheets\nFigure 23.1 shows what the spreadsheet we’re going to read into R looks like in Excel.\n\n\n\n\nFigure 23.1: Spreadsheet called students.xlsx in Excel.\n\n\n\n\nThe first argument to read_excel() is the path to the file to read.\n\nstudents <- read_excel(\"data/students.xlsx\")\n\nread_excel() will read the file in as a tibble.\n\nstudents\n#> # A tibble: 6 × 5\n#>   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#>          <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2            2 Barclay Lynn     French fries       Lunch only          5    \n#> 3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 4            4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\nWe have six students in the data and five variables on each student. However there are a few things we might want to address in this dataset:\n\n\nThe column names are all over the place. You can provide column names that follow a consistent format; we recommend snake_case using the col_names argument.\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\")\n)\n#> # A tibble: 7 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>   <chr>      <chr>            <chr>              <chr>               <chr>\n#> 1 Student ID Full Name        favourite.food     mealPlan            AGE  \n#> 2 1          Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 3 2          Barclay Lynn     French fries       Lunch only          5    \n#> 4 3          Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 5 4          Leon Rossini     Anchovies          Lunch only          <NA> \n#> 6 5          Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> # … with 1 more row\n#> # ℹ Use `print(n = ...)` to see more rows\n\nUnfortunately, this didn’t quite do the trick. You now have the variable names we want, but what was previously the header row now shows up as the first observation in the data. You can explicitly skip that row using the skip argument.\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1\n)\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nIn the favourite_food column, one of the observations is N/A, which stands for “not available” but it’s currently not recognized as an NA (note the contrast between this N/A and the age of the fourth student in the list). You can specify which character strings should be recognized as NAs with the na argument. By default, only \"\" (empty string, or, in the case of reading from a spreadsheet, an empty cell) is recognized as an NA.\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\")\n)\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nOne other remaining issue is that age is read in as a character variable, but it really should be numeric. Just like with read_csv() and friends for reading data from flat files, you can supply a col_types argument to read_excel() and specify the column types for the variables you read in. The syntax is a bit different, though. Your options are \"skip\", \"guess\", \"logical\", \"numeric\", \"date\", \"text\" or \"list\".\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = c(\"numeric\", \"text\", \"text\", \"text\", \"numeric\")\n)\n#> Warning: Expecting numeric in E6 / R6C5: got 'five'\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <chr>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch    NA\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\nHowever, this didn’t quite produce the desired result either. By specifying that age should be numeric, we have turned the one cell with the non-numeric entry (which had the value five) into an NA. In this case, we should read age in as \"text\" and then make the change once the data is loaded in R.\n\nstudents <- read_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = c(\"numeric\", \"text\", \"text\", \"text\", \"text\")\n)\n\nstudents <- students |>\n  mutate(\n    age = if_else(age == \"five\", \"5\", age),\n    age = parse_number(age)\n  )\n\nstudents\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <chr>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nIt took us multiple steps and trial-and-error to load the data in exactly the format we want, and this is not unexpected. Data science is an iterative process. There is no way to know exactly what the data will look like until you load it and take a look at it. Well, there is one way, actually. You can open the file in Excel and take a peek. That might be tempting, but it’s strongly not recommended.  Instead, you should not be afraid of doing what we did here: load the data, take a peek, make adjustments to your code, load it again, and repeat until you’re happy with the result.\n\n23.2.4 Reading individual sheets\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets. Figure 23.2 shows an Excel spreadsheet with multiple sheets. The data come from the palmerpenguins package. Each sheet contains information on penguins from a different island where data were collected.\n\n\n\n\nFigure 23.2: Spreadsheet called penguins.xlsx in Excel.\n\n\n\n\nYou can read a single sheet from a spreadsheet with the sheet argument in read_excel().\n\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\n#> # A tibble: 52 × 8\n#>   species island    bill_length_mm     bill_depth_mm flipp…¹ body_…² sex    year\n#>   <chr>   <chr>     <chr>              <chr>         <chr>   <chr>   <chr> <dbl>\n#> 1 Adelie  Torgersen 39.1               18.7          181     3750    male   2007\n#> 2 Adelie  Torgersen 39.5               17.399999999… 186     3800    fema…  2007\n#> 3 Adelie  Torgersen 40.299999999999997 18            195     3250    fema…  2007\n#> 4 Adelie  Torgersen NA                 NA            NA      NA      NA     2007\n#> 5 Adelie  Torgersen 36.700000000000003 19.3          193     3450    fema…  2007\n#> 6 Adelie  Torgersen 39.299999999999997 20.6          190     3650    male   2007\n#> # … with 46 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#> #   ²​body_mass_g\n#> # ℹ Use `print(n = ...)` to see more rows\n\nSome variables that appear to contain numerical data are read in as characters due to the character string \"NA\" not being recognized as a true NA.\n\npenguins_torgersen <- read_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\", na = \"NA\")\n\npenguins_torgersen\n#> # A tibble: 52 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n#>   <chr>   <chr>              <dbl>         <dbl>       <dbl>   <dbl> <chr> <dbl>\n#> 1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n#> 2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n#> 3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n#> 4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n#> 5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n#> 6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n#> # … with 46 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#> #   ²​body_mass_g\n#> # ℹ Use `print(n = ...)` to see more rows\n\nHowever, we cheated here a bit. We looked inside the Excel spreadsheet, which is not a recommended workflow. Instead, you can use excel_sheets() to get information on all sheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\n\nexcel_sheets(\"data/penguins.xlsx\")\n#> [1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"\n\nOnce you know the names of the sheets, you can read them in individually with read_excel().\n\npenguins_biscoe <- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\")\npenguins_dream  <- read_excel(\"data/penguins.xlsx\", sheet = \"Dream Island\", na = \"NA\")\n\nIn this case the full penguins dataset is spread across three sheets in the spreadsheet. Each sheet has the same number of columns but different numbers of rows.\n\ndim(penguins_torgersen)\n#> [1] 52  8\ndim(penguins_biscoe)\n#> [1] 168   8\ndim(penguins_dream)\n#> [1] 124   8\n\nWe can put them together with bind_rows().\n\npenguins <- bind_rows(penguins_torgersen, penguins_biscoe, penguins_dream)\npenguins\n#> # A tibble: 344 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n#>   <chr>   <chr>              <dbl>         <dbl>       <dbl>   <dbl> <chr> <dbl>\n#> 1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n#> 2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n#> 3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n#> 4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n#> 5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n#> 6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n#> # … with 338 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#> #   ²​body_mass_g\n#> # ℹ Use `print(n = ...)` to see more rows\n\nIn Chapter 29 we’ll talk about ways of doing this sort of task without repetitive code .\n\n23.2.5 Reading part of a sheet\nSince many use Excel spreadsheets for presentation as well as for data storage, it’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. Figure 23.3 shows such a spreadsheet: in the middle of the sheet is what looks like a data frame but there is extraneous text in cells above and below the data.\n\n\n\n\nFigure 23.3: Spreadsheet called deaths.xlsx in Excel.\n\n\n\n\nThis spreadsheet is one of the example spreadsheets provided in the readxl package. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel() as usual.\n\ndeaths_path <- readxl_example(\"deaths.xlsx\")\ndeaths <- read_excel(deaths_path)\n#> New names:\n#> • `` -> `...2`\n#> • `` -> `...3`\n#> • `` -> `...4`\n#> • `` -> `...5`\n#> • `` -> `...6`\ndeaths\n#> # A tibble: 18 × 6\n#>   `Lots of people`             ...2       ...3  ...4     ...5          ...6     \n#>   <chr>                        <chr>      <chr> <chr>    <chr>         <chr>    \n#> 1 simply cannot resist writing <NA>       <NA>  <NA>     <NA>          some not…\n#> 2 at                           the        top   <NA>     of            their sp…\n#> 3 or                           merging    <NA>  <NA>     <NA>          cells    \n#> 4 Name                         Profession Age   Has kids Date of birth Date of …\n#> 5 David Bowie                  musician   69    TRUE     17175         42379    \n#> 6 Carrie Fisher                actor      60    TRUE     20749         42731    \n#> # … with 12 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThe top three rows and the bottom four rows are not part of the data frame.\nWe could skip the top three rows with skip. Note that we set skip = 4 since the fourth row contains column names, not the data.\n\nread_excel(deaths_path, skip = 4)\n#> # A tibble: 14 × 6\n#>   Name          Profession Age   `Has kids` `Date of birth`     `Date of death`\n#>   <chr>         <chr>      <chr> <chr>      <dttm>              <chr>          \n#> 1 David Bowie   musician   69    TRUE       1947-01-08 00:00:00 42379          \n#> 2 Carrie Fisher actor      60    TRUE       1956-10-21 00:00:00 42731          \n#> 3 Chuck Berry   musician   90    TRUE       1926-10-18 00:00:00 42812          \n#> 4 Bill Paxton   actor      61    TRUE       1955-05-17 00:00:00 42791          \n#> 5 Prince        musician   57    TRUE       1958-06-07 00:00:00 42481          \n#> 6 Alan Rickman  actor      69    FALSE      1946-02-21 00:00:00 42383          \n#> # … with 8 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nWe could also set n_max to omit the extraneous rows at the bottom.\n\nread_excel(deaths_path, skip = 4, n_max = 10)\n#> # A tibble: 10 × 6\n#>   Name          Profession   Age Has k…¹ `Date of birth`     `Date of death`    \n#>   <chr>         <chr>      <dbl> <lgl>   <dttm>              <dttm>             \n#> 1 David Bowie   musician      69 TRUE    1947-01-08 00:00:00 2016-01-10 00:00:00\n#> 2 Carrie Fisher actor         60 TRUE    1956-10-21 00:00:00 2016-12-27 00:00:00\n#> 3 Chuck Berry   musician      90 TRUE    1926-10-18 00:00:00 2017-03-18 00:00:00\n#> 4 Bill Paxton   actor         61 TRUE    1955-05-17 00:00:00 2017-02-25 00:00:00\n#> 5 Prince        musician      57 TRUE    1958-06-07 00:00:00 2016-04-21 00:00:00\n#> 6 Alan Rickman  actor         69 FALSE   1946-02-21 00:00:00 2016-01-14 00:00:00\n#> # … with 4 more rows, and abbreviated variable name ¹​`Has kids`\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAnother approach is using cell ranges. In Excel, the top left cell is A1. As you move across columns to the right, the cell label moves down the alphabet, i.e. B1, C1, etc. And as you move down a column, the number in the cell label increases, i.e. A2, A3, etc.\nThe data we want to read in starts in cell A5 and ends in cell F15. In spreadsheet notation, this is A5:F15.\n\n\nSupply this information to the range argument:\n\nread_excel(deaths_path, range = \"A5:F15\")\n\n\n\nSpecify rows:\n\nread_excel(deaths_path, range = cell_rows(c(5, 15)))\n\n\n\nSpecify cells that mark the top-left and bottom-right corners of the data – the top-left corner, A5, translates to c(5, 1) (5th row down, 1st column) and the bottom-right corner, F15, translates to c(15, 6):\n\nread_excel(deaths_path, range = cell_limits(c(5, 1), c(15, 6)))\n\n\n\nIf you have control over the sheet, an even better way is to create a “named range”. This is useful within Excel because named ranges help repeat formulas easier to create and they have some useful properties for creating dynamic charts and graphs as well. Even if you’re not working in Excel, named ranges can be useful for identifying which cells to read into R. In the example above, the table we’re reading in is named Table1, so we can read it in with the following.\nTO DO: Add this once reading in named ranges are implemented in readxl.\n\n23.2.6 Data types\nIn CSV files, all values are strings. This is not particularly true to the data, but it is simple: everything is a string.\nThe underlying data in Excel spreadsheets is more complex. A cell can be one of five things:\n\nA logical, like TRUE / FALSE\nA number, like “10” or “10.5”\nA date, which can also include time like “11/1/21” or “11/1/21 3:00 PM”\nA string, like “ten”\nA currency, which allows numeric values in a limited range and four decimal digits of fixed precision\n\nWhen working with spreadsheet data, it’s important to keep in mind that how the underlying data is stored can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g. type '10 into a cell in Excel).\nThese differences between how the underlying data are stored vs. how they’re displayed can cause surprises when the data are loaded into R. By default readxl will guess the data type in a given column. A recommended workflow is to let readxl guess the column types, confirm that you’re happy with the guessed column types, and if not, go back and re-import specifying col_types as shown in Section 23.2.3.\nAnother challenge is when you have a column in your Excel spreadsheet that has a mix of these types, e.g. some cells are numeric, others text, others dates. When importing the data into R readxl has to make some decisions. In these cases you can set the type for this column to \"list\", which will load the column as a list of length 1 vectors, where the type of each element of the vector is guessed.\n\n23.2.7 Data not in cell values\ntidyxl is useful for importing non-tabular data from Excel files into R. For example, tidyxl doesn’t coerce a pivot table into a data frame. See https://nacnudus.github.io/spreadsheet-munging-strategies/ for more on strategies for working with non-tabular data from Excel.\n\n23.2.8 Writing to Excel\nLet’s create a small data frame that we can then write out. Note that item is a factor and quantity is an integer.\n\nbake_sale <- tibble(\n  item     = factor(c(\"brownie\", \"cupcake\", \"cookie\")),\n  quantity = c(10, 5, 8)\n)\n\nbake_sale\n#> # A tibble: 3 × 2\n#>   item    quantity\n#>   <fct>      <dbl>\n#> 1 brownie       10\n#> 2 cupcake        5\n#> 3 cookie         8\n\nYou can write data back to disk as an Excel file using the write_xlsx() from the writexl package.\n\nlibrary(writexl)\nwrite_xlsx(bake_sale, path = \"data/bake-sale.xlsx\")\n\nFigure 23.4 shows what the data looks like in Excel. Note that column names are included and bolded. These can be turned off by setting col_names and format_headers arguments to FALSE.\n\n\n\n\nFigure 23.4: Spreadsheet called bake_sale.xlsx in Excel.\n\n\n\n\nJust like reading from a CSV, information on data type is lost when we read the data back in. This makes Excel files unreliable for caching interim results as well. For alternatives, see Section 8.5.\n\nread_excel(\"data/bake-sale.xlsx\")\n#> # A tibble: 3 × 2\n#>   item    quantity\n#>   <chr>      <dbl>\n#> 1 brownie       10\n#> 2 cupcake        5\n#> 3 cookie         8\n\n\n23.2.9 Formatted output\nThe readxl package is a light-weight solution for writing a simple Excel spreadsheet, but if you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package. Note that this package is not part of the tidyverse so the functions and workflows may feel unfamiliar. For example, function names are camelCase, multiple functions can’t be composed in pipelines, and arguments are in a different order than they tend to be in the tidyverse. However, this is ok. As your R learning and usage expands outside of this book you will encounter lots of different styles used in various R packages that you might need to use to accomplish specific goals in R. A good way of familiarizing yourself with the coding style used in a new package is to run the examples provided in function documentation to get a feel for the syntax and the output formats as well as reading any vignettes that might come with the package.\nBelow we show how to write a spreadsheet with three sheets, one for each species of penguins in the penguins data frame.\n\nlibrary(openxlsx)\nlibrary(palmerpenguins)\n\n# Create a workbook (spreadsheet)\npenguins_species <- createWorkbook()\n\n# Add three sheets to the spreadsheet\naddWorksheet(penguins_species, sheetName = \"Adelie\")\naddWorksheet(penguins_species, sheetName = \"Gentoo\")\naddWorksheet(penguins_species, sheetName = \"Chinstrap\")\n\n# Write data to each sheet\nwriteDataTable(\n  penguins_species, \n  sheet = \"Adelie\", \n  x = penguins |> filter(species == \"Adelie\")\n)\nwriteDataTable(\n  penguins_species, \n  sheet = \"Gentoo\", \n  x = penguins |> filter(species == \"Gentoo\")\n)\nwriteDataTable(\n  penguins_species, \n  sheet = \"Chinstrap\", \n  x = penguins |> filter(species == \"Chinstrap\")\n)\n\nThis creates a workbook object:\n\npenguins_species\n#> A Workbook object.\n#>  \n#> Worksheets:\n#>  Sheet 1: \"Adelie\"\n#>  \n#> \n#>  Sheet 2: \"Gentoo\"\n#>  \n#> \n#>  Sheet 3: \"Chinstrap\"\n#>  \n#> \n#>  \n#>  Worksheet write order: 1, 2, 3\n#>  Active Sheet 1: \"Adelie\" \n#>  Position: 1\n\nAnd we can write this to this with saveWorkbook().\n\nsaveWorkbook(penguins_species, \"data/penguins-species.xlsx\")\n\nThe resulting spreadsheet is shown in Figure 23.5. By default, openxlsx formats the data as an Excel table.\n\n\n\n\nFigure 23.5: Spreadsheet called penguins.xlsx in Excel.\n\n\n\n\nSee https://ycphs.github.io/openxlsx/articles/Formatting.html for an extensive discussion on further formatting functionality for data written from R to Excel with openxlsx.\n\n23.2.10 Exercises\n\nRecreate the bake_sale data frame, write it out to an Excel file using the write.xlsx() function from the openxlsx package.\nWhat happens if you try to read in a file with .xlsx extension with read_xls()?"
  },
  {
    "objectID": "spreadsheets.html#google-sheets",
    "href": "spreadsheets.html#google-sheets",
    "title": "23  Spreadsheets",
    "section": "\n23.3 Google Sheets",
    "text": "23.3 Google Sheets\n\n\n23.3.1 Prerequisites\nTO DO:\n\nuse googlesheets4\nwhy 4?\n\n23.3.2 Getting started\nTO DO:\n\nreading from public sheet with read_sheet() and read_range()\n\n\n23.3.3 Authentication\n\n23.3.4 Read sheets\n\n23.3.5 Write sheets\n\n23.3.6 Exercises"
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "24  Databases",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "databases.html#introduction",
    "href": "databases.html#introduction",
    "title": "24  Databases",
    "section": "\n24.1 Introduction",
    "text": "24.1 Introduction\nA huge amount of data lives in databases, so it’s essential that you know how to access it. Sometimes you can ask someone to download a snapshot into a .csv for you, but this gets painful quickly: every time you need to make a change you’ll have to communicate with another human. You want to be able to reach into the database directly to get the data you need, when you need it.\nIn this chapter, you’ll first learn the basics of the DBI package: how to use it to connect to a database and then retrieve data with a SQL1 query. SQL, short for structured query language, is the lingua franca of databases, and is an important language for all data scientists to learn. That said, we’re not going to start with SQL, but instead we’ll teach you dbplyr, which can translate your dplyr code to the SQL. We’ll use that as way to teach you some of the most important features of SQL. You won’t become a SQL master by the end of the chapter, but you will be able to identify the most important components and understand what they do.\n\n24.1.1 Prerequisites\nIn this chapter, we’ll introduce DBI and dbplyr. DBI is a low-level interface that connects to databases and executes SQL; dbplyr is a high-level interface that translates your dplyr code to SQL queries then executes them with DBI.\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "databases.html#database-basics",
    "href": "databases.html#database-basics",
    "title": "24  Databases",
    "section": "\n24.2 Database basics",
    "text": "24.2 Database basics\nAt the simplest level, you can think about a database as a collection of data frames, called tables in database terminology. Like a data.frame, a database table is a collection of named columns, where every value in the column is the same type. There are three high level differences between data frames and database tables:\n\nDatabase tables are stored on disk and can be arbitrarily large. Data frames are stored in memory, and are fundamentally limited (although that limit is still plenty large for many problems).\nDatabase tables almost always have indexes. Much like the index of a book, a database index makes it possible to quickly find rows of interest without having to look at every single row. Data frames and tibbles don’t have indexes, but data.tables do, which is one of the reasons that they’re so fast.\nMost classical databases are optimized for rapidly collecting data, not analyzing existing data. These databases are called row-oriented because the data is stored row-by-row, rather than column-by-column like R. More recently, there’s been much development of column-oriented databases that make analyzing the existing data much faster.\n\nDatabases are run by database management systems (DBMS’s for short), which come in three basic forms:\n\n\nClient-server DBMS’s run on a powerful central server, which you connect from your computer (the client). They are great for sharing data with multiple people in an organisation. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.\n\nCloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.\n\nIn-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user."
  },
  {
    "objectID": "databases.html#connecting-to-a-database",
    "href": "databases.html#connecting-to-a-database",
    "title": "24  Databases",
    "section": "\n24.3 Connecting to a database",
    "text": "24.3 Connecting to a database\nTo connect to the database from R, you’ll use a pair of packages:\n\nYou’ll always use DBI (database interface) because it provides a set of generic functions that connect to the database, upload data, run SQL queries, etc.\nYou’ll also use a package tailored for the DBMS you’re connecting to. This package translates the generic DBI commands into the specifics needed for a given DBMS. There’s usually one package for each DMBS, e.g. RPostgres for Postgres and RMariaDB for MySQL.\n\nIf you can’t find a specific package for your DBMS, you can usually use the odbc package instead. This uses the ODBC protocol supported by many DBMS. odbc requires a little more setup because you’ll also need to install an ODBC driver and tell the odbc package where to find it.\nConcretely, you create a database connection using DBI::dbConnect(). The first argument selects the DBMS2, then the second and subsequent arguments describe how to connect to it (i.e. where it lives and the credentials that you need to access it). The following code shows a couple of typical examples:\n\ncon <- DBI::dbConnect(\n  RMariaDB::MariaDB(), \n  username = \"foo\"\n)\ncon <- DBI::dbConnect(\n  RPostgres::Postgres(), \n  hostname = \"databases.mycompany.com\", \n  port = 1234\n)\n\nThe precise details of the connection varies a lot from DBMS to DBMS so unfortunately we can’t cover all the details here. This means you’ll need to do a little research on your own. Typically you can ask the other data scientists in your team or talk to your DBA (database administrator). The initial setup will often take a little fiddling (and maybe some googling) to get right, but you’ll generally only need to do it once.\n\n24.3.1 In this book\nSetting up a client-server or cloud DBMS would be a pain for this book, so we’ll instead use an in-process DBMS that lives entirely in an R package: duckdb. Thanks to the magic of DBI, the only difference between using duckdb and any other DBMS is how you’ll connect to the database. This makes it great to teach with because you can easily run this code as well as easily take what you learn and apply it elsewhere.\nConnecting to duckdb is particularly simple because the defaults create a temporary database that is deleted when you quit R. That’s great for learning because it guarantees that you’ll start from a clean slate every time you restart R:\n\ncon <- DBI::dbConnect(duckdb::duckdb())\n\nduckdb is a high-performance database that’s designed very much for the needs of a data scientist. We use it here because it’s very to easy to get started with, but it’s also capable of handling gigabytes of data with great speed. If you want to use duckdb for a real data analysis project, you’ll also need to supply the dbdir argument to make a persistent database and tell duckdb where to save it. Assuming you’re using a project (Chapter -Chapter 9)), it’s reasonable to store it in the duckdb directory of the current project:\n\ncon <- DBI::dbConnect(duckdb::duckdb(), dbdir = \"duckdb\")\n\n\n24.3.2 Load some data\nSince this is a new database, we need to start by adding some data. Here we’ll use add mpg and diamonds datasets from ggplot2 using DBI::dbWriteTable(). The simplest usage of dbWriteTable() needs three arguments: a database connection, the name of the table to create in the database, and a data frame of data.\n\ndbWriteTable(con, \"mpg\", ggplot2::mpg)\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\n\nIf you’re using duckdb in a real project, we highly recommend learning about duckdb_read_csv() and duckdb_register_arrow(). These give you powerful and performant ways to quickly load data directly into duckdb, without having to first load it in to R."
  },
  {
    "objectID": "databases.html#dbi-basics",
    "href": "databases.html#dbi-basics",
    "title": "24  Databases",
    "section": "\n24.4 DBI basics",
    "text": "24.4 DBI basics\nNow that we’ve connected to a database with some data in it, lets perform some basic operations with DBI.\n\n24.4.1 What’s there?\nThe most important database objects for data scientists are tables. DBI provides two useful functions to either list all the tables in the database3 or to check if a specific table already exists:\n\ndbListTables(con)\n#> [1] \"diamonds\" \"mpg\"\ndbExistsTable(con, \"foo\")\n#> [1] FALSE\n\n\n24.4.2 Extract some data\nOnce you’ve determined a table exists, you can retrieve it with dbReadTable():\n\ncon |> \n  dbReadTable(\"diamonds\") |> \n  as_tibble()\n#> # A tibble: 53,940 × 10\n#>   carat cut       color clarity depth table price     x     y     z\n#>   <dbl> <fct>     <fct> <fct>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n#> 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#> 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#> 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#> 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#> 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#> 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#> # … with 53,934 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\ndbReadTable() returns a data.frame so we use as_tibble() to convert it into a tibble so that it prints nicely.\nIn real life, it’s rare that you’ll use dbReadTable() because often database tables are too big to fit in memory, and you want bring back only a subset of the rows and columns.\n\n24.4.3 Run a query\nThe way you’ll usually retrieve data is with dbGetQuery(). It takes a database connection and some SQL code and returns a data frame:\n\nsql <- \"\n  SELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price > 15000\n\"\nas_tibble(dbGetQuery(con, sql))\n#> # A tibble: 1,655 × 5\n#>   carat cut       clarity color price\n#>   <dbl> <fct>     <fct>   <fct> <int>\n#> 1  1.54 Premium   VS2     E     15002\n#> 2  1.19 Ideal     VVS1    F     15005\n#> 3  2.1  Premium   SI1     I     15007\n#> 4  1.69 Ideal     SI1     D     15011\n#> 5  1.5  Very Good VVS2    G     15013\n#> 6  1.73 Very Good VS1     G     15014\n#> # … with 1,649 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nDon’t worry if you’ve never seen SQL before; you’ll learn more about it shortly. But if you read it carefully, you might guess that it selects five columns of the diamonds dataset and all the rows where price is greater than 15,000.\nYou’ll need to be a little careful with dbGetQuery() since it can potentially return more data than you have memory. We won’t discuss it further here, but if you’re dealing with very large datasets it’s possible to deal with a “page” of data at a time by using dbSendQuery() to get a “result set” which you can page through by calling dbFetch() until dbHasCompleted() returns TRUE.\n\n24.4.4 Other functions\nThere are lots of other functions in DBI that you might find useful if you’re managing your own data (like dbWriteTable() which we used in Section 24.3.2), but we’re going to skip past them in the interest of staying focused on working with data that already lives in a database."
  },
  {
    "objectID": "databases.html#dbplyr-basics",
    "href": "databases.html#dbplyr-basics",
    "title": "24  Databases",
    "section": "\n24.5 dbplyr basics",
    "text": "24.5 dbplyr basics\nNow that you’ve learned the low-level basics for connecting to a database and running a query, we’re going to switch it up a bit and learn a bit about dbplyr. dbplyr is a dplyr backend, which means that you keep writing dplyr code but the backend executes it differently. In this, dbplyr translates to SQL; other backends include dtplyr which translates to data.table, and multidplyr which executes your code on multiple cores.\nTo use dbplyr, you must first use tbl() to create an object that represents a database table:\n\ndiamonds_db <- tbl(con, \"diamonds\")\ndiamonds_db\n#> # Source:   table<diamonds> [?? x 10]\n#> # Database: DuckDB 0.3.5-dev1410 [user1@Windows 10 x64:R 4.2.1/:memory:]\n#>   carat cut       color clarity depth table price     x     y     z\n#>   <dbl> <fct>     <fct> <fct>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n#> 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#> 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#> 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#> 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#> 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#> 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#> # … with more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n\nThere are two other common way to a database. First, many corporate databases are very large so need some hierarchy to keep all the tables organised. In that case you might need to supply a schema, or a catalog and a schema, in order to pick the table you’re interested in:\n\ndiamonds_db <- tbl(con, in_schema(\"sales\", \"diamonds\"))\ndiamonds_db <- tbl(con, in_catalog(\"north_america\", \"sales\", \"diamonds\"))\n\nOther times you might want to use your own SQL query as a starting point:\n\ndiamonds_db <- tbl(con, sql(\"SELECT * FROM diamonds\"))\n\n\n\n\nThis object is lazy; when you use dplyr verbs on it, dplyr doesn’t do any work: it just records the sequence of operations that you want to perform and only performs them when needed. For example, take the following pipeline:\n\nbig_diamonds_db <- diamonds_db |> \n  filter(price > 15000) |> \n  select(carat:clarity, price)\n\nbig_diamonds_db\n#> # Source:   SQL [?? x 5]\n#> # Database: DuckDB 0.3.5-dev1410 [user1@Windows 10 x64:R 4.2.1/:memory:]\n#>   carat cut       color clarity price\n#>   <dbl> <fct>     <fct> <fct>   <int>\n#> 1  1.54 Premium   E     VS2     15002\n#> 2  1.19 Ideal     F     VVS1    15005\n#> 3  2.1  Premium   I     SI1     15007\n#> 4  1.69 Ideal     D     SI1     15011\n#> 5  1.5  Very Good G     VVS2    15013\n#> 6  1.73 Very Good G     VS1     15014\n#> # … with more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou can tell this object represents a database query because it prints the DBMS name at the top, and while it tells you the number of columns, it typically doesn’t know the number of rows. This is because finding the total number of rows usually requires executing the complete query, something we’re trying to avoid.\nYou can see the SQL the dbplyr generates by a dbplyr query by calling show_query():\n\nbig_diamonds_db |>\n  show_query()\n#> <SQL>\n#> SELECT \"carat\", \"cut\", \"color\", \"clarity\", \"price\"\n#> FROM \"diamonds\"\n#> WHERE (\"price\" > 15000.0)\n\nTo get all the data back into R, you call collect(). Behind the scenes, this generates the SQL, calls dbGetQuery() to get the data, then turns the result into a tibble:\n\nbig_diamonds <- big_diamonds_db |> \n  collect()\nbig_diamonds\n#> # A tibble: 1,655 × 5\n#>   carat cut       color clarity price\n#>   <dbl> <fct>     <fct> <fct>   <int>\n#> 1  1.54 Premium   E     VS2     15002\n#> 2  1.19 Ideal     F     VVS1    15005\n#> 3  2.1  Premium   I     SI1     15007\n#> 4  1.69 Ideal     D     SI1     15011\n#> 5  1.5  Very Good G     VVS2    15013\n#> 6  1.73 Very Good G     VS1     15014\n#> # … with 1,649 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nTypically, you’ll use dbplyr to select the data you want from the database, performing basic filtering and aggregation using the translations described below. Then, once you’re ready to analyse the data with functions that are unique to R, you’ll collect() the data to get an in-memory tibble, and continue your work with pure R code."
  },
  {
    "objectID": "databases.html#sql",
    "href": "databases.html#sql",
    "title": "24  Databases",
    "section": "\n24.6 SQL",
    "text": "24.6 SQL\nThe rest of the chapter will teach you a little SQL through the lens of dbplyr. It’s a rather non-traditional introduction to SQL but we hope it will get you quickly up to speed with the basics. Luckily, if you understand dplyr you’re in a great place to quickly pick up SQL because so many of the concepts are the same.\nWe’ll explore the relationship between dplyr and SQL using a couple of old friends from the nycflights13 package: flights and planes. These dataset are easy to get into our learning database because dbplyr has a function designed for this exact scenario:\n\ndbplyr::copy_nycflights13(con)\n#> Creating table: airlines\n#> Creating table: airports\n#> Creating table: flights\n#> Creating table: planes\n#> Creating table: weather\nflights <- tbl(con, \"flights\")\nplanes <- tbl(con, \"planes\")\n\n\n\n\n\n24.6.1 SQL basics\nThe top-level components of SQL are called statements. Common statements include CREATE for defining new tables, INSERT for adding data, and SELECT for retrieving data. We will on focus on SELECT statements, also called queries, because they are almost exclusively what you’ll use as a data scientist.\nA query is made up of clauses. There are five important clauses: SELECT, FROM, WHERE, ORDER BY, and GROUP BY. Every query must have the SELECT4 and FROM5 clauses and the simplest query is SELECT * FROM table, which selects all columns from the specified table . This is what dplyr generates for an unadulterated table :\n\nflights |> show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\nplanes |> show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"planes\"\n\nWHERE and ORDER BY control which rows are included and how they are ordered:\n\nflights |> \n  filter(dest == \"IAH\") |> \n  arrange(dep_delay) |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\n#> WHERE (\"dest\" = 'IAH')\n#> ORDER BY \"dep_delay\"\n\nGROUP BY converts the query to a summary, causing aggregation to happen:\n\nflights |> \n  group_by(dest) |> \n  summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) |> \n  show_query()\n#> <SQL>\n#> SELECT \"dest\", AVG(\"dep_delay\") AS \"dep_delay\"\n#> FROM \"flights\"\n#> GROUP BY \"dest\"\n\nThere are two important differences between dplyr verbs and SELECT clauses:\n\nIn SQL, case doesn’t matter: you can write select, SELECT, or even SeLeCt. In this book we’ll stick with the common convention of writing SQL keywords in uppercase to distinguish them from table or variables names.\nIn SQL, order matters: you must always write the clauses in the order SELECT, FROM, WHERE, GROUP BY, ORDER BY. Confusingly, this order doesn’t match how the clauses actually evaluated which is first FROM, then WHERE, GROUP BY, SELECT, and ORDER BY.\n\nThe following sections explore each clause in more detail.\n\n\n\n\n\n\nNote that while SQL is a standard, it is extremely complex and no database follows it exactly. While the main components that we’ll focus on in this book are very similar between DBMSs, there are many minor variations. Fortunately, dbplyr is designed to handle this problem and generates different translations for different databases. It’s not perfect, but it’s continually improving, and if you hit a problem you can file an issue on GitHub to help us do better.\n\n\n\n\n24.6.2 SELECT\nThe SELECT clause is the workhorse of queries and performs the same job as select(), mutate(), rename(), relocate(), and, as you’ll learn in the next section, summarize().\nselect(), rename(), and relocate() have very direct translations to SELECT as they just affect where a column appears (if at all) along with its name:\n\nplanes |> \n  select(tailnum, type, manufacturer, model, year) |> \n  show_query()\n#> <SQL>\n#> SELECT \"tailnum\", \"type\", \"manufacturer\", \"model\", \"year\"\n#> FROM \"planes\"\n\nplanes |> \n  select(tailnum, type, manufacturer, model, year) |> \n  rename(year_built = year) |> \n  show_query()\n#> <SQL>\n#> SELECT \"tailnum\", \"type\", \"manufacturer\", \"model\", \"year\" AS \"year_built\"\n#> FROM \"planes\"\n\nplanes |> \n  select(tailnum, type, manufacturer, model, year) |> \n  relocate(manufacturer, model, .before = type) |> \n  show_query()\n#> <SQL>\n#> SELECT \"tailnum\", \"manufacturer\", \"model\", \"type\", \"year\"\n#> FROM \"planes\"\n\nThis example also shows you how SQL does renaming. In SQL terminology renaming is called aliasing and is done with AS. Note that unlike with mutate(), the old name is on the left and the new name is on the right.\n\n\n\n\n\n\nIn the examples above note that \"year\" and \"type\" are wrapped in double quotes. That’s because these are reserved words in duckdb, so dbplyr quotes them to avoid any potential confusion between column/table names and SQL operators.\nWhen working with other databases you’re likely to see every variable name quotes because only a handful of client packages, like duckdb, know what all the reserved words are, so they quote everything to be safe.\nSELECT \"tailnum\", \"type\", \"manufacturer\", \"model\", \"year\"\nFROM \"planes\"\nSome other database systems use backticks instead of quotes:\nSELECT `tailnum`, `type`, `manufacturer`, `model`, `year`\nFROM `planes`\n\n\n\nThe translations for mutate() are similarly straightforward: each variable becomes a new expression in SELECT:\n\nflights |> \n  mutate(\n    speed = distance / (air_time / 60)\n  ) |> \n  show_query()\n#> <SQL>\n#> SELECT *, \"distance\" / (\"air_time\" / 60.0) AS \"speed\"\n#> FROM \"flights\"\n\nWe’ll come back to the translation of individual components (like /) in Section 24.7.\n\n24.6.3 FROM\nThe FROM clause defines the data source. It’s going to be rather uninteresting for a little while, because we’re just using single tables. You’ll see more complex examples once we hit the join functions.\n\n24.6.4 GROUP BY\ngroup_by() is translated to the GROUP BY6 clause and summarise() is translated to the SELECT clause:\n\ndiamonds_db |> \n  group_by(cut) |> \n  summarise(\n    n = n(),\n    avg_price = mean(price, na.rm = TRUE)\n  ) |> \n  show_query()\n#> <SQL>\n#> SELECT \"cut\", COUNT(*) AS \"n\", AVG(\"price\") AS \"avg_price\"\n#> FROM \"diamonds\"\n#> GROUP BY \"cut\"\n\nWe’ll come back to what’s happening with translation n() and mean() in Section 24.7.\n\n24.6.5 WHERE\nfilter() is translated to the WHERE clause:\n\nflights |> \n  filter(dest == \"IAH\" | dest == \"HOU\") |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\n#> WHERE (\"dest\" = 'IAH' OR \"dest\" = 'HOU')\n\nflights |> \n  filter(arr_delay > 0 & arr_delay < 20) |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\n#> WHERE (\"arr_delay\" > 0.0 AND \"arr_delay\" < 20.0)\n\nThere are a few important details to note here:\n\n\n| becomes OR and & becomes AND.\nSQL uses = for comparison, not ==. SQL doesn’t have assignment, so there’s no potential for confusion there.\nSQL uses only '' for strings, not \"\". In SQL, \"\" is used to identify variables, like R’s ``.\n\nAnother useful SQL operator is IN, which is very close to R’s %in%:\n\nflights |> \n  filter(dest %in% c(\"IAH\", \"HOU\")) |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\n#> WHERE (\"dest\" IN ('IAH', 'HOU'))\n\nSQL uses NULL instead of NA. NULLs behave similarly to NAs. The main difference is that while they’re “infectious” in comparisons and arithmetic, they are silently dropped when summarizing. dbplyr will remind you about this behavior the first time you hit it:\n\nflights |> \n  group_by(dest) |> \n  summarise(delay = mean(arr_delay))\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # Source:   SQL [?? x 2]\n#> # Database: DuckDB 0.3.5-dev1410 [user1@Windows 10 x64:R 4.2.1/:memory:]\n#>   dest   delay\n#>   <chr>  <dbl>\n#> 1 ATL   11.3  \n#> 2 ORD    5.88 \n#> 3 RDU   10.1  \n#> 4 IAD   13.9  \n#> 5 DTW    5.43 \n#> 6 LAX    0.547\n#> # … with more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nIf you want to learn more about how NULLs work, you might enjoy “Three valued logic” by Markus Winand.\nIn general, you can work with NULLs using the functions you’d use for NAs in R:\n\nflights |> \n  filter(!is.na(dep_delay)) |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\n#> WHERE (NOT((\"dep_delay\" IS NULL)))\n\nThis SQL query illustrates one of the drawbacks of dbplyr: while the SQL is correct, it isn’t as simple as you might write by hand. In this case, you could drop the parentheses and use a special operator that’s easier to read:\nWHERE \"dep_delay\" IS NOT NULL\nNote that if you filter() a variable that you created using a summarize, dbplyr will generate a HAVING clause, rather than a FROM clause. This is a one of the idiosyncracies of SQL created because WHERE is evaluated before SELECT, so it needs another clause that’s evaluated afterwards.\n\ndiamonds_db |> \n  group_by(cut) |> \n  summarise(n = n()) |> \n  filter(n > 100) |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM (\n#>   SELECT \"cut\", COUNT(*) AS \"n\"\n#>   FROM \"diamonds\"\n#>   GROUP BY \"cut\"\n#> ) \"q01\"\n#> WHERE (\"n\" > 100.0)\n\n\n24.6.6 ORDER BY\nOrdering rows involves a straightforward translation from arrange() to the ORDER BY clause:\n\nflights |> \n  arrange(year, month, day, desc(dep_delay)) |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"flights\"\n#> ORDER BY \"year\", \"month\", \"day\", \"dep_delay\" DESC\n\nNotice how desc() is translated to DESC: this is one of the many dplyr functions whose name was directly inspired by SQL.\n\n24.6.7 Subqueries\nSometimes it’s not possible to translate a dplyr pipeline into a single SELECT statement and you need to use a subquery. A subquery is just a query used as a data source in the FROM clause, instead of the usual table.\ndbplyr typically uses subqueries to work around limitations of SQL. For example, expressions in the SELECT clause can’t refer to columns that were just created. That means that the following (silly) dplyr pipeline needs to happen in two steps: the first (inner) query computes year1 and then the second (outer) query can compute year2.\n\nflights |> \n  mutate(\n    year1 = year + 1,\n    year2 = year1 + 1\n  ) |> \n  show_query()\n#> <SQL>\n#> SELECT *, \"year1\" + 1.0 AS \"year2\"\n#> FROM (\n#>   SELECT *, \"year\" + 1.0 AS \"year1\"\n#>   FROM \"flights\"\n#> ) \"q01\"\n\nYou’ll also see this if you attempted to filter() a variable that you just created. Remember, even though WHERE is written after SELECT, it’s evaluated before it, so we need a subquery in this (silly) example:\n\nflights |> \n  mutate(year1 = year + 1) |> \n  filter(year1 == 2014) |> \n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM (\n#>   SELECT *, \"year\" + 1.0 AS \"year1\"\n#>   FROM \"flights\"\n#> ) \"q01\"\n#> WHERE (\"year1\" = 2014.0)\n\nSometimes dbplyr will create a subquery where it’s not needed because it doesn’t yet know how to optimize that translation. As dbplyr improves over time, these cases will get rarer but will probably never go away.\n\n24.6.8 Joins\nIf you’re familiar with dplyr’s joins, SQL joins are very similar. Here’s a simple example:\n\nflights |> \n  left_join(planes |> rename(year_built = year), by = \"tailnum\") |> \n  show_query()\n#> <SQL>\n#> SELECT\n#>   \"year\",\n#>   \"month\",\n#>   \"day\",\n#>   \"dep_time\",\n#>   \"sched_dep_time\",\n#>   \"dep_delay\",\n#>   \"arr_time\",\n#>   \"sched_arr_time\",\n#>   \"arr_delay\",\n#>   \"carrier\",\n#>   \"flight\",\n#>   \"LHS\".\"tailnum\" AS \"tailnum\",\n#>   \"origin\",\n#>   \"dest\",\n#>   \"air_time\",\n#>   \"distance\",\n#>   \"hour\",\n#>   \"minute\",\n#>   \"time_hour\",\n#>   \"year_built\",\n#>   \"type\",\n#>   \"manufacturer\",\n#>   \"model\",\n#>   \"engines\",\n#>   \"seats\",\n#>   \"speed\",\n#>   \"engine\"\n#> FROM \"flights\" AS \"LHS\"\n#> LEFT JOIN (\n#>   SELECT\n#>     \"tailnum\",\n#>     \"year\" AS \"year_built\",\n#>     \"type\",\n#>     \"manufacturer\",\n#>     \"model\",\n#>     \"engines\",\n#>     \"seats\",\n#>     \"speed\",\n#>     \"engine\"\n#>   FROM \"planes\"\n#> ) \"RHS\"\n#>   ON (\"LHS\".\"tailnum\" = \"RHS\".\"tailnum\")\n\nThe main thing to notice here is the syntax: SQL joins use sub-clauses of the FROM clause to bring in additional tables, using ON to define how the tables are related.\ndplyr’s names for these functions are so closely connected to SQL that you can easily guess the equivalent SQL for inner_join(), right_join(), and full_join():\nSELECT flights.*, \"type\", manufacturer, model, engines, seats, speed\nFROM flights\nINNER JOIN planes ON (flights.tailnum = planes.tailnum)\n\nSELECT flights.*, \"type\", manufacturer, model, engines, seats, speed\nFROM flights\nRIGHT JOIN planes ON (flights.tailnum = planes.tailnum)\n\nSELECT flights.*, \"type\", manufacturer, model, engines, seats, speed\nFROM flights\nFULL JOIN planes ON (flights.tailnum = planes.tailnum)\nYou’re likely to need many joins when working with data from a database. That’s because database tables are often stored in a highly normalized form, where each “fact” is stored in a single place and to keep a complete dataset for analysis you need to navigate a complex network of tables connected by primary and foreign keys. If you hit this scenario, the dm package, by Tobias Schieferdecker, Kirill Müller, and Darko Bergant, is a life saver. It can automatically determine the connections between tables using the constraints that DBAs often supply, visualize the connections so you can see what’s going on, and generate the joins you need to connect one table to another.\n\n24.6.9 Other verbs\ndbplyr also translates other verbs like distinct(), slice_*(), and intersect(), and a growing selection of tidyr functions like pivot_longer() and pivot_wider(). The easiest way to see the full set of what’s currently available is to visit the dbplyr website: https://dbplyr.tidyverse.org/reference/.\n\n24.6.10 Exercises\n\nWhat is distinct() translated to? How about head()?\n\nExplain what each of the following SQL queries do and try recreate them using dbplyr.\nSELECT * \nFROM flights\nWHERE dep_delay < arr_delay\n\nSELECT *, distance / (airtime / 60) AS speed\nFROM flights"
  },
  {
    "objectID": "databases.html#sec-sql-expressions",
    "href": "databases.html#sec-sql-expressions",
    "title": "24  Databases",
    "section": "\n24.7 Function translations",
    "text": "24.7 Function translations\nSo far we’ve focused on the big picture of how dplyr verbs are translated to the clauses of a query. Now we’re going to zoom in a little and talk about the translation of the R functions that work with individual columns, e.g. what happens when you use mean(x) in a summarize()?\nTo help see what’s going on, we’ll use a couple of little helper functions that run a summarise() or mutate() and show the generated SQL. That will make it a easier to explore a few variations and see how summaries and transformations can differ.\n\nsummarize_query <- function(df, ...) {\n  df |> \n    summarise(...) |> \n    show_query()\n}\nmutate_query <- function(df, ...) {\n  df |> \n    mutate(..., .keep = \"none\") |> \n    show_query()\n}\n\nLet’s dive in with some summaries! Looking at the code below you’ll notice that some summary functions, like mean(), have a relatively simple translation while others, like median(), are much more complex. The complexity is typically higher for operations that are common in statistics but less common in databases.\n\nflights |> \n  group_by(year, month, day) |>  \n  summarize_query(\n    mean = mean(arr_delay, na.rm = TRUE),\n    median = median(arr_delay, na.rm = TRUE)\n  )\n#> `summarise()` has grouped output by \"year\" and \"month\". You can override using\n#> the `.groups` argument.\n#> <SQL>\n#> SELECT\n#>   \"year\",\n#>   \"month\",\n#>   \"day\",\n#>   AVG(\"arr_delay\") AS \"mean\",\n#>   PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY \"arr_delay\") AS \"median\"\n#> FROM \"flights\"\n#> GROUP BY \"year\", \"month\", \"day\"\n\nThe translation of summary functions becomes more complicated when you use them inside a mutate() because they have to turn into a window function. In SQL, you turn an ordinary aggregation function into a window function by adding OVER after it:\n\nflights |> \n  group_by(year, month, day) |>  \n  mutate_query(\n    mean = mean(arr_delay, na.rm = TRUE),\n  )\n#> <SQL>\n#> SELECT\n#>   \"year\",\n#>   \"month\",\n#>   \"day\",\n#>   AVG(\"arr_delay\") OVER (PARTITION BY \"year\", \"month\", \"day\") AS \"mean\"\n#> FROM \"flights\"\n\nIn SQL, the GROUP BY clause is used exclusively for summary so here you can see that the grouping has moved to the PARTITION BY argument to OVER.\nWindow functions include all functions that look forward or backwards, like lead() and lag():\n\nflights |> \n  group_by(dest) |>  \n  arrange(time_hour) |> \n  mutate_query(\n    lead = lead(arr_delay),\n    lag = lag(arr_delay)\n  )\n#> <SQL>\n#> SELECT\n#>   \"dest\",\n#>   LEAD(\"arr_delay\", 1, NULL) OVER (PARTITION BY \"dest\" ORDER BY \"time_hour\") AS \"lead\",\n#>   LAG(\"arr_delay\", 1, NULL) OVER (PARTITION BY \"dest\" ORDER BY \"time_hour\") AS \"lag\"\n#> FROM \"flights\"\n#> ORDER BY \"time_hour\"\n\nHere it’s important to arrange() the data, because SQL tables have no intrinsic order. In fact, if you don’t use arrange() you might get the rows back in a different order every time! Notice for window functions, the ordering information is repeated: the ORDER BY clause of the main query doesn’t automatically apply to window functions.\nAnother important SQL function is CASE WHEN. It’s used as the translation of if_else() and case_when(), the dplyr function that it directly inspired. Here’s a couple of simple examples:\n\nflights |> \n  mutate_query(\n    description = if_else(arr_delay > 0, \"delayed\", \"on-time\")\n  )\n#> <SQL>\n#> SELECT CASE WHEN (\"arr_delay\" > 0.0) THEN 'delayed' WHEN NOT (\"arr_delay\" > 0.0) THEN 'on-time' END AS \"description\"\n#> FROM \"flights\"\nflights |> \n  mutate_query(\n    description = \n      case_when(\n        arr_delay < -5 ~ \"early\", \n        arr_delay < 5 ~ \"on-time\",\n        arr_delay >= 5 ~ \"late\"\n      )\n  )\n#> <SQL>\n#> SELECT CASE\n#> WHEN (\"arr_delay\" < -5.0) THEN 'early'\n#> WHEN (\"arr_delay\" < 5.0) THEN 'on-time'\n#> WHEN (\"arr_delay\" >= 5.0) THEN 'late'\n#> END AS \"description\"\n#> FROM \"flights\"\n\nCASE WHEN is also used for some other functions that don’t have a direct translation from R to SQL. A good example of this is cut():\n\nflights |> \n  mutate_query(\n    description =  cut(\n      arr_delay, \n      breaks = c(-Inf, -5, 5, Inf), \n      labels = c(\"early\", \"on-time\", \"late\")\n    )\n  )\n#> <SQL>\n#> SELECT CASE\n#> WHEN (\"arr_delay\" <= -5.0) THEN 'early'\n#> WHEN (\"arr_delay\" <= 5.0) THEN 'on-time'\n#> WHEN (\"arr_delay\" > 5.0) THEN 'late'\n#> END AS \"description\"\n#> FROM \"flights\"\n\ndbplyr also translates common string and date-time manipulation functions, which you can learn about in vignette(\"translation-function\", package = \"dbplyr\"). dbplyr’s translations are certainly not perfect, and there are many R functions that aren’t translated yet, but dbplyr does a surprisingly good job covering the functions that you’ll use most of the time.\n\n24.7.1 Learning more\nIf you’ve finished this chapter and would like to learn more about SQL. We have two recommendations:\n\n\nSQL for Data Scientists by Renée M. P. Teate is an introduction to SQL designed specifically for the needs of data scientists, and includes examples of the sort of highly interconnected data you’re likely to encounter in real organisations.\n\nPractical SQL by Anthony DeBarros is written from the perspective of a data journalist (a data scientist specialized in telling compelling stories) and goes into more detail about getting your data into a database and running your own DBMS."
  },
  {
    "objectID": "rectangling.html",
    "href": "rectangling.html",
    "title": "25  Data rectangling",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "rectangling.html#introduction",
    "href": "rectangling.html#introduction",
    "title": "25  Data rectangling",
    "section": "\n25.1 Introduction",
    "text": "25.1 Introduction\nIn this chapter, you’ll learn the art of data rectangling, taking data that is fundamentally tree-like and converting it into a rectangular data frames made up of rows and columns. This is important because hierarchical data is surprisingly common, especially when working with data that comes from a web API.\nTo learn about rectangling, you’ll first learn about lists, the data structure that makes hierarchical data possible in R. Then you’ll learn about two crucial tidyr functions: tidyr::unnest_longer(), which converts children in rows, and tidyr::unnest_wider(), which converts children into columns. We’ll then show you a few case studies, applying these simple function multiple times to solve real problems. We’ll finish off by talking about JSON, the most frequent source of hierarchical datasets and a common format for data exchange on the web.\n\n25.1.1 Prerequisites\nIn this chapter we’ll use many functions from tidyr, a core member of the tidyverse. We’ll also use repurrrsive to provide some interesting datasets rectangling practice, and we’ll finish up with a little jsonlite, which we’ll use to read JSON files into R lists.\n\nlibrary(tidyverse)\nlibrary(repurrrsive)\nlibrary(jsonlite)"
  },
  {
    "objectID": "rectangling.html#lists",
    "href": "rectangling.html#lists",
    "title": "25  Data rectangling",
    "section": "\n25.2 Lists",
    "text": "25.2 Lists\nSo far we’ve used simple vectors like integers, numbers, characters, date-times, and factors. These vectors are simple because they’re homogeneous: every element is same type. If you want to store element of different types in the same vector, you’ll need a list, which you create with list():\n\nx1 <- list(1:4, \"a\", TRUE)\nx1\n#> [[1]]\n#> [1] 1 2 3 4\n#> \n#> [[2]]\n#> [1] \"a\"\n#> \n#> [[3]]\n#> [1] TRUE\n\nIt’s often convenient to name the components, or children, of a list, which you can do in the same way as naming the columns of a tibble:\n\nx2 <- list(a = 1:2, b = 1:3, c = 1:4)\nx2\n#> $a\n#> [1] 1 2\n#> \n#> $b\n#> [1] 1 2 3\n#> \n#> $c\n#> [1] 1 2 3 4\n\nEven for these very simple lists, printing takes up quite a lot of space. A useful alternative is str(), which generates a compact display of the structure, de-emphasizing the contents:\n\nstr(x1)\n#> List of 3\n#>  $ : int [1:4] 1 2 3 4\n#>  $ : chr \"a\"\n#>  $ : logi TRUE\nstr(x2)\n#> List of 3\n#>  $ a: int [1:2] 1 2\n#>  $ b: int [1:3] 1 2 3\n#>  $ c: int [1:4] 1 2 3 4\n\nAs you can see, str() displays each child of the list on its own line. It displays the name, if present, then an abbreviation of the type, then the first few values.\n\n25.2.1 Hierarchy\nLists can contain any type of object, including other lists. This makes them suitable for representing hierarchical (tree-like) structures:\n\nx3 <- list(list(1, 2), list(3, 4))\nstr(x3)\n#> List of 2\n#>  $ :List of 2\n#>   ..$ : num 1\n#>   ..$ : num 2\n#>  $ :List of 2\n#>   ..$ : num 3\n#>   ..$ : num 4\n\nThis is notably different to c(), which generates a flat vector:\n\nc(c(1, 2), c(3, 4))\n#> [1] 1 2 3 4\n\nx4 <- c(list(1, 2), list(3, 4))\nstr(x4)\n#> List of 4\n#>  $ : num 1\n#>  $ : num 2\n#>  $ : num 3\n#>  $ : num 4\n\nAs lists get more complex, str() gets more useful, as it lets you see the hierarchy at a glance:\n\nx5 <- list(1, list(2, list(3, list(4, list(5)))))\nstr(x5)\n#> List of 2\n#>  $ : num 1\n#>  $ :List of 2\n#>   ..$ : num 2\n#>   ..$ :List of 2\n#>   .. ..$ : num 3\n#>   .. ..$ :List of 2\n#>   .. .. ..$ : num 4\n#>   .. .. ..$ :List of 1\n#>   .. .. .. ..$ : num 5\n\nAs lists get even large and more complex, even str() starts to fail, you’ll need to switch to View()1. Figure 25.1 shows the result of calling View(x4). The viewer starts by showing just the top level of the list, but you can interactively expand any of the components to see more, as in Figure 25.2. RStudio will also show you the code you need to access that element, as in Figure 25.3. We’ll come back to how this code works in Section 28.4.5.\n\n\n\n\nFigure 25.1: The RStudio allows you to interactively explore a complex list. The viewer opens showing only the top level of the list.\n\n\n\n\n\n\n\n\nFigure 25.2: Clicking on the rightward facing triangle expands that component of the list so that you can also see its children.\n\n\n\n\n\n\n\n\nFigure 25.3: You can repeat this operation as many times as needed to get to the data you’re interested in. Note the bottom-right corner: if you click an element of the list, RStudio will give you the subsetting code needed to access it, in this case x4[[2]][[2]][[2]].\n\n\n\n\n\n25.2.2 List-columns\nLists can also live inside a tibble, where we call them list-columns. List-columns are useful because they allow you to shoehorn in objects that wouldn’t wouldn’t usually belong in a tibble. In particular, list-columns are are used a lot in the tidymodels ecosystem, because they allows you to store things like models or resamples in a data frame.\nHere’s a simple example of a list-column:\n\ndf <- tibble(\n  x = 1:2, \n  y = c(\"a\", \"b\"),\n  z = list(list(1, 2), list(3, 4, 5))\n)\ndf\n#> # A tibble: 2 × 3\n#>       x y     z         \n#>   <int> <chr> <list>    \n#> 1     1 a     <list [2]>\n#> 2     2 b     <list [3]>\n\nThere’s nothing special about lists in a tibble; they behave like any other column:\n\ndf |> \n  filter(x == 1)\n#> # A tibble: 1 × 3\n#>       x y     z         \n#>   <int> <chr> <list>    \n#> 1     1 a     <list [2]>\n\nComputing with list-columns is harder, but that’s because computing with lists is harder in general; we’ll come back to that in Chapter 29. In this chapter, we’ll focus on unnesting list-columns out into regular variables so you can use your existing tools on them.\nThe default print method just displays a rough summary of the contents. The list column could be arbitrarily complex, so there’s no good way to print it. If you want to see it, you’ll need to pull the list-column out and apply one of the techniques that you learned above:\n\ndf |> \n  filter(x == 1) |> \n  pull(z) |> \n  str()\n#> List of 1\n#>  $ :List of 2\n#>   ..$ : num 1\n#>   ..$ : num 2\n\nSimilarly, if you View() a data frame in RStudio, you’ll get the standard tabular view, which doesn’t allow you to selectively expand list columns. To explore those fields you’ll need to pull() and view, e.g. df |> pull(z) |> View().\n\n\n\n\n\n\nBase R\n\n\n\nIt’s possible to put a list in a column of a data.frame, but it’s a lot fiddlier because data.frame() treats a list as a list of columns:\n\ndata.frame(x = list(1:3, 3:5))\n#>   x.1.3 x.3.5\n#> 1     1     3\n#> 2     2     4\n#> 3     3     5\n\nYou can force data.frame() to treat a list as a list of rows by wrapping it in list I(), but the result doesn’t print particularly usefully:\n\ndata.frame(\n  x = I(list(1:3, 3:5)), \n  y = c(\"1, 2\", \"3, 4, 5\")\n)\n#>         x       y\n#> 1 1, 2, 3    1, 2\n#> 2 3, 4, 5 3, 4, 5\n\nIt’s easier to use list-columns with tibbles because tibble() treats lists like either vectors and the print method has been designed with lists in mind."
  },
  {
    "objectID": "rectangling.html#unnesting",
    "href": "rectangling.html#unnesting",
    "title": "25  Data rectangling",
    "section": "\n25.3 Unnesting",
    "text": "25.3 Unnesting\nNow that you’ve learned the basics of lists and list-columns, lets explore how you can turn them back into regular rows and columns. We’ll start with very simple sample data so you can get the basic idea, and then switch to more realistic examples in the next section.\nList-columns tend to come in two basic forms: named and unnamed. When the children are named, they tend to have the same names in every row. When the children are unnamed, the number of elements tends to vary from row-to-row. The following code creates an example of each. In df1, every element of list-column y has two elements named a and b. If df2, the elements of list-column y are unnamed and vary in length.\n\ndf1 <- tribble(\n  ~x, ~y,\n  1, list(a = 11, b = 12),\n  2, list(a = 21, b = 22),\n  3, list(a = 31, b = 32),\n)\n\ndf2 <- tribble(\n  ~x, ~y,\n  1, list(11, 12, 13),\n  2, list(21),\n  3, list(31, 32),\n)\n\nNamed list-columns naturally unnest into columns: each named element becomes a new named column. Unnamed list-columns naturally unnested in to rows: you’ll get one row for each child. tidyr provides two functions for these two case: unnest_wider() and unnest_longer(). The following sections explain how they work.\n\n25.3.1 unnest_wider()\n\nWhen each row has the same number of elements with the same names, like df1, it’s natural to put each component into its own column with unnest_wider():\n\ndf1 |> \n  unnest_wider(y)\n#> # A tibble: 3 × 3\n#>       x     a     b\n#>   <dbl> <dbl> <dbl>\n#> 1     1    11    12\n#> 2     2    21    22\n#> 3     3    31    32\n\nBy default, the names of the new columns come exclusively from the names of the list, but you can use the names_sep argument to request that they combine the column name and the list names. This is useful for disambiguating repeated names.\n\ndf1 |> \n  unnest_wider(y, names_sep = \"_\")\n#> # A tibble: 3 × 3\n#>       x   y_a   y_b\n#>   <dbl> <dbl> <dbl>\n#> 1     1    11    12\n#> 2     2    21    22\n#> 3     3    31    32\n\nWe can also use unnest_wider() with unnamed list-columns, as in df2. Since columns require names but the list lacks them, unnest_wider() will label them with consecutive integers:\n\ndf2 |> \n  unnest_wider(y, names_sep = \"_\")\n#> # A tibble: 3 × 4\n#>       x   y_1   y_2   y_3\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1    11    12    13\n#> 2     2    21    NA    NA\n#> 3     3    31    32    NA\n\nYou’ll notice that unnested_wider(), much like pivot_wider(), turns implicit missing values in to explicit missing values.\n\n25.3.2 unnest_longer()\n\nWhen each row contains an unnamed list, it’s most natural to put each element into its own row with unnest_longer():\n\ndf2 |> \n  unnest_longer(y)\n#> # A tibble: 6 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1    11\n#> 2     1    12\n#> 3     1    13\n#> 4     2    21\n#> 5     3    31\n#> 6     3    32\n\nNote how x is duplicated for each element inside of y: we get one row of output for each element inside the list-column. But what happens if the list-column is empty, as in the following example?\n\ndf6 <- tribble(\n  ~x, ~y,\n  \"a\", list(1, 2),\n  \"b\", list(3),\n  \"c\", list()\n)\ndf6 |> unnest_longer(y)\n#> # A tibble: 3 × 2\n#>   x         y\n#>   <chr> <dbl>\n#> 1 a         1\n#> 2 a         2\n#> 3 b         3\n\nWe get zero rows in the output, so the row effectively disappears. Once https://github.com/tidyverse/tidyr/issues/1339 is fixed, you’ll be able to keep this row, replacing y with NA by setting keep_empty = TRUE.\nYou can also unnest named list-columns, like df1$y into the rows. Because the elements are named, and those names might be useful data, puts them in a new column with the suffix_id:\n\ndf1 |> \n  unnest_longer(y)\n#> # A tibble: 6 × 3\n#>       x     y y_id \n#>   <dbl> <dbl> <chr>\n#> 1     1    11 a    \n#> 2     1    12 b    \n#> 3     2    21 a    \n#> 4     2    22 b    \n#> 5     3    31 a    \n#> 6     3    32 b\n\nIf you don’t want these ids, you can suppress this with indices_include = FALSE. On the other hand, it’s sometimes useful to retain the position of unnamed elements in unnamed list-columns. You can do this with indices_include = TRUE:\n\ndf2 |> \n  unnest_longer(y, indices_include = TRUE)\n#> # A tibble: 6 × 3\n#>       x     y  y_id\n#>   <dbl> <dbl> <int>\n#> 1     1    11     1\n#> 2     1    12     2\n#> 3     1    13     3\n#> 4     2    21     1\n#> 5     3    31     1\n#> 6     3    32     2\n\n\n25.3.3 Inconsistent types\nWhat happens if you unnest a list-column contains different types of vector? For example, take the following dataset where the list-column y contains two numbers, a factor, and a logical, which can’t normally be mixed in a single column.\n\ndf4 <- tribble(\n  ~x, ~y,\n  \"a\", list(1, \"a\"),\n  \"b\", list(TRUE, factor(\"a\"), 5)\n)\n\nunnest_longer() always keeps the set of columns change, while changing the number of rows. So what happens? How does unnest_longer() produce five rows while keeping everything in y?\n\ndf4 |> \n  unnest_longer(y)\n#> # A tibble: 5 × 2\n#>   x     y        \n#>   <chr> <list>   \n#> 1 a     <dbl [1]>\n#> 2 a     <chr [1]>\n#> 3 b     <lgl [1]>\n#> 4 b     <fct [1]>\n#> 5 b     <dbl [1]>\n\nAs you can see, the output contains a list-column, but every element of the list-column contains a single element. Because unnest_longer() can’t find a common type of vector, it keeps the original types in a list-column. You might wonder if this breaks the commandment that every element of a column must be the same type — not quite, because every element is a still a list, and each component of that list contains something different.\nWhat happens if you find this problem in a dataset you’re trying to rectangle? There are two basic options. You could use the transform argument to coerce all inputs to a common type. It’s not particularly useful here because there’s only really one class that these five class can be converted to: character.\n\ndf4 |> \n  unnest_longer(y, transform = as.character)\n#> # A tibble: 5 × 2\n#>   x     y    \n#>   <chr> <chr>\n#> 1 a     1    \n#> 2 a     a    \n#> 3 b     TRUE \n#> 4 b     a    \n#> 5 b     5\n\nAnother option would be to filter down to the rows that have values of a specific type:\n\ndf4 |> \n  unnest_longer(y) |> \n  rowwise() |> \n  filter(is.numeric(y))\n#> # A tibble: 2 × 2\n#> # Rowwise: \n#>   x     y        \n#>   <chr> <list>   \n#> 1 a     <dbl [1]>\n#> 2 b     <dbl [1]>\n\nThen you can call unnest_longer() once more:\n\ndf4 |> \n  unnest_longer(y) |> \n  rowwise() |> \n  filter(is.numeric(y)) |> \n  unnest_longer(y)\n#> # A tibble: 2 × 2\n#>   x         y\n#>   <chr> <dbl>\n#> 1 a         1\n#> 2 b         5\n\n\n25.3.4 Other functions\ntidyr has a few other useful rectangling functions that we’re not going to cover in this book:\n\n\nunnest_auto() automatically picks between unnest_longer() and unnest_wider() based on the structure of the list-column. It’s a great for rapid exploration, but ultimately its a bad idea because it doesn’t force you to understand how your data is structured, and makes your code harder to understand.\n\nunnest() expands both rows and columns. It’s useful when you have a list-column that contains a 2d structure like a data frame, which we don’t see in this book.\n\nhoist() allows you to reach into a deeply nested list and extract just the components that you need. It’s mostly equivalent to repeated invocations of unnest_wider() + select() so read up on it if you’re trying to extract just a couple of important variables embedded in a bunch of data that you don’t care about.\n\nThese are good to know about when you’re other people’s code and for tackling rarer rectangling challenges.\n\n25.3.5 Exercises\n\n\nFrom time-to-time you encounter data frames with multiple list-columns with aligned values. For example, in the following data frame, the values of y and z are aligned (i.e. y and z will always have the same length within a row, and the first value of y corresponds to the first value of z). What happens if you apply two unnest_longer() calls to this data frame? How can you preserve the relationship between x and y? (Hint: carefully read the docs).\n\ndf4 <- tribble(\n  ~x, ~y, ~z,\n  \"a\", list(\"y-a-1\", \"y-a-2\"), list(\"z-a-1\", \"z-a-2\"),\n  \"b\", list(\"y-b-1\", \"y-b-2\", \"y-b-3\"), list(\"z-b-1\", \"z-b-2\", \"z-b-3\")\n)"
  },
  {
    "objectID": "rectangling.html#case-studies",
    "href": "rectangling.html#case-studies",
    "title": "25  Data rectangling",
    "section": "\n25.4 Case studies",
    "text": "25.4 Case studies\nSo far you’ve learned about the simplest case of list-columns, where rectangling only requires a single call to unnest_longer() or unnest_wider(). The main difference between real data and these simple examples is that real data typically containsmultiple levels of nesting that requires multiple calls to unnest_longer() and unnest_wider(). This section will work through four real rectangling challenges using datasets from the repurrrsive package that are inspired by datasets that we’ve encountered in the wild.\n\n25.4.1 Very wide data\nWe’ll start by exploring gh_repos. This is a list that contains data about a collection of GitHub repositories retrieved using the GitHub API. It’s a very deeply nested list so it’s difficult to show the structure in this book; you might want to explore a little on your own with View(gh_repos) before we continue.\ngh_repos is a list, but our tools work with list-columns, so we’ll begin by putting it into a tibble. We call the column json for reasons we’ll get to later.\n\nrepos <- tibble(json = gh_repos)\nrepos\n#> # A tibble: 6 × 1\n#>   json       \n#>   <list>     \n#> 1 <list [30]>\n#> 2 <list [30]>\n#> 3 <list [30]>\n#> 4 <list [26]>\n#> 5 <list [30]>\n#> 6 <list [30]>\n\nThis tibble contains 6 rows, one row for each child of gh_repos. Each row contains a unnamed list with either 26 or 30 rows. Since these are unnamed, we’ll start with an unnest_longer() to put each child in its own row:\n\nrepos |> \n  unnest_longer(json)\n#> # A tibble: 176 × 1\n#>   json             \n#>   <list>           \n#> 1 <named list [68]>\n#> 2 <named list [68]>\n#> 3 <named list [68]>\n#> 4 <named list [68]>\n#> 5 <named list [68]>\n#> 6 <named list [68]>\n#> # … with 170 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAt first glance, it might seem like we haven’t improved the situation: while we have more rows (176 instead of 6) each element of json is still a list. However, there’s an important difference: now each element is a named list so we can use unnamed_wider() to put each element into its own column:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) \n#> # A tibble: 176 × 68\n#>        id name  full_…¹ owner        private html_…² descr…³ fork  url   forks…⁴\n#>     <int> <chr> <chr>   <list>       <lgl>   <chr>   <chr>   <lgl> <chr> <chr>  \n#> 1  6.12e7 after gaborc… <named list> FALSE   https:… Run Co… FALSE http… https:…\n#> 2  4.05e7 argu… gaborc… <named list> FALSE   https:… Declar… FALSE http… https:…\n#> 3  3.64e7 ask   gaborc… <named list> FALSE   https:… Friend… FALSE http… https:…\n#> 4  3.49e7 base… gaborc… <named list> FALSE   https:… Do we … FALSE http… https:…\n#> 5  6.16e7 cite… gaborc… <named list> FALSE   https:… Test R… TRUE  http… https:…\n#> 6  3.39e7 clis… gaborc… <named list> FALSE   https:… Unicod… FALSE http… https:…\n#> # … with 170 more rows, 58 more variables: keys_url <chr>,\n#> #   collaborators_url <chr>, teams_url <chr>, hooks_url <chr>,\n#> #   issue_events_url <chr>, events_url <chr>, assignees_url <chr>,\n#> #   branches_url <chr>, tags_url <chr>, blobs_url <chr>, git_tags_url <chr>,\n#> #   git_refs_url <chr>, trees_url <chr>, statuses_url <chr>,\n#> #   languages_url <chr>, stargazers_url <chr>, contributors_url <chr>,\n#> #   subscribers_url <chr>, subscription_url <chr>, commits_url <chr>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThis has worked but the result is a little overwhelming: there are so many columns that tibble doesn’t even print all of them! We can see them all with names():\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  names()\n#>  [1] \"id\"                \"name\"              \"full_name\"        \n#>  [4] \"owner\"             \"private\"           \"html_url\"         \n#>  [7] \"description\"       \"fork\"              \"url\"              \n#> [10] \"forks_url\"         \"keys_url\"          \"collaborators_url\"\n#> [13] \"teams_url\"         \"hooks_url\"         \"issue_events_url\" \n#> [16] \"events_url\"        \"assignees_url\"     \"branches_url\"     \n#> [19] \"tags_url\"          \"blobs_url\"         \"git_tags_url\"     \n#> [22] \"git_refs_url\"      \"trees_url\"         \"statuses_url\"     \n#> [25] \"languages_url\"     \"stargazers_url\"    \"contributors_url\" \n#> [28] \"subscribers_url\"   \"subscription_url\"  \"commits_url\"      \n#> [31] \"git_commits_url\"   \"comments_url\"      \"issue_comment_url\"\n#> [34] \"contents_url\"      \"compare_url\"       \"merges_url\"       \n#> [37] \"archive_url\"       \"downloads_url\"     \"issues_url\"       \n#> [40] \"pulls_url\"         \"milestones_url\"    \"notifications_url\"\n#> [43] \"labels_url\"        \"releases_url\"      \"deployments_url\"  \n#> [46] \"created_at\"        \"updated_at\"        \"pushed_at\"        \n#> [49] \"git_url\"           \"ssh_url\"           \"clone_url\"        \n#> [52] \"svn_url\"           \"homepage\"          \"size\"             \n#> [55] \"stargazers_count\"  \"watchers_count\"    \"language\"         \n#> [58] \"has_issues\"        \"has_downloads\"     \"has_wiki\"         \n#> [61] \"has_pages\"         \"forks_count\"       \"mirror_url\"       \n#> [64] \"open_issues_count\" \"forks\"             \"open_issues\"      \n#> [67] \"watchers\"          \"default_branch\"\n\nLet’s select a few that look interesting:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  select(id, full_name, owner, description)\n#> # A tibble: 176 × 4\n#>         id full_name               owner             description                \n#>      <int> <chr>                   <list>            <chr>                      \n#> 1 61160198 gaborcsardi/after       <named list [17]> Run Code in the Background \n#> 2 40500181 gaborcsardi/argufy      <named list [17]> Declarative function argum…\n#> 3 36442442 gaborcsardi/ask         <named list [17]> Friendly CLI interaction i…\n#> 4 34924886 gaborcsardi/baseimports <named list [17]> Do we get warnings for und…\n#> 5 61620661 gaborcsardi/citest      <named list [17]> Test R package and repo fo…\n#> 6 33907457 gaborcsardi/clisymbols  <named list [17]> Unicode symbols for CLI ap…\n#> # … with 170 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou can use this to work back to understand how gh_repos was strucured: each child was a GitHub user containing a list of up to 30 GitHub repositories that they created.\nowner is another list-column, and since it a contains a named list, we can use unnest_wider() to get at the values:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  select(id, full_name, owner, description) |> \n  unnest_wider(owner)\n#> Error in `unpack()`:\n#> ! Names must be unique.\n#> ✖ These names are duplicated:\n#>   * \"id\" at locations 1 and 4.\n#> ℹ Use argument `names_repair` to specify repair strategy.\n\nUh oh, this list column also contains an id column and we can’t have two id columns in the same data frame. Rather than following the advice to use names_repair (which would also work), we’ll instead use names_sep:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  select(id, full_name, owner, description) |> \n  unnest_wider(owner, names_sep = \"_\")\n#> # A tibble: 176 × 20\n#>       id full_…¹ owner…² owner…³ owner…⁴ owner…⁵ owner…⁶ owner…⁷ owner…⁸ owner…⁹\n#>    <int> <chr>   <chr>     <int> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n#> 1 6.12e7 gaborc… gaborc…  660288 https:… \"\"      https:… https:… https:… https:…\n#> 2 4.05e7 gaborc… gaborc…  660288 https:… \"\"      https:… https:… https:… https:…\n#> 3 3.64e7 gaborc… gaborc…  660288 https:… \"\"      https:… https:… https:… https:…\n#> 4 3.49e7 gaborc… gaborc…  660288 https:… \"\"      https:… https:… https:… https:…\n#> 5 6.16e7 gaborc… gaborc…  660288 https:… \"\"      https:… https:… https:… https:…\n#> 6 3.39e7 gaborc… gaborc…  660288 https:… \"\"      https:… https:… https:… https:…\n#> # … with 170 more rows, 10 more variables: owner_gists_url <chr>,\n#> #   owner_starred_url <chr>, owner_subscriptions_url <chr>,\n#> #   owner_organizations_url <chr>, owner_repos_url <chr>,\n#> #   owner_events_url <chr>, owner_received_events_url <chr>, owner_type <chr>,\n#> #   owner_site_admin <lgl>, description <chr>, and abbreviated variable names\n#> #   ¹​full_name, ²​owner_login, ³​owner_id, ⁴​owner_avatar_url, ⁵​owner_gravatar_id,\n#> #   ⁶​owner_url, ⁷​owner_html_url, ⁸​owner_followers_url, ⁹​owner_following_url\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nThis gives another wide dataset, but you can see that owner appears to contain a lot of additional data about the person who “owns” the repository.\n\n25.4.2 Relational data\nNested data is sometimes used to represent data that we’d usually spread out into multiple data frames. For example, take got_chars. Like gh_repos it’s a list, so we start by turning it into a list-column of a tibble:\n\nchars <- tibble(json = got_chars)\nchars\n#> # A tibble: 30 × 1\n#>   json             \n#>   <list>           \n#> 1 <named list [18]>\n#> 2 <named list [18]>\n#> 3 <named list [18]>\n#> 4 <named list [18]>\n#> 5 <named list [18]>\n#> 6 <named list [18]>\n#> # … with 24 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThe json column contains named values, so we’ll start by widening it:\n\nchars |> \n  unnest_wider(json)\n#> # A tibble: 30 × 18\n#>   url            id name  gender culture born  died  alive titles aliases father\n#>   <chr>       <int> <chr> <chr>  <chr>   <chr> <chr> <lgl> <list> <list>  <chr> \n#> 1 https://ww…  1022 Theo… Male   \"Ironb… \"In … \"\"    TRUE  <chr>  <chr>   \"\"    \n#> 2 https://ww…  1052 Tyri… Male   \"\"      \"In … \"\"    TRUE  <chr>  <chr>   \"\"    \n#> 3 https://ww…  1074 Vict… Male   \"Ironb… \"In … \"\"    TRUE  <chr>  <chr>   \"\"    \n#> 4 https://ww…  1109 Will  Male   \"\"      \"\"    \"In … FALSE <chr>  <chr>   \"\"    \n#> 5 https://ww…  1166 Areo… Male   \"Norvo… \"In … \"\"    TRUE  <chr>  <chr>   \"\"    \n#> 6 https://ww…  1267 Chett Male   \"\"      \"At … \"In … FALSE <chr>  <chr>   \"\"    \n#> # … with 24 more rows, and 7 more variables: mother <chr>, spouse <chr>,\n#> #   allegiances <list>, books <list>, povBooks <list>, tvSeries <list>,\n#> #   playedBy <list>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nAnd selecting a few columns just to make it easier to read:\n\ncharacters <- chars |> \n  unnest_wider(json) |> \n  select(id, name, gender, culture, born, died, alive)\ncharacters\n#> # A tibble: 30 × 7\n#>      id name              gender culture    born                     died  alive\n#>   <int> <chr>             <chr>  <chr>      <chr>                    <chr> <lgl>\n#> 1  1022 Theon Greyjoy     Male   \"Ironborn\" \"In 278 AC or 279 AC, a… \"\"    TRUE \n#> 2  1052 Tyrion Lannister  Male   \"\"         \"In 273 AC, at Casterly… \"\"    TRUE \n#> 3  1074 Victarion Greyjoy Male   \"Ironborn\" \"In 268 AC or before, a… \"\"    TRUE \n#> 4  1109 Will              Male   \"\"         \"\"                       \"In … FALSE\n#> 5  1166 Areo Hotah        Male   \"Norvoshi\" \"In 257 AC or before, a… \"\"    TRUE \n#> 6  1267 Chett             Male   \"\"         \"At Hag's Mire\"          \"In … FALSE\n#> # … with 24 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThere are also many list-columns:\n\nchars |> \n  unnest_wider(json) |> \n  select(id, where(is.list))\n#> # A tibble: 30 × 8\n#>      id titles    aliases    allegiances books     povBooks  tvSeries  playedBy \n#>   <int> <list>    <list>     <list>      <list>    <list>    <list>    <list>   \n#> 1  1022 <chr [3]> <chr [4]>  <chr [1]>   <chr [3]> <chr [2]> <chr [6]> <chr [1]>\n#> 2  1052 <chr [2]> <chr [11]> <chr [1]>   <chr [2]> <chr [4]> <chr [6]> <chr [1]>\n#> 3  1074 <chr [2]> <chr [1]>  <chr [1]>   <chr [3]> <chr [2]> <chr [1]> <chr [1]>\n#> 4  1109 <chr [1]> <chr [1]>  <NULL>      <chr [1]> <chr [1]> <chr [1]> <chr [1]>\n#> 5  1166 <chr [1]> <chr [1]>  <chr [1]>   <chr [3]> <chr [2]> <chr [2]> <chr [1]>\n#> 6  1267 <chr [1]> <chr [1]>  <NULL>      <chr [2]> <chr [1]> <chr [1]> <chr [1]>\n#> # … with 24 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nLets explore the titles column. It’s an unnamed list-column, so we’ll unnest it into rows:\n\nchars |> \n  unnest_wider(json) |> \n  select(id, titles) |> \n  unnest_longer(titles)\n#> # A tibble: 60 × 2\n#>      id titles                                              \n#>   <int> <chr>                                               \n#> 1  1022 Prince of Winterfell                                \n#> 2  1022 Captain of Sea Bitch                                \n#> 3  1022 Lord of the Iron Islands (by law of the green lands)\n#> 4  1052 Acting Hand of the King (former)                    \n#> 5  1052 Master of Coin (former)                             \n#> 6  1074 Lord Captain of the Iron Fleet                      \n#> # … with 54 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou might expect to see this data in its own table because it would be easy to join to the characters data as needed. To do so, we’ll do a little cleaning: removing the rows containing empty strings and renaming titles to title since each row now only contains a single title.\n\ntitles <- chars |> \n  unnest_wider(json) |> \n  select(id, titles) |> \n  unnest_longer(titles) |> \n  filter(titles != \"\") |> \n  rename(title = titles)\ntitles\n#> # A tibble: 53 × 2\n#>      id title                                               \n#>   <int> <chr>                                               \n#> 1  1022 Prince of Winterfell                                \n#> 2  1022 Captain of Sea Bitch                                \n#> 3  1022 Lord of the Iron Islands (by law of the green lands)\n#> 4  1052 Acting Hand of the King (former)                    \n#> 5  1052 Master of Coin (former)                             \n#> 6  1074 Lord Captain of the Iron Fleet                      \n#> # … with 47 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nNow, for example, we could use this table to all the characters that are captains and see all their titles:\n\ncaptains <- titles |> filter(str_detect(title, \"Captain\"))\ncaptains\n#> # A tibble: 5 × 2\n#>      id title                                 \n#>   <int> <chr>                                 \n#> 1  1022 Captain of Sea Bitch                  \n#> 2  1074 Lord Captain of the Iron Fleet        \n#> 3  1166 Captain of the Guard at Sunspear      \n#> 4   150 Captain of the Black Wind             \n#> 5    60 Captain of the Golden Storm (formerly)\n\ncharacters |> \n  semi_join(captains, by = \"id\") |> \n  select(id, name) |> \n  left_join(titles, by = \"id\", multiple = \"all\")\n#> # A tibble: 11 × 3\n#>      id name              title                                               \n#>   <int> <chr>             <chr>                                               \n#> 1  1022 Theon Greyjoy     Prince of Winterfell                                \n#> 2  1022 Theon Greyjoy     Captain of Sea Bitch                                \n#> 3  1022 Theon Greyjoy     Lord of the Iron Islands (by law of the green lands)\n#> 4  1074 Victarion Greyjoy Lord Captain of the Iron Fleet                      \n#> 5  1074 Victarion Greyjoy Master of the Iron Victory                          \n#> 6  1166 Areo Hotah        Captain of the Guard at Sunspear                    \n#> # … with 5 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nYou could imagine creating a table like this for each of the list-columns, then using joins to combine them with the character data as you need it.\n\n25.4.3 A dash of text analysis\nWhat if we wanted to find the most common words in the title? One simple approach starts by using str_split() to break each element of title up into words by spitting on \" \":\n\ntitles |> \n  mutate(word = str_split(title, \" \"), .keep = \"unused\")\n#> # A tibble: 53 × 2\n#>      id word      \n#>   <int> <list>    \n#> 1  1022 <chr [3]> \n#> 2  1022 <chr [4]> \n#> 3  1022 <chr [11]>\n#> 4  1052 <chr [6]> \n#> 5  1052 <chr [4]> \n#> 6  1074 <chr [6]> \n#> # … with 47 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThis creates a unnamed variable length list-column, so we can use unnest_longer():\n\ntitles |> \n  mutate(word = str_split(title, \" \"), .keep = \"unused\") |> \n  unnest_longer(word)\n#> # A tibble: 202 × 2\n#>      id word      \n#>   <int> <chr>     \n#> 1  1022 Prince    \n#> 2  1022 of        \n#> 3  1022 Winterfell\n#> 4  1022 Captain   \n#> 5  1022 of        \n#> 6  1022 Sea       \n#> # … with 196 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nAnd then we can count that column to find the most common:\n\ntitles |> \n  mutate(word = str_split(title, \" \"), .keep = \"unused\") |> \n  unnest_longer(word) |> \n  count(word, sort = TRUE)\n#> # A tibble: 78 × 2\n#>   word        n\n#>   <chr>   <int>\n#> 1 of         41\n#> 2 the        29\n#> 3 Lord        9\n#> 4 Hand        6\n#> 5 Captain     5\n#> 6 King        5\n#> # … with 72 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nSome of those words are not very interesting so we could create a list of common words to drop. In text analysis these is commonly called stop words.\n\nstop_words <- tibble(word = c(\"of\", \"the\"))\n\ntitles |> \n  mutate(word = str_split(title, \" \"), .keep = \"unused\") |> \n  unnest_longer(word) |> \n  anti_join(stop_words) |> \n  count(word, sort = TRUE)\n#> Joining, by = \"word\"\n#> # A tibble: 76 × 2\n#>   word         n\n#>   <chr>    <int>\n#> 1 Lord         9\n#> 2 Hand         6\n#> 3 Captain      5\n#> 4 King         5\n#> 5 Princess     5\n#> 6 Queen        5\n#> # … with 70 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nBreaking up text into individual fragments is a powerful idea that underlies much of text analysis. If this sounds interesting, a good place to learn more is Text Mining with R by Julia Silge and David Robinson.\n\n25.4.4 Deeply nested\nWe’ll finish off these case studies with a list-column that’s very deeply nested and requires repeated rounds of unnest_wider() and unnest_longer() to unravel: gmaps_cities. This is a two column tibble containing five city names and the results of using Google’s geocoding API to determine their location:\n\n# gmaps_cities\n#> # A tibble: 5 × 2\n#>   city       json            \n#>   <chr>      <list>          \n#> 1 Houston    <named list [2]>\n#> 2 Washington <named list [2]>\n#> 3 New York   <named list [2]>\n#> 4 Chicago    <named list [2]>\n#> 5 Arlington  <named list [2]>\n\njson is a list-column with internal names, so we start with an unnest_wider():\n\n# gmaps_cities |> \n#   unnest_wider(json)\n\nThis gives us the status and the results. We’ll drop the status column since they’re all OK; in a real analysis, you’d also want capture all the rows where status != \"OK\" and figure out what went wrong. results is an unnamed list, with either one or two elements (we’ll see why shortly) so we’ll unnest it into rows:\n\n# gmaps_cities |> \n#   unnest_wider(json) |> \n#   select(-status) |> \n#   unnest_longer(results)\n\nNow results is a named list, so we’ll use unnest_wider():\n\n# locations <- gmaps_cities |> \n#   unnest_wider(json) |> \n#   select(-status) |> \n#   unnest_longer(results) |> \n#   unnest_wider(results)\n# locations\n\nNow we can see why two cities got two results: Washington matched both Washington state and Washington, DC, and Arlington matched Arlington, Virginia and Arlington, Texas.\nThere are few different places we could go from here. We might want to determine the exact location of the match, which is stored in the geometry list-column:\n\n# locations |> \n#   select(city, formatted_address, geometry) |> \n#   unnest_wider(geometry)\n\nThat gives us new bounds (a rectangular region) and location (a point). We can unnest location to see the latitude (lat) and longitude (lng):\n\n# locations |> \n#   select(city, formatted_address, geometry) |> \n#   unnest_wider(geometry) |> \n#   unnest_wider(location)\n\nExtracting the bounds requires a few more steps\n\n# locations |> \n#   select(city, formatted_address, geometry) |> \n#   unnest_wider(geometry) |> \n#   # focus on the variables of interest\n#   select(!location:viewport) |>\n#   unnest_wider(bounds)\n\nWe then rename southwest and northeast (the corners of the rectangle) so we can use names_sep to create short but evocative names:"
  },
  {
    "objectID": "rectangling.html#json",
    "href": "rectangling.html#json",
    "title": "25  Data rectangling",
    "section": "\n26.1 JSON",
    "text": "26.1 JSON\nAll of the case studies in the previous section were sourced from wild-caught JSON files. JSON is short for javascript object notation and is the way that most web APIs return data. It’s important to understand it because while JSON and R’s data types are pretty similar, there isn’t a perfect 1-to-1 mapping, so it’s good to understand a bit about JSON if things go wrong.\n\n26.1.1 Data types\nJSON is a simple format designed to be easily read and written by machines, not humans. It has six key data types. Four of them are scalars:\n\nThe simplest type is a null, which is written null, which plays the same role as both NULL and NA in R. It represents the absence of data.\nA string is much like a string in R, but must use double quotes, not single quotes.\nA number is similar to R’s numbers: they can be use integer (e.g. 123), decimal (e.g. 123.45), or scientific (e.g. 1.23e3) notation. JSON doesn’t support Inf, -Inf, or NaN.\nA boolean is similar to R’s TRUE and FALSE, but use lower case true and false.\n\nJSON’s strings, numbers, and booleans are pretty similar to R’s character, numeric, and logical vectors. The main difference is that JSON’s scalars can only represent a single value. To represent multiple values you need to use one of the two remaining two types, arrays and objects.\nBoth arrays and objects are similar to lists in R; the difference is whether or not they’re named. An array is like an unnamed list, and is written with []. For example [1, 2, 3] is an array containing 3 numbers, and [null, 1, \"string\", false] is an array that contains a null, a number, a string, and a boolean. An object is like a named list, and they’re written with {}. For example, {\"x\": 1, \"y\": 2} is an object that maps x to 1 and y to 2.\n\n26.1.2 jsonlite\nTo convert JSON into R data structures, we recommend that you use the jsonlite package, by Jeroen Oooms. We’ll use only two jsonlite functions: read_json() and parse_json(). In real life, you’ll use read_json() to read a JSON file from disk. For example, we the repurrsive package also provides the source for gh_user as a JSON file:\n\n# A path to a json file inside the package:\ngh_users_json()\n#> [1] \"C:/Users/user1/AppData/Local/R/win-library/4.2/repurrrsive/extdata/gh_users.json\"\n\n# Read it with read_json()\ngh_users2 <- read_json(gh_users_json())\n\n# Check it's the same as the data we were using previously\nidentical(gh_users, gh_users2)\n#> [1] TRUE\n\nIn this book, I’ll also use parse_json(), since it takes a string containing JSON, which makes it good for generating simple examples. To get started, here’s three simple JSON datasets, starting with a number, then putting a few number in an array, then putting that array in an object:\n\nstr(parse_json('1'))\n#>  int 1\nstr(parse_json('[1, 2, 3]'))\n#> List of 3\n#>  $ : int 1\n#>  $ : int 2\n#>  $ : int 3\nstr(parse_json('{\"x\": [1, 2, 3]}'))\n#> List of 1\n#>  $ x:List of 3\n#>   ..$ : int 1\n#>   ..$ : int 2\n#>   ..$ : int 3\n\njsonlite has another important function called fromJSON(). We don’t use it here because it performs automatic simplification (simplifyVector = TRUE). This often works well, particularly in simple cases, but we think you’re better off doing the rectangling yourself so you know exactly what’s happening and can more easily handle the most complicated nested structures.\n\n26.1.3 Starting the rectangling process\nIn most cases, JSON files contain a single top-level array, because they’re designed to provide data about multiple “things”, e.g. multiple pages, or multiple records, or multiple results. In this case, you’ll start your rectangling with tibble(json) so that each element becomes a row:\n\njson <- '[\n  {\"name\": \"John\", \"age\": 34},\n  {\"name\": \"Susan\", \"age\": 27}\n]'\ndf <- tibble(json = parse_json(json))\ndf\n#> # A tibble: 2 × 1\n#>   json            \n#>   <list>          \n#> 1 <named list [2]>\n#> 2 <named list [2]>\ndf |> \n  unnest_wider(json)\n#> # A tibble: 2 × 2\n#>   name    age\n#>   <chr> <int>\n#> 1 John     34\n#> 2 Susan    27\n\nIn rarer cases, the JSON consists of a single top-level JSON object, representing one “thing”. In this case, you’ll need to kick off the rectangling process by wrapping it a list, before you put it in a tibble.\n\njson <- '{\n  \"status\": \"OK\", \n  \"results\": [\n    {\"name\": \"John\", \"age\": 34},\n    {\"name\": \"Susan\", \"age\": 27}\n ]\n}\n'\ndf <- tibble(json = list(parse_json(json)))\ndf\n#> # A tibble: 1 × 1\n#>   json            \n#>   <list>          \n#> 1 <named list [2]>\ndf |> \n  unnest_wider(json) |> \n  unnest_longer(results) |> \n  unnest_wider(results)\n#> # A tibble: 2 × 3\n#>   status name    age\n#>   <chr>  <chr> <int>\n#> 1 OK     John     34\n#> 2 OK     Susan    27\n\nAlternatively, you can reach inside the parsed JSON and start with the bit that you actually care about:\n\ndf <- tibble(results = parse_json(json)$results)\ndf |> \n  unnest_wider(results)\n#> # A tibble: 2 × 2\n#>   name    age\n#>   <chr> <int>\n#> 1 John     34\n#> 2 Susan    27\n\n\n26.1.4 Translation challenges\nSince JSON doesn’t have any way to represent dates or date-times, they’re often stored as ISO8601 date times in strings, and you’ll need to use readr::parse_date() or readr::parse_datetime() to turn them into the correct data structure. Similarly, JSON’s rules for representing floating point numbers in JSON are a little imprecise, so you’ll also sometimes find numbers stored in strings. Apply readr::parse_double() as needed to the get correct variable type.\n\n26.1.5 Exercises\n\n\nRectangle the df_col and df_row below. They represent the two ways of encoding a data frame in JSON.\n\njson_col <- parse_json('\n  {\n    \"x\": [\"a\", \"x\", \"z\"],\n    \"y\": [10, null, 3]\n  }\n')\njson_row <- parse_json('\n  [\n    {\"x\": \"a\", \"y\": 10},\n    {\"x\": \"x\", \"y\": null},\n    {\"x\": \"z\", \"y\": 3}\n  ]\n')\n\ndf_col <- tibble(json = list(json_col)) \ndf_row <- tibble(json = json_row)"
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "26  Web scraping",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "In this part of the book, you’ll improve your programming skills. Programming is a cross-cutting skill needed for all data science work: you must use a computer to do data science; you cannot do it in your head, or with pencil and paper.\nProgramming produces code, and code is a tool of communication. Obviously code tells the computer what you want it to do. But it also communicates meaning to other humans. Thinking about code as a vehicle for communication is important because every project you do is fundamentally collaborative. Even if you’re not working with other people, you’ll definitely be working with future-you! Writing clear code is important so that others (like future-you) can understand why you tackled an analysis in the way you did. That means getting better at programming also involves getting better at communicating. Over time, you want your code to become not just easier to write, but easier for others to read.\nWriting code is similar in many ways to writing prose. One parallel which we find particularly useful is that in both cases rewriting is the key to clarity. The first expression of your ideas is unlikely to be particularly clear, and you may need to rewrite multiple times. After solving a data analysis challenge, it’s often worth looking at your code and thinking about whether or not it’s obvious what you’ve done. If you spend a little time rewriting your code while the ideas are fresh, you can save a lot of time later trying to recreate what your code did. But this doesn’t mean you should rewrite every function: you need to balance what you need to achieve now with saving time in the long run. (But the more you rewrite your functions the more likely your first attempt will be clear.)\nIn the following four chapters, you’ll learn skills that will allow you to both tackle new programs and to solve existing problems with greater clarity and ease:"
  },
  {
    "objectID": "program.html#learning-more",
    "href": "program.html#learning-more",
    "title": "Program",
    "section": "Learning more",
    "text": "Learning more\nThe goal of these chapters is to teach you the minimum about programming that you need to practice data science, which turns out to be a reasonable amount. Once you have mastered the material in this book, we strongly believe you should invest further in your programming skills. Learning more about programming is a long-term investment: it won’t pay off immediately, but in the long term it will allow you to solve new problems more quickly, and let you reuse your insights from previous problems in new scenarios.\nTo learn more you need to study R as a programming language, not just an interactive environment for data science. We have written two books that will help you do so:\n\nHands on Programming with R, by Garrett Grolemund. This is an introduction to R as a programming language and is a great place to start if R is your first programming language. It covers similar material to these chapters, but with a different style and different motivation examples (based in the casino). It’s a useful complement if you find that these four chapters go by too quickly.\nAdvanced R by Hadley Wickham. This dives into the details of R the programming language. This is a great place to start if you have existing programming experience. It’s also a great next step once you’ve internalised the ideas in these chapters."
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "27  Functions",
    "section": "",
    "text": "One of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nWriting good functions is a lifetime journey. Even after using R for many years we still learn new techniques and better ways of approaching old problems. The goal of this chapter is not to teach you every esoteric detail of functions but to get you started with some pragmatic advice that you can apply immediately.\nAs well as practical advice for writing functions, this chapter also gives you some suggestions for how to style your code. Good code style is like correct punctuation. Youcanmanagewithoutit, but it sure makes things easier to read! As with styles of punctuation, there are many possible variations. Here we present the style we use in our code, but the most important thing is to be consistent.\n\nThe focus of this chapter is on writing functions in base R, so you won’t need any extra packages."
  },
  {
    "objectID": "functions.html#when-should-you-write-a-function",
    "href": "functions.html#when-should-you-write-a-function",
    "title": "27  Functions",
    "section": "\n27.2 When should you write a function?",
    "text": "27.2 When should you write a function?\nYou should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). For example, take a look at this code. What does it do?\n\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\nYou might be able to puzzle out that this rescales each column to have a range from 0 to 1. But did you spot the mistake? Hadley made an error when copying-and-pasting the code for df$b: he forgot to change an a to a b. Extracting repeated code out into a function is a good idea because it prevents you from making this type of mistake.\nTo write a function you need to first analyse the code. How many inputs does it have?\n\n(df$a - min(df$a, na.rm = TRUE)) /\n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code only has one input: df$a. (If you’re surprised that TRUE is not an input, you can explore why in the exercise below.) To make the inputs more clear, it’s a good idea to rewrite the code using temporary variables with general names. Here this code only requires a single numeric vector, so we’ll call it x:\n\nx <- df$a\n(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n#>  [1] 0.2892677 0.7509271 0.0000000 0.6781686 0.8530656 1.0000000 0.1716402\n#>  [8] 0.6107464 0.6116181 0.6008793\n\nThere is some duplication in this code. We’re computing the range of the data three times, so it makes sense to do it in one step:\n\nrng <- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n#>  [1] 0.2892677 0.7509271 0.0000000 0.6781686 0.8530656 1.0000000 0.1716402\n#>  [8] 0.6107464 0.6116181 0.6008793\n\nPulling out intermediate calculations into named variables is a good practice because it makes it more clear what the code is doing. Now that we’ve simplified the code, and checked that it still works, we can turn it into a function:\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(c(0, 5, 10))\n#> [1] 0.0 0.5 1.0\n\nThere are three key steps to creating a new function:\n\nYou need to pick a name for the function. Here we used rescale01 because this function rescales a vector to lie between 0 and 1.\nYou list the inputs, or arguments, to the function inside function. Here we have just one argument. If we had more the call would look like function(x, y, z).\nYou place the code you have developed in the body of the function, a { block that immediately follows function(...).\n\nNote the overall process: we only made the function after we’d figured out how to make it work with a simple input. It’s easier to start with working code and turn it into a function; it’s harder to create a function and then try to make it work.\nAt this point it’s a good idea to check your function with a few different inputs:\n\nrescale01(c(-10, 0, 10))\n#> [1] 0.0 0.5 1.0\nrescale01(c(1, 2, 3, NA, 5))\n#> [1] 0.00 0.25 0.50   NA 1.00\n\nAs you write more and more functions you’ll eventually want to convert these informal, interactive tests into formal, automated tests. That process is called unit testing. Unfortunately, it’s beyond the scope of this book, but you can learn about it in http://r-pkgs.had.co.nz/tests.html.\nWe can simplify the original example now that we have a function:\n\ndf$a <- rescale01(df$a)\ndf$b <- rescale01(df$b)\ndf$c <- rescale01(df$c)\ndf$d <- rescale01(df$d)\n\nCompared to the original, this code is easier to understand and we’ve eliminated one class of copy-and-paste errors. There is still quite a bit of duplication since we’re doing the same thing to multiple columns. We’ll learn how to eliminate that duplication with iteration in Chapter 29, once you’ve learned more about R’s data structures in Chapter 28.\nAnother advantage of functions is that if our requirements change, we only need to make the change in one place. For example, we might discover that some of our variables include infinite values, and rescale01() fails:\n\nx <- c(1:10, Inf)\nrescale01(x)\n#>  [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nBecause we’ve extracted the code into a function, we only need to make the fix in one place:\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(x)\n#>  [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n#>  [8] 0.7777778 0.8888889 1.0000000       Inf\n\nThis is an important part of the “do not repeat yourself” (or DRY) principle. The more repetition you have in your code, the more places you need to remember to update when things change (and they always do!), and the more likely you are to create bugs over time.\n\n27.2.1 Exercises\n\nWhy is TRUE not a parameter to rescale01()? What would happen if x contained a single missing value, and na.rm was FALSE?\nIn the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1.\n\nPractice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative?\n\nmean(is.na(x))\n\nx / sum(x, na.rm = TRUE)\n\nsd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)\n\n\nWrite your own functions to compute the variance and skewness of a numeric vector. Variance is defined as \\[\n\\mathrm{Var}(x) = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x}) ^2 \\text{,}\n\\] where \\(\\bar{x} = (\\sum_i^n x_i) / n\\) is the sample mean. Skewness is defined as \\[\n\\mathrm{Skew}(x) = \\frac{\\frac{1}{n-2}\\left(\\sum_{i=1}^n(x_i - \\bar x)^3\\right)}{\\mathrm{Var}(x)^{3/2}} \\text{.}\n\\]\nWrite both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.\n\nWhat do the following functions do? Why are they useful even though they are so short?\n\nis_directory <- function(x) file.info(x)$isdir\nis_readable <- function(x) file.access(x, 4) == 0\n\n\nRead the complete lyrics to “Little Bunny Foo Foo”. There’s a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication."
  },
  {
    "objectID": "functions.html#functions-are-for-humans-and-computers",
    "href": "functions.html#functions-are-for-humans-and-computers",
    "title": "27  Functions",
    "section": "\n27.3 Functions are for humans and computers",
    "text": "27.3 Functions are for humans and computers\nIt’s important to remember that functions are not just for the computer, but are also for humans. R doesn’t care what your function is called, or what comments it contains, but these are important for human readers. This section discusses some things that you should bear in mind when writing functions that humans can understand.\nThe name of a function is important. Ideally, the name of your function will be short, but clearly evoke what the function does. That’s hard! But it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names.\nGenerally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()). A good sign that a noun might be a better choice is if you’re using a very broad verb like “get”, “compute”, “calculate”, or “determine”. Use your best judgement and don’t be afraid to rename a function if you figure out a better name later.\n\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\ncollapse_years()\n\nIf your function name is composed of multiple words, we recommend using “snake_case”, where each lowercase word is separated by an underscore. camelCase is a popular alternative. It doesn’t really matter which one you pick, the important thing is to be consistent: pick one or the other and stick with it. R itself is not very consistent, but there’s nothing you can do about that. Make sure you don’t fall into the same trap by making your code as consistent as possible.\n\n# Never do this!\ncol_mins <- function(x, y) {}\nrowMaxes <- function(y, x) {}\n\nIf you have a family of functions that do similar things, make sure they have consistent names and arguments. Use a common prefix to indicate that they are connected. That’s better than a common suffix because autocomplete allows you to type the prefix and see all the members of the family.\n\n# Good\ninput_select()\ninput_checkbox()\ninput_text()\n\n# Not so good\nselect_input()\ncheckbox_input()\ntext_input()\n\nA good example of this design is the stringr package: if you don’t remember exactly which function you need, you can type str_ and jog your memory.\nWhere possible, avoid overriding existing functions and variables. It’s impossible to do in general because so many good names are already taken by other packages, but avoiding the most common names from base R will avoid confusion.\n\n# Don't do this!\nT <- FALSE\nc <- 10\nmean <- function(x) sum(x)\n\nUse comments, lines starting with #, to explain the “why” of your code. You generally should avoid comments that explain the “what” or the “how”. If you can’t understand what the code does from reading it, you should think about how to rewrite it to be more clear. Do you need to add some intermediate variables with useful names? Do you need to break out a subcomponent of a large function so you can name it? However, your code can never capture the reasoning behind your decisions: why did you choose this approach instead of an alternative? What else did you try that didn’t work? It’s a great idea to capture that sort of thinking in a comment.\n\n27.3.1 Exercises\n\n\nRead the source code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\nf1 <- function(string, prefix) {\n  substr(string, 1, nchar(prefix)) == prefix\n}\nf2 <- function(x) {\n  if (length(x) <= 1) return(NULL)\n  x[-length(x)]\n}\nf3 <- function(x, y) {\n  rep(y, length.out = length(x))\n}\n\n\nTake a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments.\nCompare and contrast rnorm() and MASS::mvrnorm(). How could you make them more consistent?\nMake a case for why norm_r(), norm_d() etc would be better than rnorm(), dnorm(). Make a case for the opposite."
  },
  {
    "objectID": "functions.html#sec-conditional-execution",
    "href": "functions.html#sec-conditional-execution",
    "title": "27  Functions",
    "section": "\n27.4 Conditional execution",
    "text": "27.4 Conditional execution\nAn if statement allows you to conditionally execute code. It looks like this:\n\nif (condition) {\n  # code executed when condition is TRUE\n} else {\n  # code executed when condition is FALSE\n}\n\nTo get help on if you need to surround it in backticks: ?`if`. The help isn’t particularly helpful if you’re not already an experienced programmer, but at least you know how to get to it!\nHere’s a simple function that uses an if statement. The goal of this function is to return a logical vector describing whether or not each element of a vector is named.\n\nhas_name <- function(x) {\n  nms <- names(x)\n  if (is.null(nms)) {\n    rep(FALSE, length(x))\n  } else {\n    !is.na(nms) & nms != \"\"\n  }\n}\n\nThis function takes advantage of the standard return rule: a function returns the last value that it computed. Here that is either one of the two branches of the if statement.\n\n27.4.1 Conditions\nThe condition must evaluate to either TRUE or FALSE. If it’s a vector, you’ll get a warning message; if it’s an NA, you’ll get an error. Watch out for these messages in your own code:\n\nif (c(TRUE, FALSE)) {}\n#> Error in if (c(TRUE, FALSE)) {: the condition has length > 1\n\nif (NA) {}\n#> Error in if (NA) {: missing value where TRUE/FALSE needed\n\nYou can use || (or) and && (and) to combine multiple logical expressions. These operators are “short-circuiting”: as soon as || sees the first TRUE it returns TRUE without computing anything else. As soon as && sees the first FALSE it returns FALSE. You should never use | or & in an if statement: these are vectorised operations that apply to multiple values (that’s why you use them in filter()). If you do have a logical vector, you can use any() or all() to collapse it to a single value.\nBe careful when testing for equality. == is vectorised, which means that it’s easy to get more than one output. Either check the length is already 1, collapse with all() or any(), or use the non-vectorised identical(). identical() is very strict: it always returns either a single TRUE or a single FALSE, and doesn’t coerce types. This means that you need to be careful when comparing integers and doubles:\n\nidentical(0L, 0)\n#> [1] FALSE\n\nYou also need to be wary of floating point numbers:\n\nx <- sqrt(2) ^ 2\nx\n#> [1] 2\nx == 2\n#> [1] FALSE\nx - 2\n#> [1] 4.440892e-16\n\nInstead use dplyr::near() for comparisons, as described in [comparisons].\nAnd remember, x == NA doesn’t do anything useful!\n\n27.4.2 Multiple conditions\nYou can chain multiple if statements together:\n\nif (this) {\n  # do that\n} else if (that) {\n  # do something else\n} else {\n  # \n}\n\nBut if you end up with a very long series of chained if statements, you should consider rewriting. One useful technique is the switch() function. It allows you to evaluate selected code based on position or name.\n\n#> function(x, y, op) {\n#>   switch(op,\n#>     plus = x + y,\n#>     minus = x - y,\n#>     times = x * y,\n#>     divide = x / y,\n#>     stop(\"Unknown op!\")\n#>   )\n#> }\n\nAnother useful function that can often eliminate long chains of if statements is cut(). It’s used to discretise continuous variables.\n\n27.4.3 Code style\nBoth if and function should (almost) always be followed by squiggly brackets ({}), and the contents should be indented by two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin.\nAn opening curly brace should never go on its own line and should always be followed by a new line. A closing curly brace should always go on its own line, unless it’s followed by else. Always indent the code inside curly braces.\n\n# Good\nif (y < 0 && debug) {\n  message(\"Y is negative\")\n}\n\nif (y == 0) {\n  log(x)\n} else {\n  y ^ x\n}\n\n# Bad\nif (y < 0 && debug)\nmessage(\"Y is negative\")\n\nif (y == 0) {\n  log(x)\n} \nelse {\n  y ^ x\n}\n\nIt’s ok to drop the curly braces if you have a very short if statement that can fit on one line:\n\ny <- 10\nx <- if (y < 20) \"Too low\" else \"Too high\"\n\nWe recommend this only for very brief if statements. Otherwise, the full form is easier to read:\n\nif (y < 20) {\n  x <- \"Too low\" \n} else {\n  x <- \"Too high\"\n}\n\n\n27.4.4 Exercises\n\nWhat’s the difference between if and ifelse()? Carefully read the help and construct three examples that illustrate the key differences.\nWrite a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.)\nImplement a fizzbuzz function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function.\n\nHow could you use cut() to simplify this set of nested if-else statements?\n\nif (temp <= 0) {\n  \"freezing\"\n} else if (temp <= 10) {\n  \"cold\"\n} else if (temp <= 20) {\n  \"cool\"\n} else if (temp <= 30) {\n  \"warm\"\n} else {\n  \"hot\"\n}\n\nHow would you change the call to cut() if we used < instead of <=? What is the other chief advantage of cut() for this problem? (Hint: what happens if you have many values in temp?)\n\nWhat happens if you use switch() with numeric values?\n\nWhat does this switch() call do? What happens if x is “e”?\n\nswitch(x, \n  a = ,\n  b = \"ab\",\n  c = ,\n  d = \"cd\"\n)\n\nExperiment, then carefully read the documentation."
  },
  {
    "objectID": "functions.html#function-arguments",
    "href": "functions.html#function-arguments",
    "title": "27  Functions",
    "section": "\n27.5 Function arguments",
    "text": "27.5 Function arguments\nThe arguments to a function typically fall into two broad sets: one set supplies the data to compute on, and the other supplies arguments that control the details of the computation. For example:\n\nIn log(), the data is x, and the detail is the base of the logarithm.\nIn mean(), the data is x, and the details are how much data to trim from the ends (trim) and how to handle missing values (na.rm).\nIn t.test(), the data are x and y, and the details of the test are alternative, mu, paired, var.equal, and conf.level.\nIn str_c() you can supply any number of strings to ..., and the details of the concatenation are controlled by sep and collapse.\n\nGenerally, data arguments should come first. Detail arguments should go on the end, and usually should have default values. You specify a default value in the same way you call a function with a named argument:\n\n# Compute confidence interval around mean using normal approximation\nmean_ci <- function(x, conf = 0.95) {\n  se <- sd(x) / sqrt(length(x))\n  alpha <- 1 - conf\n  mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2))\n}\n\nx <- runif(100)\nmean_ci(x)\n#> [1] 0.4976111 0.6099594\nmean_ci(x, conf = 0.99)\n#> [1] 0.4799599 0.6276105\n\nThe default value should almost always be the most common value. The few exceptions to this rule are to do with safety. For example, it makes sense for na.rm to default to FALSE because missing values are important. Even though na.rm = TRUE is what you usually put in your code, it’s a bad idea to silently ignore missing values by default.\nWhen you call a function, you typically omit the names of the data arguments, because they are used so commonly. If you override the default value of a detail argument, you should use the full name:\n\n# Good\nmean(1:10, na.rm = TRUE)\n\n# Bad\nmean(x = 1:10, , FALSE)\nmean(, TRUE, x = c(1:10, NA))\n\nYou can refer to an argument by its unique prefix (e.g. mean(x, n = TRUE)), but this is generally best avoided given the possibilities for confusion.\nNotice that when you call a function, you should place a space around = in function calls, and always put a space after a comma, not before (just like in regular English). Using whitespace makes it easier to skim the function for the important components.\n\n# Good\naverage <- mean(feet / 12 + inches, na.rm = TRUE)\n\n# Bad\naverage<-mean(feet/12+inches,na.rm=TRUE)\n\n\n27.5.1 Choosing names\nThe names of the arguments are also important. R doesn’t care, but the readers of your code (including future-you!) will. Generally you should prefer longer, more descriptive names, but there are a handful of very common, very short names. It’s worth memorising these:\n\n\nx, y, z: vectors.\n\nw: a vector of weights.\n\ndf: a data frame.\n\ni, j: numeric indices (typically rows and columns).\n\nn: length, or number of rows.\n\np: number of columns.\n\nOtherwise, consider matching names of arguments in existing R functions. For example, use na.rm to determine if missing values should be removed.\n\n27.5.2 Checking values\nAs you start to write more functions, you’ll eventually get to the point where you don’t remember exactly how your function works. At this point it’s easy to call your function with invalid inputs. To avoid this problem, it’s often useful to make constraints explicit. For example, imagine you’ve written some functions for computing weighted summary statistics:\n\nwt_mean <- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_var <- function(x, w) {\n  mu <- wt_mean(x, w)\n  sum(w * (x - mu) ^ 2) / sum(w)\n}\nwt_sd <- function(x, w) {\n  sqrt(wt_var(x, w))\n}\n\nWhat happens if x and w are not the same length?\n\nwt_mean(1:6, 1:3)\n#> [1] 7.666667\n\nIn this case, because of R’s vector recycling rules, we don’t get an error.\nIt’s good practice to check important preconditions, and throw an error (with stop()), if they are not true:\n\nwt_mean <- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  }\n  sum(w * x) / sum(w)\n}\n\nBe careful not to take this too far. There’s a tradeoff between how much time you spend making your function robust, versus how long you spend writing it. For example, if you also added a na.rm argument, you don’t need to check it carefully:\n\nwt_mean <- function(x, w, na.rm = FALSE) {\n  if (!is.logical(na.rm)) {\n    stop(\"`na.rm` must be logical\")\n  }\n  if (length(na.rm) != 1) {\n    stop(\"`na.rm` must be length 1\")\n  }\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  }\n  \n  if (na.rm) {\n    miss <- is.na(x) | is.na(w)\n    x <- x[!miss]\n    w <- w[!miss]\n  }\n  sum(w * x) / sum(w)\n}\n\nThis is a lot of extra work for little additional gain. A useful compromise is the built-in stopifnot(): it checks that each argument is TRUE, and produces a generic error message if not.\n\nwt_mean <- function(x, w, na.rm = FALSE) {\n  stopifnot(is.logical(na.rm), length(na.rm) == 1)\n  stopifnot(length(x) == length(w))\n  \n  if (na.rm) {\n    miss <- is.na(x) | is.na(w)\n    x <- x[!miss]\n    w <- w[!miss]\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean(1:6, 6:1, na.rm = \"foo\")\n#> Error in wt_mean(1:6, 6:1, na.rm = \"foo\"): is.logical(na.rm) is not TRUE\n\nNote that when using stopifnot() you assert what should be true rather than checking for what might be wrong.\n\n27.5.3 Dot-dot-dot (…)\nMany functions in R take an arbitrary number of inputs:\n\nsum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n#> [1] 55\nstringr::str_c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\n#> [1] \"abcdef\"\n\nHow do these functions work? They rely on a special argument: ... (pronounced dot-dot-dot). This special argument captures any number of arguments that aren’t otherwise matched.\nIt’s useful because you can then send those ... on to another function. This is a useful catch-all if your function primarily wraps another function. For example, Hadley often create’s these helper functions that wrap around str_c():\n\ncommas <- function(...) stringr::str_c(..., collapse = \", \")\ncommas(letters[1:10])\n#> [1] \"a, b, c, d, e, f, g, h, i, j\"\n\nrule <- function(..., pad = \"-\") {\n  title <- paste0(...)\n  width <- getOption(\"width\") - nchar(title) - 5\n  cat(title, \" \", stringr::str_dup(pad, width), \"\\n\", sep = \"\")\n}\nrule(\"Important output\")\n#> Important output -----------------------------------------------------------\n\nHere ... lets you forward on any extra arguments to str_c(). It’s a very convenient technique. But it does come at a price: any misspelled arguments will not raise an error. This makes it easy for typos to go unnoticed:\n\nx <- c(1, 2)\nsum(x, na.mr = TRUE)\n#> [1] 4\n\nIf you just want to capture the values of the ..., use list(...).\n\n27.5.4 Lazy evaluation\nArguments in R are lazily evaluated: they’re not computed until they’re needed. That means if they’re never used, they’re never called. This is an important property of R as a programming language, but is generally not important when you’re writing your own functions for data analysis. You can read more about lazy evaluation at http://adv-r.had.co.nz/Functions.html#lazy-evaluation.\n\n27.5.5 Exercises\n\nWhat does commas(letters, collapse = \"-\") do? Why?\nIt’d be nice if you could supply multiple characters to the pad argument, e.g. rule(\"Title\", pad = \"-+\"). Why doesn’t this currently work? How could you fix it?\nWhat does the trim argument to mean() do? When might you use it?\nThe default value for the method argument to cor() is c(\"pearson\", \"kendall\", \"spearman\"). What does that mean? What value is used by default?"
  },
  {
    "objectID": "functions.html#return-values",
    "href": "functions.html#return-values",
    "title": "27  Functions",
    "section": "\n27.6 Return values",
    "text": "27.6 Return values\nFiguring out what your function should return is usually straightforward: it’s why you created the function in the first place! There are two things you should consider when returning a value:\n\nDoes returning early make your function easier to read?\nCan you make your function pipeable?\n\n\n27.6.1 Explicit return statements\nThe value returned by the function is usually the last statement it evaluates, but you can choose to return early by using return(). We think it’s best to save the use of return() to signal that you can return early with a simpler solution. A common reason to do this is because the inputs are empty:\n\ncomplicated_function <- function(x, y, z) {\n  if (length(x) == 0 || length(y) == 0) {\n    return(0)\n  }\n    \n  # Complicated code here\n}\n\nAnother reason is because you have a if statement with one complex block and one simple block. For example, you might write an if statement like this:\n\nf <- function() {\n  if (x) {\n    # Do \n    # something\n    # that\n    # takes\n    # many\n    # lines\n    # to\n    # express\n  } else {\n    # return something short\n  }\n}\n\nBut if the first block is very long, by the time you get to the else, you’ve forgotten the condition. One way to rewrite it is to use an early return for the simple case:\n\nf <- function() {\n  if (!x) {\n    return(something_short)\n  }\n\n  # Do \n  # something\n  # that\n  # takes\n  # many\n  # lines\n  # to\n  # express\n}\n\nThis tends to make the code easier to understand, because you don’t need quite so much context to understand it.\n\n27.6.2 Writing pipeable functions\nIf you want to write your own pipeable functions, it’s important to think about the return value. Knowing the return value’s object type will mean that your pipeline will “just work”. For example, with dplyr and tidyr the object type is the data frame.\nThere are two basic types of pipeable functions: transformations and side-effects. With transformations, an object is passed to the function’s first argument and a modified object is returned. With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline. For example, this simple function prints the number of missing values in a data frame:\n\nshow_missings <- function(df) {\n  n <- sum(is.na(df))\n  cat(\"Missing values: \", n, \"\\n\", sep = \"\")\n  \n  invisible(df)\n}\n\nIf we call it interactively, the invisible() means that the input df doesn’t get printed out:\n\nshow_missings(mtcars)\n#> Missing values: 0\n\nBut it’s still there, it’s just not printed by default:\n\nx <- show_missings(mtcars) \n#> Missing values: 0\nclass(x)\n#> [1] \"data.frame\"\ndim(x)\n#> [1] 32 11\n\nAnd we can still use it in a pipe:\n\nmtcars |> \n  show_missings() |> \n  mutate(mpg = ifelse(mpg < 20, NA, mpg)) |> \n  show_missings() \n#> Missing values: 0\n#> Missing values: 18"
  },
  {
    "objectID": "functions.html#environment",
    "href": "functions.html#environment",
    "title": "27  Functions",
    "section": "\n27.7 Environment",
    "text": "27.7 Environment\nThe last component of a function is its environment. This is not something you need to understand deeply when you first start writing functions. However, it’s important to know a little bit about environments because they are crucial to how functions work. The environment of a function controls how R finds the value associated with a name. For example, take this function:\n\nf <- function(x) {\n  x + y\n} \n\nIn many programming languages, this would be an error, because y is not defined inside the function. In R, this is valid code because R uses rules called lexical scoping to find the value associated with a name. Since y is not defined inside the function, R will look in the environment where the function was defined:\n\ny <- 100\nf(10)\n#> [1] 110\n\ny <- 1000\nf(10)\n#> [1] 1010\n\nThis behaviour seems like a recipe for bugs, and indeed you should avoid creating functions like this deliberately, but by and large it doesn’t cause too many problems (especially if you regularly restart R to get to a clean slate).\nThe advantage of this behaviour is that from a language standpoint it allows R to be very consistent. Every name is looked up using the same set of rules. For f() that includes the behaviour of two things that you might not expect: { and +. This allows you to do devious things like:\n\n`+` <- function(x, y) {\n  if (runif(1) < 0.1) {\n    sum(x, y)\n  } else {\n    sum(x, y) * 1.1\n  }\n}\ntable(replicate(1000, 1 + 2))\n#> \n#>   3 3.3 \n#> 100 900\nrm(`+`)\n\nThis is a common phenomenon in R. R places few limits on your power. You can do many things that you can’t do in other programming languages. You can do many things that 99% of the time are extremely ill-advised (like overriding how addition works!). But this power and flexibility is what makes tools like ggplot2 and dplyr possible. Learning how to make best use of this flexibility is beyond the scope of this book, but you can read about it in Advanced R."
  },
  {
    "objectID": "vectors.html",
    "href": "vectors.html",
    "title": "28  Vectors",
    "section": "",
    "text": "So far this book has focussed on tibbles and packages that work with them. But as you start to write your own functions, and dig deeper into R, you need to learn about vectors, the objects that underlie tibbles. If you’ve learned R in a more traditional way, you’re probably already familiar with vectors, as most R resources start with vectors and work their way up to tibbles. We think it’s better to start with tibbles because they’re immediately useful, and then work your way down to the underlying components.\n\nThe focus of this chapter is on base R data structures, so it isn’t essential to load any packages. We will, however, use a handful of functions from the purrr package to avoid some inconsistencies in base R.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "vectors.html#vector-basics",
    "href": "vectors.html#vector-basics",
    "title": "28  Vectors",
    "section": "\n28.2 Vector basics",
    "text": "28.2 Vector basics\nThere are two types of vectors:\n\nAtomic vectors, of which there are six types: logical, integer, double, character, complex, and raw. Integer and double vectors are collectively known as numeric vectors.\nLists, which are sometimes called recursive vectors because lists can contain other lists.\n\nThe chief difference between atomic vectors and lists is that atomic vectors are homogeneous, while lists can be heterogeneous. There’s one other related object: NULL. NULL is often used to represent the absence of a vector (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0. Figure 28.1 summarises the interrelationships.\n\n\n\n\nFigure 28.1: The hierarchy of R’s vector types.\n\n\n\n\nEvery vector has two key properties:\n\n\nIts type, which you can determine with typeof().\n\ntypeof(letters)\n#> [1] \"character\"\ntypeof(1:10)\n#> [1] \"integer\"\n\n\n\nIts length, which you can determine with length().\n\nx <- list(\"a\", \"b\", 1:10)\nlength(x)\n#> [1] 3\n\n\n\nVectors can also contain arbitrary additional metadata in the form of attributes. These attributes are used to create augmented vectors which build on additional behaviour. There are three important types of augmented vector:\n\nFactors are built on top of integer vectors.\nDates and date-times are built on top of numeric vectors.\nData frames and tibbles are built on top of lists.\n\nThis chapter will introduce you to these important vectors from simplest to most complicated. You’ll start with atomic vectors, then build up to lists, and finish off with augmented vectors."
  },
  {
    "objectID": "vectors.html#important-types-of-atomic-vector",
    "href": "vectors.html#important-types-of-atomic-vector",
    "title": "28  Vectors",
    "section": "\n28.3 Important types of atomic vector",
    "text": "28.3 Important types of atomic vector\nThe four most important types of atomic vector are logical, integer, double, and character. Raw and complex are rarely used during a data analysis, so we won’t discuss them here.\n\n28.3.1 Logical\nLogical vectors are the simplest type of atomic vector because they can take only three possible values: FALSE, TRUE, and NA. Logical vectors are usually constructed with comparison operators, as described in [comparisons]. You can also create them by hand with c():\n\n1:10 %% 3 == 0\n#>  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\nc(TRUE, TRUE, FALSE, NA)\n#> [1]  TRUE  TRUE FALSE    NA\n\n\n28.3.2 Numeric\nInteger and double vectors are known collectively as numeric vectors. In R, numbers are doubles by default. To make an integer, place an L after the number:\n\ntypeof(1)\n#> [1] \"double\"\ntypeof(1L)\n#> [1] \"integer\"\n1.5L\n#> [1] 1.5\n\nThe distinction between integers and doubles is not usually important, but there are two important differences that you should be aware of:\n\n\nDoubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory. This means that you should consider all doubles to be approximations. For example, what is square of the square root of two?\n\nx <- sqrt(2) ^ 2\nx\n#> [1] 2\nx - 2\n#> [1] 4.440892e-16\n\nThis behaviour is common when working with floating point numbers: most calculations include some approximation error. Instead of comparing floating point numbers using ==, you should use dplyr::near() which allows for some numerical tolerance.\n\n\nIntegers have one special value: NA, while doubles have four: NA, NaN, Inf and -Inf. All three special values NaN, Inf and -Inf can arise during division:\n\nc(-1, 0, 1) / 0\n#> [1] -Inf  NaN  Inf\n\nAvoid using == to check for these other special values. Instead use the helper functions is.finite(), is.infinite(), and is.nan():\n\n\n\n0\nInf\nNA\nNaN\n\n\n\nis.finite()\nx\n\n\n\n\n\nis.infinite()\n\nx\n\n\n\n\nis.na()\n\n\nx\nx\n\n\nis.nan()\n\n\n\nx\n\n\n\n\n\n28.3.3 Character\nCharacter vectors are the most complex type of atomic vector, because each element of a character vector is a string, and a string can contain an arbitrary amount of data.\nYou’ve already learned a lot about working with strings in [strings]. Here we wanted to mention one important feature of the underlying string implementation: R uses a global string pool. This means that each unique string is only stored in memory once, and every use of the string points to that representation. This reduces the amount of memory needed by duplicated strings. You can see this behaviour in practice with lobstr::obj_size():\n\nx <- \"This is a reasonably long string.\"\nlobstr::obj_size(x)\n#> 152 B\n\ny <- rep(x, 1000)\nlobstr::obj_size(y)\n#> 8.14 kB\n\ny doesn’t take up 1,000x as much memory as x, because each element of y is just a pointer to that same string. A pointer is 8 bytes, so 1000 pointers to a 152 B string is 8 * 1000 + 152 = 8,144 B.\n\n28.3.4 Missing values\nNote that each type of atomic vector has its own missing value:\n\nNA            # logical\n#> [1] NA\nNA_integer_   # integer\n#> [1] NA\nNA_real_      # double\n#> [1] NA\nNA_character_ # character\n#> [1] NA\n\nNormally you don’t need to know about these different types because you can always use NA and it will be converted to the correct type using the implicit coercion rules described next. However, there are some functions that are strict about their inputs, so it’s useful to have this knowledge sitting in your back pocket so you can be specific when needed.\n\n28.3.5 Exercises\n\nDescribe the difference between is.finite(x) and !is.infinite(x).\nRead the source code for dplyr::near() (Hint: to see the source code, drop the ()). How does it work?\nA logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research.\nBrainstorm at least four functions that allow you to convert a double to an integer. How do they differ? Be precise.\nWhat functions from the readr package allow you to turn a string into logical, integer, and double vector?"
  },
  {
    "objectID": "vectors.html#using-atomic-vectors",
    "href": "vectors.html#using-atomic-vectors",
    "title": "28  Vectors",
    "section": "\n28.4 Using atomic vectors",
    "text": "28.4 Using atomic vectors\nNow that you understand the different types of atomic vector, it’s useful to review some of the important tools for working with them. These include:\n\nHow to convert from one type to another, and when that happens automatically.\nHow to tell if an object is a specific type of vector.\nWhat happens when you work with vectors of different lengths.\nHow to name the elements of a vector.\nHow to pull out elements of interest.\n\n\n28.4.1 Coercion\nThere are two ways to convert, or coerce, one type of vector to another:\n\nExplicit coercion happens when you call a function like as.logical(), as.integer(), as.double(), or as.character(). Whenever you find yourself using explicit coercion, you should always check whether you can make the fix upstream, so that the vector never had the wrong type in the first place. For example, you may need to tweak your readr col_types specification.\nImplicit coercion happens when you use a vector in a specific context that expects a certain type of vector. For example, when you use a logical vector with a numeric summary function, or when you use a double vector where an integer vector is expected.\n\nBecause explicit coercion is used relatively rarely, and is largely easy to understand, we’ll focus on implicit coercion here.\nYou’ve already seen the most important type of implicit coercion: using a logical vector in a numeric context. In this case TRUE is converted to 1 and FALSE converted to 0. That means the sum of a logical vector is the number of trues, and the mean of a logical vector is the proportion of trues:\n\nx <- sample(20, 100, replace = TRUE)\ny <- x > 10\nsum(y)  # how many are greater than 10?\n#> [1] 38\nmean(y) # what proportion are greater than 10?\n#> [1] 0.38\n\nYou may see some code (typically older) that relies on implicit coercion in the opposite direction, from integer to logical:\n\nif (length(x)) {\n  # do something\n}\n\nIn this case, 0 is converted to FALSE and everything else is converted to TRUE. We think this makes it harder to understand your code, and we don’t recommend it. Instead be explicit: length(x) > 0.\nIt’s also important to understand what happens when you try and create a vector containing multiple types with c(): the most complex type always wins.\n\ntypeof(c(TRUE, 1L))\n#> [1] \"integer\"\ntypeof(c(1L, 1.5))\n#> [1] \"double\"\ntypeof(c(1.5, \"a\"))\n#> [1] \"character\"\n\nAn atomic vector can not have a mix of different types because the type is a property of the complete vector, not the individual elements. If you need to mix multiple types in the same vector, you should use a list, which you’ll learn about shortly.\n\n28.4.2 Test functions\nSometimes you want to do different things based on the type of vector. One option is to use typeof(). Another is to use a test function which returns a TRUE or FALSE. Base R provides many functions like is.vector() and is.atomic(), but they often return surprising results. Instead, it’s safer to use the is_* functions provided by purrr, which are summarised in the table below.\n\n\n\nlgl\nint\ndbl\nchr\nlist\n\n\n\nis_logical()\nx\n\n\n\n\n\n\nis_integer()\n\nx\n\n\n\n\n\nis_double()\n\n\nx\n\n\n\n\nis_numeric()\n\nx\nx\n\n\n\n\nis_character()\n\n\n\nx\n\n\n\nis_atomic()\nx\nx\nx\nx\n\n\n\nis_list()\n\n\n\n\nx\n\n\nis_vector()\nx\nx\nx\nx\nx\n\n\n\n28.4.3 Scalars and recycling rules\nAs well as implicitly coercing the types of vectors to be compatible, R will also implicitly coerce the length of vectors. This is called vector recycling, because the shorter vector is repeated, or recycled, to the same length as the longer vector.\nThis is generally most useful when you are mixing vectors and “scalars”. We put scalars in quotes because R doesn’t actually have scalars: instead, a single number is a vector of length 1. Because there are no scalars, most built-in functions are vectorised, meaning that they will operate on a vector of numbers. That’s why, for example, this code works:\n\nsample(10) + 100\n#>  [1] 107 104 103 109 102 101 106 110 105 108\nrunif(10) > 0.5\n#>  [1] FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nIn R, basic mathematical operations work with vectors. That means that you should never need to perform explicit iteration when performing simple mathematical computations.\nIt’s intuitive what should happen if you add two vectors of the same length, or a vector and a “scalar”, but what happens if you add two vectors of different lengths?\n\n1:10 + 1:2\n#>  [1]  2  4  4  6  6  8  8 10 10 12\n\nHere, R will expand the shortest vector to the same length as the longest, so called recycling. This is silent except when the length of the longer is not an integer multiple of the length of the shorter:\n\n1:10 + 1:3\n#> Warning in 1:10 + 1:3: longer object length is not a multiple of shorter object\n#> length\n#>  [1]  2  4  6  5  7  9  8 10 12 11\n\nWhile vector recycling can be used to create very succinct, clever code, it can also silently conceal problems. For this reason, the vectorised functions in tidyverse will throw errors when you recycle anything other than a scalar. If you do want to recycle, you’ll need to do it yourself with rep():\n\ntibble(x = 1:4, y = 1:2)\n#> Error:\n#> ! Tibble columns must have compatible sizes.\n#> • Size 4: Existing data.\n#> • Size 2: Column `y`.\n#> ℹ Only values of size one are recycled.\n\ntibble(x = 1:4, y = rep(1:2, 2))\n#> # A tibble: 4 × 2\n#>       x     y\n#>   <int> <int>\n#> 1     1     1\n#> 2     2     2\n#> 3     3     1\n#> 4     4     2\n\ntibble(x = 1:4, y = rep(1:2, each = 2))\n#> # A tibble: 4 × 2\n#>       x     y\n#>   <int> <int>\n#> 1     1     1\n#> 2     2     1\n#> 3     3     2\n#> 4     4     2\n\n\n28.4.4 Naming vectors\nAll types of vectors can be named. You can name them during creation with c():\n\nc(x = 1, y = 2, z = 4)\n#> x y z \n#> 1 2 4\n\nOr after the fact with purrr::set_names():\n\nset_names(1:3, c(\"a\", \"b\", \"c\"))\n#> a b c \n#> 1 2 3\n\nNamed vectors are most useful for subsetting, described next.\n\n28.4.5 Subsetting\nSo far we’ve used dplyr::filter() to filter the rows in a tibble. filter() only works with tibble, so we’ll need a new tool for vectors: [. [ is the subsetting function, and is called like x[a]. There are four types of things that you can subset a vector with:\n\n\nA numeric vector containing only integers. The integers must either be all positive, all negative, or zero.\nSubsetting with positive integers keeps the elements at those positions:\n\nx <- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n#> [1] \"three\" \"two\"   \"five\"\n\nBy repeating a position, you can actually make a longer output than input:\n\nx[c(1, 1, 5, 5, 5, 2)]\n#> [1] \"one\"  \"one\"  \"five\" \"five\" \"five\" \"two\"\n\nNegative values drop the elements at the specified positions:\n\nx[c(-1, -3, -5)]\n#> [1] \"two\"  \"four\"\n\nIt’s an error to mix positive and negative values:\n\nx[c(1, -1)]\n#> Error in x[c(1, -1)]: only 0's may be mixed with negative subscripts\n\nThe error message mentions subsetting with zero, which returns no values:\n\nx[0]\n#> character(0)\n\nThis is not useful very often, but it can be helpful if you want to create unusual data structures to test your functions with.\n\n\nSubsetting with a logical vector keeps all values corresponding to a TRUE value. This is most often useful in conjunction with the comparison functions.\n\nx <- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n#> [1] 10  3  5  8  1\n\n# All even (or missing!) values of x\nx[x %% 2 == 0]\n#> [1] 10 NA  8 NA\n\n\n\nIf you have a named vector, you can subset it with a character vector:\n\nx <- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n#> xyz def \n#>   5   2\n\nLike with positive integers, you can also use a character vector to duplicate individual entries.\n\nThe simplest type of subsetting is nothing, x[], which returns the complete x. This is not useful for subsetting vectors, but it is useful when subsetting matrices (and other high dimensional structures) because it lets you select all the rows or all the columns, by leaving that index blank. For example, if x is 2d, x[1, ] selects the first row and all the columns, and x[, -1] selects all rows and all columns except the first.\n\nTo learn more about the applications of subsetting, reading the “Subsetting” chapter of Advanced R: http://adv-r.had.co.nz/Subsetting.html#applications.\nThere is an important variation of [ called [[. [[ only ever extracts a single element, and always drops names. It’s a good idea to use it whenever you want to make it clear that you’re extracting a single item, as in a for loop. The distinction between [ and [[ is most important for lists, as we’ll see shortly.\n\n28.4.6 Exercises\n\nWhat does mean(is.na(x)) tell you about a vector x? What about sum(!is.finite(x))?\nCarefully read the documentation of is.vector(). What does it actually test for? Why does is.atomic() not agree with the definition of atomic vectors above?\nCompare and contrast setNames() with purrr::set_names().\n\nCreate functions that take a vector as input and return:\n\nThe last value. Should you use [ or [[?\nThe elements at even numbered positions.\nEvery element except the last value.\nOnly even numbers (and no missing values).\n\n\nWhy is x[-which(x > 0)] not the same as x[x <= 0]?\nWhat happens when you subset with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist?"
  },
  {
    "objectID": "vectors.html#sec-lists",
    "href": "vectors.html#sec-lists",
    "title": "28  Vectors",
    "section": "\n28.5 Recursive vectors (lists)",
    "text": "28.5 Recursive vectors (lists)\nLists are a step up in complexity from atomic vectors, because lists can contain other lists. This makes them suitable for representing hierarchical or tree-like structures. You create a list with list():\n\nx <- list(1, 2, 3)\nx\n#> [[1]]\n#> [1] 1\n#> \n#> [[2]]\n#> [1] 2\n#> \n#> [[3]]\n#> [1] 3\n\nA very useful tool for working with lists is str() because it focusses on the structure, not the contents.\n\nstr(x)\n#> List of 3\n#>  $ : num 1\n#>  $ : num 2\n#>  $ : num 3\n\nx_named <- list(a = 1, b = 2, c = 3)\nstr(x_named)\n#> List of 3\n#>  $ a: num 1\n#>  $ b: num 2\n#>  $ c: num 3\n\nUnlike atomic vectors, list() can contain a mix of objects:\n\ny <- list(\"a\", 1L, 1.5, TRUE)\nstr(y)\n#> List of 4\n#>  $ : chr \"a\"\n#>  $ : int 1\n#>  $ : num 1.5\n#>  $ : logi TRUE\n\nLists can even contain other lists!\n\nz <- list(list(1, 2), list(3, 4))\nstr(z)\n#> List of 2\n#>  $ :List of 2\n#>   ..$ : num 1\n#>   ..$ : num 2\n#>  $ :List of 2\n#>   ..$ : num 3\n#>   ..$ : num 4\n\n\n28.5.1 Visualising lists\nTo explain more complicated list manipulation functions, it’s helpful to have a visual representation of lists. For example, take these three lists:\n\nx1 <- list(c(1, 2), c(3, 4))\nx2 <- list(list(1, 2), list(3, 4))\nx3 <- list(1, list(2, list(3)))\n\nWe’ll draw them as follows:\n\n\n\n\n\nThere are three principles:\n\nLists have rounded corners. Atomic vectors have square corners.\nChildren are drawn inside their parent, and have a slightly darker background to make it easier to see the hierarchy.\nThe orientation of the children (i.e. rows or columns) isn’t important, so we’ll pick a row or column orientation to either save space or illustrate an important property in the example.\n\n28.5.2 Subsetting\nThere are three ways to subset a list, which we’ll illustrate with a list named a:\n\na <- list(a = 1:3, b = \"a string\", c = pi, d = list(-1, -5))\n\n\n\n[ extracts a sub-list. The result will always be a list.\n\nstr(a[1:2])\n#> List of 2\n#>  $ a: int [1:3] 1 2 3\n#>  $ b: chr \"a string\"\nstr(a[4])\n#> List of 1\n#>  $ d:List of 2\n#>   ..$ : num -1\n#>   ..$ : num -5\n\nLike with vectors, you can subset with a logical, integer, or character vector.\n\n\n[[ extracts a single component from a list. It removes a level of hierarchy from the list.\n\nstr(a[[1]])\n#>  int [1:3] 1 2 3\nstr(a[[4]])\n#> List of 2\n#>  $ : num -1\n#>  $ : num -5\n\n\n\n$ is a shorthand for extracting named elements of a list. It works similarly to [[ except that you don’t need to use quotes.\n\na$a\n#> [1] 1 2 3\na[[\"a\"]]\n#> [1] 1 2 3\n\n\n\nThe distinction between [ and [[ is really important for lists, because [[ drills down into the list while [ returns a new, smaller list. Compare the code and output above with the visual representation in Figure 28.2.\n\n\n\n\nFigure 28.2: Subsetting a list, visually.\n\n\n\n\n\n28.5.3 Lists of condiments\nThe difference between [ and [[ is very important, but it’s easy to get confused. To help you remember, let me show you an unusual pepper shaker.\n\n\n\n\n\nIf this pepper shaker is your list x, then, x[1] is a pepper shaker containing a single pepper packet:\n\n\n\n\n\nx[2] would look the same, but would contain the second packet. x[1:2] would be a pepper shaker containing two pepper packets.\nx[[1]] is:\n\n\n\n\n\nIf you wanted to get the content of the pepper package, you’d need x[[1]][[1]]:\n\n\n\n\n\n\n28.5.4 Exercises\n\n\nDraw the following lists as nested sets:\n\nlist(a, b, list(c, d), list(e, f))\nlist(list(list(list(list(list(a))))))\n\n\nWhat happens if you subset a tibble as if you’re subsetting a list? What are the key differences between a list and a tibble?"
  },
  {
    "objectID": "vectors.html#attributes",
    "href": "vectors.html#attributes",
    "title": "28  Vectors",
    "section": "\n28.6 Attributes",
    "text": "28.6 Attributes\nAny vector can contain arbitrary additional metadata through its attributes. You can think of attributes as named list of vectors that can be attached to any object. You can get and set individual attribute values with attr() or see them all at once with attributes().\n\nx <- 1:10\nattr(x, \"greeting\")\n#> NULL\nattr(x, \"greeting\") <- \"Hi!\"\nattr(x, \"farewell\") <- \"Bye!\"\nattributes(x)\n#> $greeting\n#> [1] \"Hi!\"\n#> \n#> $farewell\n#> [1] \"Bye!\"\n\nThere are three very important attributes that are used to implement fundamental parts of R:\n\n\nNames are used to name the elements of a vector.\n\nDimensions (dims, for short) make a vector behave like a matrix or array.\n\nClass is used to implement the S3 object oriented system.\n\nYou’ve seen names above, and we won’t cover dimensions because we don’t use matrices in this book. It remains to describe the class, which controls how generic functions work. Generic functions are key to object oriented programming in R, because they make functions behave differently for different classes of input. A detailed discussion of object oriented programming is beyond the scope of this book, but you can read more about it in Advanced R at http://adv-r.had.co.nz/OO-essentials.html#s3.\nHere’s what a typical generic function looks like:\n\nas.Date\n#> function (x, ...) \n#> UseMethod(\"as.Date\")\n#> <bytecode: 0x000001be30c98aa8>\n#> <environment: namespace:base>\n\nThe call to “UseMethod” means that this is a generic function, and it will call a specific method, a function, based on the class of the first argument. (All methods are functions; not all functions are methods). You can list all the methods for a generic with methods():\n\nmethods(\"as.Date\")\n#> [1] as.Date.character   as.Date.default     as.Date.factor     \n#> [4] as.Date.numeric     as.Date.POSIXct     as.Date.POSIXlt    \n#> [7] as.Date.vctrs_sclr* as.Date.vctrs_vctr*\n#> see '?methods' for accessing help and source code\n\nFor example, if x is a character vector, as.Date() will call as.Date.character(); if it’s a factor, it’ll call as.Date.factor().\nYou can see the specific implementation of a method with getS3method():\n\ngetS3method(\"as.Date\", \"default\")\n#> function (x, ...) \n#> {\n#>     if (inherits(x, \"Date\")) \n#>         x\n#>     else if (is.null(x)) \n#>         .Date(numeric())\n#>     else if (is.logical(x) && all(is.na(x))) \n#>         .Date(as.numeric(x))\n#>     else stop(gettextf(\"do not know how to convert '%s' to class %s\", \n#>         deparse1(substitute(x)), dQuote(\"Date\")), domain = NA)\n#> }\n#> <bytecode: 0x000001be34184b00>\n#> <environment: namespace:base>\ngetS3method(\"as.Date\", \"numeric\")\n#> function (x, origin, ...) \n#> {\n#>     if (missing(origin)) {\n#>         if (!length(x)) \n#>             return(.Date(numeric()))\n#>         if (!any(is.finite(x))) \n#>             return(.Date(x))\n#>         stop(\"'origin' must be supplied\")\n#>     }\n#>     as.Date(origin, ...) + x\n#> }\n#> <bytecode: 0x000001be36d89778>\n#> <environment: namespace:base>\n\nThe most important S3 generic is print(): it controls how the object is printed when you type its name at the console. Other important generics are the subsetting functions [, [[, and $."
  },
  {
    "objectID": "vectors.html#augmented-vectors",
    "href": "vectors.html#augmented-vectors",
    "title": "28  Vectors",
    "section": "\n28.7 Augmented vectors",
    "text": "28.7 Augmented vectors\nAtomic vectors and lists are the building blocks for other important vector types like factors and dates. We call these augmented vectors, because they are vectors with additional attributes, including class. Because augmented vectors have a class, they behave differently to the atomic vector on which they are built. In this book, we make use of four important augmented vectors:\n\nFactors\nDates\nDate-times\nTibbles\n\nThese are described below.\n\n28.7.1 Factors\nFactors are designed to represent categorical data that can take a fixed set of possible values. Factors are built on top of integers, and have a levels attribute:\n\nx <- factor(c(\"ab\", \"cd\", \"ab\"), levels = c(\"ab\", \"cd\", \"ef\"))\ntypeof(x)\n#> [1] \"integer\"\nattributes(x)\n#> $levels\n#> [1] \"ab\" \"cd\" \"ef\"\n#> \n#> $class\n#> [1] \"factor\"\n\n\n28.7.2 Dates and date-times\nDates in R are numeric vectors that represent the number of days since 1 January 1970.\n\nx <- as.Date(\"1971-01-01\")\nunclass(x)\n#> [1] 365\n\ntypeof(x)\n#> [1] \"double\"\nattributes(x)\n#> $class\n#> [1] \"Date\"\n\nDate-times are numeric vectors with class POSIXct that represent the number of seconds since 1 January 1970. (In case you were wondering, “POSIXct” stands for “Portable Operating System Interface”, calendar time.)\n\nx <- lubridate::ymd_hm(\"1970-01-01 01:00\")\nunclass(x)\n#> [1] 3600\n#> attr(,\"tzone\")\n#> [1] \"UTC\"\n\ntypeof(x)\n#> [1] \"double\"\nattributes(x)\n#> $class\n#> [1] \"POSIXct\" \"POSIXt\" \n#> \n#> $tzone\n#> [1] \"UTC\"\n\nThe tzone attribute is optional. It controls how the time is printed, not what absolute time it refers to.\n\nattr(x, \"tzone\") <- \"US/Pacific\"\nx\n#> [1] \"1969-12-31 17:00:00 PST\"\n\nattr(x, \"tzone\") <- \"US/Eastern\"\nx\n#> [1] \"1969-12-31 20:00:00 EST\"\n\nThere is another type of date-times called POSIXlt. These are built on top of named lists:\n\ny <- as.POSIXlt(x)\ntypeof(y)\n#> [1] \"list\"\nattributes(y)\n#> $names\n#>  [1] \"sec\"    \"min\"    \"hour\"   \"mday\"   \"mon\"    \"year\"   \"wday\"   \"yday\"  \n#>  [9] \"isdst\"  \"zone\"   \"gmtoff\"\n#> \n#> $class\n#> [1] \"POSIXlt\" \"POSIXt\" \n#> \n#> $tzone\n#> [1] \"US/Eastern\" \"EST\"        \"EDT\"\n\nPOSIXlts are rare inside the tidyverse. They do crop up in base R, because they are needed to extract specific components of a date, like the year or month. Since lubridate provides helpers for you to do this instead, you don’t need them. POSIXct’s are always easier to work with, so if you find you have a POSIXlt, you should always convert it to a regular date time with lubridate::as_date_time().\n\n28.7.3 Tibbles\nTibbles are augmented lists: they have class “tbl_df” + “tbl” + “data.frame”, and names (column) and row.names attributes:\n\ntb <- tibble::tibble(x = 1:5, y = 5:1)\ntypeof(tb)\n#> [1] \"list\"\nattributes(tb)\n#> $class\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n#> \n#> $row.names\n#> [1] 1 2 3 4 5\n#> \n#> $names\n#> [1] \"x\" \"y\"\n\nThe difference between a tibble and a list is that all the elements of a data frame must be vectors with the same length. All functions that work with tibbles enforce this constraint.\nTraditional data.frames have a very similar structure:\n\ndf <- data.frame(x = 1:5, y = 5:1)\ntypeof(df)\n#> [1] \"list\"\nattributes(df)\n#> $names\n#> [1] \"x\" \"y\"\n#> \n#> $class\n#> [1] \"data.frame\"\n#> \n#> $row.names\n#> [1] 1 2 3 4 5\n\nThe main difference is the class. The class of tibble includes “data.frame” which means tibbles inherit the regular data frame behaviour by default.\n\n28.7.4 Exercises\n\nWhat does hms::hms(3600) return? How does it print? What primitive type is the augmented vector built on top of? What attributes does it use?\nTry and make a tibble that has columns with different lengths. What happens?\nBased on the definition above, is it ok to have a list as a column of a tibble?"
  },
  {
    "objectID": "iteration.html",
    "href": "iteration.html",
    "title": "29  Iteration",
    "section": "",
    "text": "In Chapter 27, we talked about how important it is to reduce duplication in your code by creating functions instead of copying-and-pasting. Reducing code duplication has three main benefits:\n\nIt’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same.\nIt’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code.\nYou’re likely to have fewer bugs because each line of code is used in more places.\n\nOne tool for reducing duplication is functions, which reduce duplication by identifying repeated patterns of code and extract them out into independent pieces that can be easily reused and updated. Another tool for reducing duplication is iteration, which helps you when you need to do the same thing to multiple inputs: repeating the same operation on different columns, or on different datasets. In this chapter you’ll learn about two important iteration paradigms: imperative programming and functional programming. On the imperative side you have tools like for loops and while loops, which are a great place to start because they make iteration very explicit, so it’s obvious what’s happening. However, for loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. Functional programming (FP) offers tools to extract out this duplicated code, so each common for loop pattern gets its own function. Once you master the vocabulary of FP, you can solve many common iteration problems with less code, more ease, and fewer errors.\n\nOnce you’ve mastered the for loops provided by base R, you’ll learn some of the powerful programming tools provided by purrr, one of the tidyverse core packages.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "iteration.html#for-loops",
    "href": "iteration.html#for-loops",
    "title": "29  Iteration",
    "section": "\n29.2 For loops",
    "text": "29.2 For loops\nImagine we have this simple tibble:\n\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nWe want to compute the median of each column. You could do with copy-and-paste:\n\nmedian(df$a)\n#> [1] -0.2457625\nmedian(df$b)\n#> [1] -0.2873072\nmedian(df$c)\n#> [1] -0.05669771\nmedian(df$d)\n#> [1] 0.1442633\n\nBut that breaks our rule of thumb: never copy and paste more than twice. Instead, we could use a for loop:\n\noutput <- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output[[i]] <- median(df[[i]])      # 3. body\n}\noutput\n#> [1] -0.24576245 -0.28730721 -0.05669771  0.14426335\n\nEvery for loop has three components:\n\n\nThe output: output <- vector(\"double\", length(x)). Before you start the loop, you must always allocate sufficient space for the output. This is very important for efficiency: if you grow the for loop at each iteration using c() (for example), your for loop will be very slow.\nA general way of creating an empty vector of given length is the vector() function. It has two arguments: the type of the vector (“logical”, “integer”, “double”, “character”, etc) and the length of the vector.\n\n\nThe sequence: i in seq_along(df). This determines what to loop over: each run of the for loop will assign i to a different value from seq_along(df). It’s useful to think of i as a pronoun, like “it”.\nYou might not have seen seq_along() before. It’s a safe version of the familiar 1:length(l), with an important difference: if you have a zero-length vector, seq_along() does the right thing:\n\ny <- vector(\"double\", 0)\nseq_along(y)\n#> integer(0)\n1:length(y)\n#> [1] 1 0\n\nYou probably won’t create a zero-length vector deliberately, but it’s easy to create them accidentally. If you use 1:length(x) instead of seq_along(x), you’re likely to get a confusing error message.\n\nThe body: output[[i]] <- median(df[[i]]). This is the code that does the work. It’s run repeatedly, each time with a different value for i. The first iteration will run output[[1]] <- median(df[[1]]), the second will run output[[2]] <- median(df[[2]]), and so on.\n\nThat’s all there is to the for loop! Now is a good time to practice creating some basic (and not so basic) for loops using the exercises below. Then we’ll move on to some variations of the for loop that help you solve other problems that will crop up in practice.\n\n29.2.1 Exercises\n\n\nWrite for loops to:\n\nCompute the mean of every column in mtcars.\nDetermine the type of each column in nycflights13::flights.\nCompute the number of unique values in each column of palmerpenguins::penguins.\nGenerate 10 random normals from distributions with means of -10, 0, 10, and 100.\n\nThink about the output, sequence, and body before you start writing the loop.\n\n\nEliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:\n\nout <- \"\"\nfor (x in letters) {\n  out <- stringr::str_c(out, x)\n}\n\nx <- sample(100)\nsd <- 0\nfor (i in seq_along(x)) {\n  sd <- sd + (x[i] - mean(x)) ^ 2\n}\nsd <- sqrt(sd / (length(x) - 1))\n\nx <- runif(100)\nout <- vector(\"numeric\", length(x))\nout[1] <- x[1]\nfor (i in 2:length(x)) {\n  out[i] <- out[i - 1] + x[i]\n}\n\n\n\nCombine your function writing and for loop skills:\n\nWrite a for loop that prints() the lyrics to the children’s song “Alice the camel”.\nConvert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure.\nConvert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface.\n\n\n\nIt’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step:\n\noutput <- vector(\"integer\", 0)\nfor (i in seq_along(x)) {\n  output <- c(output, lengths(x[[i]]))\n}\noutput\n\nHow does this affect performance? Design and execute an experiment."
  },
  {
    "objectID": "iteration.html#for-loop-variations",
    "href": "iteration.html#for-loop-variations",
    "title": "29  Iteration",
    "section": "\n29.3 For loop variations",
    "text": "29.3 For loop variations\nOnce you have the basic for loop under your belt, there are some variations that you should be aware of. These variations are important regardless of how you do iteration, so don’t forget about them once you’ve mastered the FP techniques you’ll learn about in the next section.\nThere are four variations on the basic theme of the for loop:\n\nModifying an existing object, instead of creating a new object.\nLooping over names or values, instead of indices.\nHandling outputs of unknown length.\nHandling sequences of unknown length.\n\n\n29.3.1 Modifying an existing object\nSometimes you want to use a for loop to modify an existing object. For example, remember our challenge from Chapter 27 on functions. We wanted to rescale every column in a data frame:\n\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a <- rescale01(df$a)\ndf$b <- rescale01(df$b)\ndf$c <- rescale01(df$c)\ndf$d <- rescale01(df$d)\n\nTo solve this with a for loop we again think about the three components:\n\nOutput: we already have the output — it’s the same as the input!\nSequence: we can think about a data frame as a list of columns, so we can iterate over each column with seq_along(df).\nBody: apply rescale01().\n\nThis gives us:\n\nfor (i in seq_along(df)) {\n  df[[i]] <- rescale01(df[[i]])\n}\n\nTypically you’ll be modifying a list or data frame with this sort of loop, so remember to use [[, not [. You might have spotted that we used [[ in all my for loops: we think it’s better to use [[ even for atomic vectors because it makes it clear that you want to work with a single element.\n\n29.3.2 Looping patterns\nThere are three basic ways to loop over a vector. So far we’ve shown you the most general: looping over the numeric indices with for (i in seq_along(xs)), and extracting the value with x[[i]]. There are two other forms:\n\nLoop over the elements: for (x in xs). This is most useful if you only care about side-effects, like plotting or saving a file, because it’s difficult to save the output efficiently.\n\nLoop over the names: for (nm in names(xs)). This gives you a name, which you can use to access the value with x[[nm]]. This is useful if you want to use the name in a plot title or a file name. If you’re creating named output, make sure to name the results vector like so:\n\nresults <- vector(\"list\", length(x))\nnames(results) <- names(x)\n\n\n\nIteration over the numeric indices is the most general form, because given the position you can extract both the name and the value:\n\nfor (i in seq_along(x)) {\n  name <- names(x)[[i]]\n  value <- x[[i]]\n}\n\n\n29.3.3 Unknown output length\nSometimes you might not know how long the output will be. For example, imagine you want to simulate some random vectors of random lengths. You might be tempted to solve this problem by progressively growing the vector:\n\nmeans <- c(0, 1, 2)\n\noutput <- double()\nfor (i in seq_along(means)) {\n  n <- sample(100, 1)\n  output <- c(output, rnorm(n, means[[i]]))\n}\nstr(output)\n#>  num [1:138] 0.912 0.205 2.584 -0.789 0.588 ...\n\nBut this is not very efficient because in each iteration, R has to copy all the data from the previous iterations. In technical terms you get “quadratic” (\\(O(n^2)\\)) behaviour which means that a loop with three times as many elements would take nine (\\(3^2\\)) times as long to run.\nA better solution to save the results in a list, and then combine into a single vector after the loop is done:\n\nout <- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  n <- sample(100, 1)\n  out[[i]] <- rnorm(n, means[[i]])\n}\nstr(out)\n#> List of 3\n#>  $ : num [1:76] -0.3389 -0.0756 0.0402 0.1243 -0.9984 ...\n#>  $ : num [1:17] -0.11 1.149 0.614 0.77 1.392 ...\n#>  $ : num [1:41] 1.88 2.46 2.62 1.82 1.88 ...\nstr(unlist(out))\n#>  num [1:134] -0.3389 -0.0756 0.0402 0.1243 -0.9984 ...\n\nHere we’ve used unlist() to flatten a list of vectors into a single vector. A stricter option is to use purrr::flatten_dbl() — it will throw an error if the input isn’t a list of doubles.\nThis pattern occurs in other places too:\n\nYou might be generating a long string. Instead of paste()ing together each iteration with the previous, save the output in a character vector and then combine that vector into a single string with paste(output, collapse = \"\").\nYou might be generating a big data frame. Instead of sequentially rbind()ing in each iteration, save the output in a list, then use dplyr::bind_rows(output) to combine the output into a single data frame.\n\nWatch out for this pattern. Whenever you see it, switch to a more complex result object, and then combine in one step at the end.\n\n29.3.4 Unknown sequence length\nSometimes you don’t even know how long the input sequence should run for. This is common when doing simulations. For example, you might want to loop until you get three heads in a row. You can’t do that sort of iteration with the for loop. Instead, you can use a while loop. A while loop is simpler than a for loop because it only has two components, a condition and a body:\n\nwhile (condition) {\n  # body\n}\n\nA while loop is also more general than a for loop, because you can rewrite any for loop as a while loop, but you can’t rewrite every while loop as a for loop:\n\nfor (i in seq_along(x)) {\n  # body\n}\n\n# Equivalent to\ni <- 1\nwhile (i <= length(x)) {\n  # body\n  i <- i + 1 \n}\n\nHere’s how we could use a while loop to find how many tries it takes to get three heads in a row:\n\nflip <- function() sample(c(\"T\", \"H\"), 1)\n\nflips <- 0\nnheads <- 0\n\nwhile (nheads < 3) {\n  if (flip() == \"H\") {\n    nheads <- nheads + 1\n  } else {\n    nheads <- 0\n  }\n  flips <- flips + 1\n}\nflips\n#> [1] 21\n\nI mention while loops only briefly, because we hardly ever use them. They’re most often used for simulation, which is outside the scope of this book. However, it is good to know they exist so that you’re prepared for problems where the number of iterations is not known in advance.\n\n29.3.5 Exercises\n\nImagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files <- dir(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame.\nWhat happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique?\n\nWrite a function that prints the mean of each numeric column in a data frame, along with its name. For example, show_mean(mpg) would print:\n\nshow_mean(mpg)\n#> displ:   3.47\n#> year:    2004\n#> cyl:     5.89\n#> cty:    16.86\n\n(Extra challenge: what function did we use to make sure that the numbers lined up nicely, even though the variable names had different lengths?)\n\n\nWhat does this code do? How does it work?\n\ntrans <- list( \n  disp = function(x) x * 0.0163871,\n  am = function(x) {\n    factor(x, labels = c(\"auto\", \"manual\"))\n  }\n)\nfor (var in names(trans)) {\n  mtcars[[var]] <- trans[[var]](mtcars[[var]])\n}"
  },
  {
    "objectID": "iteration.html#for-loops-vs.-functionals",
    "href": "iteration.html#for-loops-vs.-functionals",
    "title": "29  Iteration",
    "section": "\n29.4 For loops vs. functionals",
    "text": "29.4 For loops vs. functionals\nFor loops are not as important in R as they are in other languages because R is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly.\nTo see why this is important, consider (again) this simple data frame:\n\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nImagine you want to compute the mean of every column. You could do that with a for loop:\n\noutput <- vector(\"double\", length(df))\nfor (i in seq_along(df)) {\n  output[[i]] <- mean(df[[i]])\n}\noutput\n#> [1] -0.3260369  0.1356639  0.4291403 -0.2498034\n\nYou realise that you’re going to want to compute the means of every column pretty frequently, so you extract it out into a function:\n\ncol_mean <- function(df) {\n  output <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    output[i] <- mean(df[[i]])\n  }\n  output\n}\n\nBut then you think it’d also be helpful to be able to compute the median, and the standard deviation, so you copy and paste your col_mean() function and replace the mean() with median() and sd():\n\ncol_median <- function(df) {\n  output <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    output[i] <- median(df[[i]])\n  }\n  output\n}\ncol_sd <- function(df) {\n  output <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    output[i] <- sd(df[[i]])\n  }\n  output\n}\n\nUh oh! You’ve copied-and-pasted this code twice, so it’s time to think about how to generalise it. Notice that most of this code is for-loop boilerplate and it’s hard to see the one thing (mean(), median(), sd()) that is different between the functions.\nWhat would you do if you saw a set of functions like this:\n\nf1 <- function(x) abs(x - mean(x)) ^ 1\nf2 <- function(x) abs(x - mean(x)) ^ 2\nf3 <- function(x) abs(x - mean(x)) ^ 3\n\nHopefully, you’d notice that there’s a lot of duplication, and extract it out into an additional argument:\n\nf <- function(x, i) abs(x - mean(x)) ^ i\n\nYou’ve reduced the chance of bugs (because you now have 1/3 of the original code), and made it easy to generalise to new situations.\nWe can do exactly the same thing with col_mean(), col_median() and col_sd() by adding an argument that supplies the function to apply to each column:\n\ncol_summary <- function(df, fun) {\n  out <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] <- fun(df[[i]])\n  }\n  out\n}\ncol_summary(df, median)\n#> [1] -0.51850298  0.02779864  0.17295591 -0.61163819\ncol_summary(df, mean)\n#> [1] -0.3260369  0.1356639  0.4291403 -0.2498034\n\nThe idea of passing a function to another function is an extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language. It might take you a while to wrap your head around the idea, but it’s worth the investment. In the rest of the chapter, you’ll learn about and use the purrr package, which provides functions that eliminate the need for many common for loops. The apply family of functions in base R (apply(), lapply(), tapply(), etc) solve a similar problem, but purrr is more consistent and thus is easier to learn.\nThe goal of using purrr functions instead of for loops is to allow you to break common list manipulation challenges into independent pieces:\n\nHow can you solve the problem for a single element of the list? Once you’ve solved that problem, purrr takes care of generalising your solution to every element in the list.\nIf you’re solving a complex problem, how can you break it down into bite-sized pieces that allow you to advance one small step towards a solution? With purrr, you get lots of small pieces that you can compose together with the pipe.\n\nThis structure makes it easier to solve new problems. It also makes it easier to understand your solutions to old problems when you re-read your old code.\n\n29.4.1 Exercises\n\nRead the documentation for apply(). In the 2d case, what two for loops does it generalise?\nAdapt col_summary() so that it only applies to numeric columns You might want to start with an is_numeric() function that returns a logical vector that has a TRUE corresponding to each numeric column."
  },
  {
    "objectID": "iteration.html#the-map-functions",
    "href": "iteration.html#the-map-functions",
    "title": "29  Iteration",
    "section": "\n29.5 The map functions",
    "text": "29.5 The map functions\nThe pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output:\n\n\nmap() makes a list.\n\nmap_lgl() makes a logical vector.\n\nmap_int() makes an integer vector.\n\nmap_dbl() makes a double vector.\n\nmap_chr() makes a character vector.\n\nEach function takes a vector as input, applies a function to each piece, and then returns a new vector that’s the same length (and has the same names) as the input. The type of the vector is determined by the suffix to the map function.\nOnce you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!).\nSome people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years.) The chief benefits of using functions like map() is not speed, but clarity: they make your code easier to write and to read.\nWe can use these functions to perform the same computations as the last for loop. Those summary functions returned doubles, so we need to use map_dbl():\n\nmap_dbl(df, mean)\n#>          a          b          c          d \n#> -0.3260369  0.1356639  0.4291403 -0.2498034\nmap_dbl(df, median)\n#>           a           b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819\nmap_dbl(df, sd)\n#>         a         b         c         d \n#> 0.9214834 0.4848945 0.9816016 1.1563324\n\nCompared to using a for loop, focus is on the operation being performed (i.e. mean(), median(), sd()), not the bookkeeping required to loop over every element and store the output. This is even more apparent if we use the pipe:\n\ndf |> map_dbl(mean)\n#>          a          b          c          d \n#> -0.3260369  0.1356639  0.4291403 -0.2498034\ndf |> map_dbl(median)\n#>           a           b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819\ndf |> map_dbl(sd)\n#>         a         b         c         d \n#> 0.9214834 0.4848945 0.9816016 1.1563324\n\nThere are a few differences between map_*() and col_summary():\n\nAll purrr functions are implemented in C. This makes them a little faster at the expense of readability.\nThe second argument, .f, the function to apply, can be a formula, a character vector, or an integer vector. You’ll learn about those handy shortcuts in the next section.\n\nmap_*() uses … ([dot dot dot]) to pass along additional arguments to .f each time it’s called:\n\nmap_dbl(df, mean, trim = 0.5)\n#>           a           b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819\n\n\n\nThe map functions also preserve names:\n\nz <- list(x = 1:3, y = 4:5)\nmap_int(z, length)\n#> x y \n#> 3 2\n\n\n\n\n29.5.1 Shortcuts\nThere are a few shortcuts that you can use with .f in order to save a little typing. Imagine you want to fit a linear model to each group in a dataset. The following toy example splits up the mtcars dataset into three pieces (one for each value of cylinder) and fits the same linear model to each piece:\n\nmodels <- mtcars |> \n  split(mtcars$cyl) |> \n  map(function(df) lm(mpg ~ wt, data = df))\n\nThe syntax for creating an anonymous function in R is quite verbose so purrr provides a convenient shortcut: a one-sided formula.\n\nmodels <- mtcars |> \n  split(mtcars$cyl) |> \n  map(~lm(mpg ~ wt, data = .x))\n\nHere we’ve used .x as a pronoun: it refers to the current list element (in the same way that i referred to the current index in the for loop). .x in a one-sided formula corresponds to an argument in an anonymous function.\nWhen you’re looking at many models, you might want to extract a summary statistic like the \\(R^2\\). To do that we need to first run summary() and then extract the component called r.squared. We could do that using the shorthand for anonymous functions:\n\nmodels |> \n  map(summary) |> \n  map_dbl(~ .x$r.squared)\n#>         4         6         8 \n#> 0.5086326 0.4645102 0.4229655\n\nBut extracting named components is a common operation, so purrr provides an even shorter shortcut: you can use a string.\n\nmodels |> \n  map(summary) |> \n  map_dbl(\"r.squared\")\n#>         4         6         8 \n#> 0.5086326 0.4645102 0.4229655\n\nYou can also use an integer to select elements by position:\n\nx <- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\nx |> map_dbl(2)\n#> [1] 2 5 8\n\n\n29.5.2 Base R\nIf you’re familiar with the apply family of functions in base R, you might have noticed some similarities with the purrr functions:\n\nlapply() is basically identical to map(), except that map() is consistent with all the other functions in purrr, and you can use the shortcuts for .f.\n\nBase sapply() is a wrapper around lapply() that automatically simplifies the output. This is useful for interactive work but is problematic in a function because you never know what sort of output you’ll get:\n\nx1 <- list(\n  c(0.27, 0.37, 0.57, 0.91, 0.20),\n  c(0.90, 0.94, 0.66, 0.63, 0.06), \n  c(0.21, 0.18, 0.69, 0.38, 0.77)\n)\nx2 <- list(\n  c(0.50, 0.72, 0.99, 0.38, 0.78), \n  c(0.93, 0.21, 0.65, 0.13, 0.27), \n  c(0.39, 0.01, 0.38, 0.87, 0.34)\n)\n\nthreshold <- function(x, cutoff = 0.8) x[x > cutoff]\nx1 |> sapply(threshold) |> str()\n#> List of 3\n#>  $ : num 0.91\n#>  $ : num [1:2] 0.9 0.94\n#>  $ : num(0)\nx2 |> sapply(threshold) |> str()\n#>  num [1:3] 0.99 0.93 0.87\n\n\nvapply() is a safe alternative to sapply() because you supply an additional argument that defines the type. The only problem with vapply() is that it’s a lot of typing: vapply(df, is.numeric, logical(1)) is equivalent to map_lgl(df, is.numeric). One advantage of vapply() over purrr’s map functions is that it can also produce matrices — the map functions only ever produce vectors.\n\nWe focus on purrr functions here because they have more consistent names and arguments, helpful shortcuts, and in the future will provide easy parallelism and progress bars.\n\n29.5.3 Exercises\n\n\nWrite code that uses one of the map functions to:\n\nCompute the mean of every column in mtcars.\nDetermine the type of each column in nycflights13::flights.\nCompute the number of unique values in each column of palmerpenguins::penguins.\nGenerate 10 random normals from distributions with means of -10, 0, 10, and 100.\n\n\nHow can you create a single vector that for each column in a data frame indicates whether or not it’s a factor?\nWhat happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why?\nWhat does map(-2:2, rnorm, n = 5) do? Why? What does map_dbl(-2:2, rnorm, n = 5) do? Why?\nRewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function."
  },
  {
    "objectID": "iteration.html#dealing-with-failure",
    "href": "iteration.html#dealing-with-failure",
    "title": "29  Iteration",
    "section": "\n29.6 Dealing with failure",
    "text": "29.6 Dealing with failure\nWhen you use the map functions to repeat many operations, the chances are much higher that one of those operations will fail. When this happens, you’ll get an error message, and no output. This is annoying: why does one failure prevent you from accessing all the other successes? How do you ensure that one bad apple doesn’t ruin the whole barrel?\nIn this section you’ll learn how to deal with this situation with a new function: safely(). safely() is an adverb: it takes a function (a verb) and returns a modified version. In this case, the modified function will never throw an error. Instead, it always returns a list with two elements:\n\nresult is the original result. If there was an error, this will be NULL.\nerror is an error object. If the operation was successful, this will be NULL.\n\n(You might be familiar with the try() function in base R. It’s similar, but because it sometimes returns the original result and it sometimes returns an error object it’s more difficult to work with.)\nLet’s illustrate this with a simple example: log():\n\nsafe_log <- safely(log)\nstr(safe_log(10))\n#> List of 2\n#>  $ result: num 2.3\n#>  $ error : NULL\nstr(safe_log(\"a\"))\n#> List of 2\n#>  $ result: NULL\n#>  $ error :List of 2\n#>   ..$ message: chr \"non-numeric argument to mathematical function\"\n#>   ..$ call   : language .Primitive(\"log\")(x, base)\n#>   ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\nWhen the function succeeds, the result element contains the result and the error element is NULL. When the function fails, the result element is NULL and the error element contains an error object.\nsafely() is designed to work with map:\n\nx <- list(1, 10, \"a\")\ny <- x |> map(safely(log))\nstr(y)\n#> List of 3\n#>  $ :List of 2\n#>   ..$ result: num 0\n#>   ..$ error : NULL\n#>  $ :List of 2\n#>   ..$ result: num 2.3\n#>   ..$ error : NULL\n#>  $ :List of 2\n#>   ..$ result: NULL\n#>   ..$ error :List of 2\n#>   .. ..$ message: chr \"non-numeric argument to mathematical function\"\n#>   .. ..$ call   : language .Primitive(\"log\")(x, base)\n#>   .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\nThis would be easier to work with if we had two lists: one of all the errors and one of all the output. That’s easy to get with purrr::transpose():\n\ny <- y |> transpose()\nstr(y)\n#> List of 2\n#>  $ result:List of 3\n#>   ..$ : num 0\n#>   ..$ : num 2.3\n#>   ..$ : NULL\n#>  $ error :List of 3\n#>   ..$ : NULL\n#>   ..$ : NULL\n#>   ..$ :List of 2\n#>   .. ..$ message: chr \"non-numeric argument to mathematical function\"\n#>   .. ..$ call   : language .Primitive(\"log\")(x, base)\n#>   .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\nIt’s up to you how to deal with the errors, but typically you’ll either look at the values of x where y is an error, or work with the values of y that are ok:\n\nis_ok <- y$error |> map_lgl(is_null)\nx[!is_ok]\n#> [[1]]\n#> [1] \"a\"\ny$result[is_ok] |> flatten_dbl()\n#> [1] 0.000000 2.302585\n\nPurrr provides two other useful adverbs:\n\n\nLike safely(), possibly() always succeeds. It’s simpler than safely(), because you give it a default value to return when there is an error.\n\nx <- list(1, 10, \"a\")\nx |> map_dbl(possibly(log, NA_real_))\n#> [1] 0.000000 2.302585       NA\n\n\n\nquietly() performs a similar role to safely(), but instead of capturing errors, it captures printed output, messages, and warnings:\n\nx <- list(1, -1)\nx |> map(quietly(log)) |> str()\n#> List of 2\n#>  $ :List of 4\n#>   ..$ result  : num 0\n#>   ..$ output  : chr \"\"\n#>   ..$ warnings: chr(0) \n#>   ..$ messages: chr(0) \n#>  $ :List of 4\n#>   ..$ result  : num NaN\n#>   ..$ output  : chr \"\"\n#>   ..$ warnings: chr \"NaNs produced\"\n#>   ..$ messages: chr(0)"
  },
  {
    "objectID": "iteration.html#mapping-over-multiple-arguments",
    "href": "iteration.html#mapping-over-multiple-arguments",
    "title": "29  Iteration",
    "section": "\n29.7 Mapping over multiple arguments",
    "text": "29.7 Mapping over multiple arguments\nSo far we’ve mapped along a single input. But often you have multiple related inputs that you need to iterate along in parallel. That’s the job of the map2() and pmap() functions. For example, imagine you want to simulate some random normals with different means. You know how to do that with map():\n\nmu <- list(5, 10, -3)\nmu |> \n  map(rnorm, n = 5) |> \n  str()\n#> List of 3\n#>  $ : num [1:5] 5.63 7.1 4.39 3.37 4.99\n#>  $ : num [1:5] 9.34 9.33 9.52 11.32 10.64\n#>  $ : num [1:5] -2.49 -4.75 -2.11 -2.78 -2.42\n\nWhat if you also want to vary the standard deviation? One way to do that would be to iterate over the indices and index into vectors of means and sds:\n\nsigma <- list(1, 5, 10)\nseq_along(mu) |> \n  map(~rnorm(5, mu[[.x]], sigma[[.x]])) |> \n  str()\n#> List of 3\n#>  $ : num [1:5] 4.82 5.74 4 2.06 5.72\n#>  $ : num [1:5] 6.51 0.529 10.381 14.377 12.269\n#>  $ : num [1:5] -11.51 2.66 8.52 -10.56 -7.89\n\nBut that obfuscates the intent of the code. Instead we could use map2() which iterates over two vectors in parallel:\n\nmap2(mu, sigma, rnorm, n = 5) |> str()\n#> List of 3\n#>  $ : num [1:5] 3.83 4.52 5.12 3.23 3.59\n#>  $ : num [1:5] 13.55 3.8 8.16 12.31 8.39\n#>  $ : num [1:5] -15.872 -13.3 12.141 0.469 14.794\n\nmap2() generates this series of function calls:\n\n\n\n\n\nNote that the arguments that vary for each call come before the function; arguments that are the same for every call come after.\nLike map(), map2() is just a wrapper around a for loop:\n\nmap2 <- function(x, y, f, ...) {\n  out <- vector(\"list\", length(x))\n  for (i in seq_along(x)) {\n    out[[i]] <- f(x[[i]], y[[i]], ...)\n  }\n  out\n}\n\nYou could also imagine map3(), map4(), map5(), map6() etc, but that would get tedious quickly. Instead, purrr provides pmap() which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples:\n\nn <- list(1, 3, 5)\nargs1 <- list(n, mu, sigma)\nargs1 |>\n  pmap(rnorm) |> \n  str()\n#> List of 3\n#>  $ : num 5.39\n#>  $ : num [1:3] 5.41 2.08 9.58\n#>  $ : num [1:5] -23.85 -2.96 -6.56 8.46 -5.21\n\nThat looks like:\n\n\n\n\n\nIf you don’t name the list’s elements, pmap() will use positional matching when calling the function. That’s a little fragile, and makes the code harder to read, so it’s better to name the arguments:\n\nargs2 <- list(mean = mu, sd = sigma, n = n)\nargs2 |> \n  pmap(rnorm) |> \n  str()\n\nThat generates longer, but safer, calls:\n\n\n\n\n\nSince the arguments are all the same length, it makes sense to store them in a data frame:\n\nparams <- tribble(\n  ~mean, ~sd, ~n,\n    5,     1,  1,\n   10,     5,  3,\n   -3,    10,  5\n)\nparams |> \n  pmap(rnorm)\n#> [[1]]\n#> [1] 6.018179\n#> \n#> [[2]]\n#> [1]  8.681404 18.292712  6.129566\n#> \n#> [[3]]\n#> [1] -12.239379  -5.755334  -8.933997  -4.222859   8.797842\n\nAs soon as your code gets complicated, we think a data frame is a good approach because it ensures that each column has a name and is the same length as all the other columns.\n\n29.7.1 Invoking different functions\nThere’s one more step up in complexity - as well as varying the arguments to the function you might also vary the function itself:\n\nf <- c(\"runif\", \"rnorm\", \"rpois\")\nparam <- list(\n  list(min = -1, max = 1), \n  list(sd = 5), \n  list(lambda = 10)\n)\n\nTo handle this case, you can use invoke_map():\n\ninvoke_map(f, param, n = 5) |> str()\n#> List of 3\n#>  $ : num [1:5] 0.479 0.439 -0.471 0.348 -0.581\n#>  $ : num [1:5] 2.48 3.9 7.54 -9.12 3.94\n#>  $ : int [1:5] 6 11 5 8 9\n\n\n\n\n\n\nThe first argument is a list of functions or character vector of function names. The second argument is a list of lists giving the arguments that vary for each function. The subsequent arguments are passed on to every function.\nAnd again, you can use tribble() to make creating these matching pairs a little easier:\n\nsim <- tribble(\n  ~f,      ~params,\n  \"runif\", list(min = -1, max = 1),\n  \"rnorm\", list(sd = 5),\n  \"rpois\", list(lambda = 10)\n)\nsim |> \n  mutate(sim = invoke_map(f, params, n = 10))"
  },
  {
    "objectID": "iteration.html#sec-walk",
    "href": "iteration.html#sec-walk",
    "title": "29  Iteration",
    "section": "\n29.8 Walk",
    "text": "29.8 Walk\nWalk is an alternative to map that you use when you want to call a function for its side effects, rather than for its return value. You typically do this because you want to render output to the screen or save files to disk - the important thing is the action, not the return value. Here’s a very simple example:\n\nx <- list(1, \"a\", 3)\n\nx |> \n  walk(print)\n#> [1] 1\n#> [1] \"a\"\n#> [1] 3\n\nwalk() is generally not that useful compared to walk2() or pwalk(). For example, if you had a list of plots and a vector of file names, you could use pwalk() to save each file to the corresponding location on disk:\n\nlibrary(ggplot2)\nplots <- mtcars |> \n  split(.$cyl) |> \n  map(~ggplot(.x, aes(mpg, wt)) + geom_point())\npaths <- stringr::str_c(names(plots), \".pdf\")\n\npwalk(list(paths, plots), ggsave, path = tempdir())\n\nwalk(), walk2() and pwalk() all invisibly return ., the first argument. This makes them suitable for use in the middle of pipelines."
  },
  {
    "objectID": "iteration.html#other-patterns-of-for-loops",
    "href": "iteration.html#other-patterns-of-for-loops",
    "title": "29  Iteration",
    "section": "\n29.9 Other patterns of for loops",
    "text": "29.9 Other patterns of for loops\nPurrr provides a number of other functions that abstract over other types of for loops. You’ll use them less frequently than the map functions, but they’re useful to know about. The goal here is to briefly illustrate each function, so hopefully it will come to mind if you see a similar problem in the future. Then you can go look up the documentation for more details.\n\n29.9.1 Predicate functions\nA number of functions work with predicate functions that return either a single TRUE or FALSE.\nkeep() and discard() keep elements of the input where the predicate is TRUE or FALSE respectively:\n\ngss_cat |> \n  keep(is.factor) |> \n  str()\n#> tibble [21,483 × 6] (S3: tbl_df/tbl/data.frame)\n#>  $ marital: Factor w/ 6 levels \"No answer\",\"Never married\",..: 2 4 5 2 4 6 2 4 6 6 ...\n#>  $ race   : Factor w/ 4 levels \"Other\",\"Black\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ rincome: Factor w/ 16 levels \"No answer\",\"Don't know\",..: 8 8 16 16 16 5 4 9 4 4 ...\n#>  $ partyid: Factor w/ 10 levels \"No answer\",\"Don't know\",..: 6 5 7 6 9 10 5 8 9 4 ...\n#>  $ relig  : Factor w/ 16 levels \"No answer\",\"Don't know\",..: 15 15 15 6 12 15 5 15 15 15 ...\n#>  $ denom  : Factor w/ 30 levels \"No answer\",\"Don't know\",..: 25 23 3 30 30 25 30 15 4 25 ...\n\ngss_cat |> \n  discard(is.factor) |> \n  str()\n#> tibble [21,483 × 3] (S3: tbl_df/tbl/data.frame)\n#>  $ year   : int [1:21483] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n#>  $ age    : int [1:21483] 26 48 67 39 25 25 36 44 44 47 ...\n#>  $ tvhours: int [1:21483] 12 NA 2 4 1 NA 3 NA 0 3 ...\n\nsome() and every() determine if the predicate is true for any or for all of the elements.\n\nx <- list(1:5, letters, list(10))\n\nx |> \n  some(is_character)\n#> [1] TRUE\n\nx |> \n  every(is_vector)\n#> [1] TRUE\n\ndetect() finds the first element where the predicate is true; detect_index() returns its position.\n\nx <- sample(10)\nx\n#>  [1] 10  6  1  3  2  4  5  8  9  7\n\nx |> \n  detect(~ .x > 5)\n#> [1] 10\n\nx |> \n  detect_index(~ .x > 5)\n#> [1] 1\n\nhead_while() and tail_while() take elements from the start or end of a vector while a predicate is true:\n\nx |> \n  head_while(~ .x > 5)\n#> [1] 10  6\n\nx |> \n  tail_while(~ .x > 5)\n#> [1] 8 9 7\n\n\n29.9.2 Reduce and accumulate\nSometimes you have a complex list that you want to reduce to a simple list by repeatedly applying a function that reduces a pair to a singleton. This is useful if you want to apply a two-table dplyr verb to multiple tables. For example, you might have a list of data frames, and you want to reduce to a single data frame by joining the elements together:\n\ndfs <- list(\n  age = tibble(name = \"John\", age = 30),\n  sex = tibble(name = c(\"John\", \"Mary\"), sex = c(\"M\", \"F\")),\n  trt = tibble(name = \"Mary\", treatment = \"A\")\n)\n\ndfs |> reduce(full_join)\n#> Joining, by = \"name\"\n#> Joining, by = \"name\"\n#> # A tibble: 2 × 4\n#>   name    age sex   treatment\n#>   <chr> <dbl> <chr> <chr>    \n#> 1 John     30 M     <NA>     \n#> 2 Mary     NA F     A\n\nOr maybe you have a list of vectors, and want to find the intersection:\n\nvs <- list(\n  c(1, 3, 5, 6, 10),\n  c(1, 2, 3, 7, 8, 10),\n  c(1, 2, 3, 4, 8, 9, 10)\n)\n\nvs |> reduce(intersect)\n#> [1]  1  3 10\n\nreduce() takes a “binary” function (i.e. a function with two primary inputs), and applies it repeatedly to a list until there is only a single element left.\naccumulate() is similar but it keeps all the interim results. You could use it to implement a cumulative sum:\n\nx <- sample(10)\nx\n#>  [1]  7  5 10  9  8  3  1  4  2  6\nx |> accumulate(`+`)\n#>  [1]  7 12 22 31 39 42 43 47 49 55\n\n\n29.9.3 Exercises\n\nImplement your own version of every() using a for loop. Compare it with purrr::every(). What does purrr’s version do that your version doesn’t?\nCreate an enhanced col_summary() that applies a summary function to every numeric column in a data frame.\n\nA possible base R equivalent of col_summary() is:\n\ncol_sum3 <- function(df, f) {\n  is_num <- sapply(df, is.numeric)\n  df_num <- df[, is_num]\n\n  sapply(df_num, f)\n}\n\nBut it has a number of bugs as illustrated with the following inputs:\n\ndf <- tibble(\n  x = 1:3, \n  y = 3:1,\n  z = c(\"a\", \"b\", \"c\")\n)\n# OK\ncol_sum3(df, mean)\n# Has problems: don't always return numeric vector\ncol_sum3(df[1:2], mean)\ncol_sum3(df[1], mean)\ncol_sum3(df[0], mean)\n\nWhat causes the bugs?"
  },
  {
    "objectID": "iteration.html#case-study",
    "href": "iteration.html#case-study",
    "title": "29  Iteration",
    "section": "\n29.10 Case study",
    "text": "29.10 Case study"
  },
  {
    "objectID": "prog-strings.html",
    "href": "prog-strings.html",
    "title": "30  Programming with strings",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it. You can find the complete first edition at https://r4ds.had.co.nz."
  },
  {
    "objectID": "prog-strings.html#performance",
    "href": "prog-strings.html#performance",
    "title": "30  Programming with strings",
    "section": "\n30.1 Performance",
    "text": "30.1 Performance\nfixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. The following microbenchmark shows that it’s about 3x faster for a simple example.\n\nmicrobenchmark::microbenchmark(\n  fixed = str_detect(sentences, fixed(\"the\")),\n  regex = str_detect(sentences, \"the\"),\n  times = 20\n)\n#> Unit: microseconds\n#>   expr   min    lq    mean median     uq    max neval\n#>  fixed  93.7  98.0 282.165  107.2 147.85 3192.0    20\n#>  regex 368.5 372.6 399.355  375.4 400.75  568.3    20\n\nAs you saw with str_split() you can use boundary() to match boundaries. You can also use it with the other functions:\n\nx <- \"This is a sentence.\"\nstr_view_all(x, boundary(\"word\"))\n\n\n\n\nstr_extract_all(x, boundary(\"word\"))\n#> [[1]]\n#> [1] \"This\"     \"is\"       \"a\"        \"sentence\"\n\n\n30.1.1 Extract\n\ncolours <- c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\")\ncolour_match <- str_c(colours, collapse = \"|\")\ncolour_match\n#> [1] \"red|orange|yellow|green|blue|purple\"\n\nmore <- sentences[str_count(sentences, colour_match) > 1]\nstr_extract_all(more, colour_match)\n#> [[1]]\n#> [1] \"blue\" \"red\" \n#> \n#> [[2]]\n#> [1] \"green\" \"red\"  \n#> \n#> [[3]]\n#> [1] \"orange\" \"red\"\n\nIf you use simplify = TRUE, str_extract_all() will return a matrix with short matches expanded to the same length as the longest:\n\n\nstr_extract_all(more, colour_match, simplify = TRUE)\n#>      [,1]     [,2] \n#> [1,] \"blue\"   \"red\"\n#> [2,] \"green\"  \"red\"\n#> [3,] \"orange\" \"red\"\n\nx <- c(\"a\", \"a b\", \"a b c\")\nstr_extract_all(x, \"[a-z]\", simplify = TRUE)\n#>      [,1] [,2] [,3]\n#> [1,] \"a\"  \"\"   \"\"  \n#> [2,] \"a\"  \"b\"  \"\"  \n#> [3,] \"a\"  \"b\"  \"c\"\n\nWe don’t talk about matrices here, but they are useful elsewhere.\n\n30.1.2 Exercises\n\n\nFrom the Harvard sentences data, extract:\n\nThe first word from each sentence.\nAll words ending in ing.\nAll plurals."
  },
  {
    "objectID": "prog-strings.html#grouped-matches",
    "href": "prog-strings.html#grouped-matches",
    "title": "30  Programming with strings",
    "section": "\n30.2 Grouped matches",
    "text": "30.2 Grouped matches\nEarlier in this chapter we talked about the use of parentheses for clarifying precedence and for backreferences when matching. You can also use parentheses to extract parts of a complex match. For example, imagine we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here we use a simple approximation: a sequence of at least one character that isn’t a space.\n\nnoun <- \"(a|the) ([^ ]+)\"\n\nhas_noun <- sentences |>\n  str_subset(noun) |>\n  head(10)\nhas_noun |> \n  str_extract(noun)\n#>  [1] \"the smooth\" \"the sheet\"  \"the depth\"  \"a chicken\"  \"the parked\"\n#>  [6] \"the sun\"    \"the huge\"   \"the ball\"   \"the woman\"  \"a helps\"\n\nstr_extract() gives us the complete match; str_match() gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group:\n\nhas_noun |> \n  str_match(noun)\n#>       [,1]         [,2]  [,3]     \n#>  [1,] \"the smooth\" \"the\" \"smooth\" \n#>  [2,] \"the sheet\"  \"the\" \"sheet\"  \n#>  [3,] \"the depth\"  \"the\" \"depth\"  \n#>  [4,] \"a chicken\"  \"a\"   \"chicken\"\n#>  [5,] \"the parked\" \"the\" \"parked\" \n#>  [6,] \"the sun\"    \"the\" \"sun\"    \n#>  [7,] \"the huge\"   \"the\" \"huge\"   \n#>  [8,] \"the ball\"   \"the\" \"ball\"   \n#>  [9,] \"the woman\"  \"the\" \"woman\"  \n#> [10,] \"a helps\"    \"a\"   \"helps\"\n\n(Unsurprisingly, our heuristic for detecting nouns is poor, and also picks up adjectives like smooth and parked.)"
  },
  {
    "objectID": "prog-strings.html#splitting",
    "href": "prog-strings.html#splitting",
    "title": "30  Programming with strings",
    "section": "\n30.3 Splitting",
    "text": "30.3 Splitting\nUse str_split() to split a string up into pieces. For example, we could split sentences into words:\n\nsentences |>\n  head(5) |> \n  str_split(\" \")\n#> [[1]]\n#> [1] \"The\"     \"birch\"   \"canoe\"   \"slid\"    \"on\"      \"the\"     \"smooth\" \n#> [8] \"planks.\"\n#> \n#> [[2]]\n#> [1] \"Glue\"        \"the\"         \"sheet\"       \"to\"          \"the\"        \n#> [6] \"dark\"        \"blue\"        \"background.\"\n#> \n#> [[3]]\n#> [1] \"It's\"  \"easy\"  \"to\"    \"tell\"  \"the\"   \"depth\" \"of\"    \"a\"     \"well.\"\n#> \n#> [[4]]\n#> [1] \"These\"   \"days\"    \"a\"       \"chicken\" \"leg\"     \"is\"      \"a\"      \n#> [8] \"rare\"    \"dish.\"  \n#> \n#> [[5]]\n#> [1] \"Rice\"   \"is\"     \"often\"  \"served\" \"in\"     \"round\"  \"bowls.\"\n\nBecause each component might contain a different number of pieces, this returns a list. If you’re working with a length-1 vector, the easiest thing is to just extract the first element of the list:\n\nstr_split(\"a|b|c|d\", \"\\\\|\")[[1]]\n#> [1] \"a\" \"b\" \"c\" \"d\"\n\nOtherwise, like the other stringr functions that return a list, you can use simplify = TRUE to return a matrix:\n\nsentences |>\n  head(5) |> \n  str_split(\" \", simplify = TRUE)\n#>      [,1]    [,2]    [,3]    [,4]      [,5]  [,6]    [,7]     [,8]         \n#> [1,] \"The\"   \"birch\" \"canoe\" \"slid\"    \"on\"  \"the\"   \"smooth\" \"planks.\"    \n#> [2,] \"Glue\"  \"the\"   \"sheet\" \"to\"      \"the\" \"dark\"  \"blue\"   \"background.\"\n#> [3,] \"It's\"  \"easy\"  \"to\"    \"tell\"    \"the\" \"depth\" \"of\"     \"a\"          \n#> [4,] \"These\" \"days\"  \"a\"     \"chicken\" \"leg\" \"is\"    \"a\"      \"rare\"       \n#> [5,] \"Rice\"  \"is\"    \"often\" \"served\"  \"in\"  \"round\" \"bowls.\" \"\"           \n#>      [,9]   \n#> [1,] \"\"     \n#> [2,] \"\"     \n#> [3,] \"well.\"\n#> [4,] \"dish.\"\n#> [5,] \"\"\n\nYou can also request a maximum number of pieces:\n\nfields <- c(\"Name: Hadley\", \"Country: NZ\", \"Age: 35\")\nfields |> str_split(\": \", n = 2, simplify = TRUE)\n#>      [,1]      [,2]    \n#> [1,] \"Name\"    \"Hadley\"\n#> [2,] \"Country\" \"NZ\"    \n#> [3,] \"Age\"     \"35\"\n\nInstead of splitting up strings by patterns, you can also split up by character, line, sentence and word boundary()s:\n\nx <- \"This is a sentence.  This is another sentence.\"\nstr_view_all(x, boundary(\"word\"))\n\n\n\n\n\nstr_split(x, \" \")[[1]]\n#> [1] \"This\"      \"is\"        \"a\"         \"sentence.\" \"\"          \"This\"     \n#> [7] \"is\"        \"another\"   \"sentence.\"\nstr_split(x, boundary(\"word\"))[[1]]\n#> [1] \"This\"     \"is\"       \"a\"        \"sentence\" \"This\"     \"is\"       \"another\" \n#> [8] \"sentence\"\n\nShow how separate_rows() is a special case of str_split() + summarise()."
  },
  {
    "objectID": "prog-strings.html#replace-with-function",
    "href": "prog-strings.html#replace-with-function",
    "title": "30  Programming with strings",
    "section": "\n30.4 Replace with function",
    "text": "30.4 Replace with function"
  },
  {
    "objectID": "prog-strings.html#locations",
    "href": "prog-strings.html#locations",
    "title": "30  Programming with strings",
    "section": "\n30.5 Locations",
    "text": "30.5 Locations\nstr_locate() and str_locate_all() give you the starting and ending positions of each match. These are particularly useful when none of the other functions does exactly what you want. You can use str_locate() to find the matching pattern, str_sub() to extract and/or modify them."
  },
  {
    "objectID": "prog-strings.html#stringi",
    "href": "prog-strings.html#stringi",
    "title": "30  Programming with strings",
    "section": "\n30.6 stringi",
    "text": "30.6 stringi\nstringr is built on top of the stringi package. stringr is useful when you’re learning because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions. stringi, on the other hand, is designed to be comprehensive. It contains almost every function you might ever need: stringi has 256 functions to stringr’s 49.\nIf you find yourself struggling to do something in stringr, it’s worth taking a look at stringi. The packages work very similarly, so you should be able to translate your stringr knowledge in a natural way. The main difference is the prefix: str_ vs. stri_.\n\n30.6.1 Exercises\n\n\nFind the stringi functions that:\n\nCount the number of words.\nFind duplicated strings.\nGenerate random text.\n\n\nHow do you control the language that stri_sort() uses for sorting?\n\n30.6.2 Exercises\n\n\nWhat do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets.\n\ntibble(x = c(\"a,b,c\", \"d,e,f,g\", \"h,i,j\")) |>\n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) |>\n  separate(x, c(\"one\", \"two\", \"three\"))\n\n\nBoth unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\nCompare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite?\n\nIn the following example we’re using unite() to create a date column from month and day columns. How would you achieve the same outcome using mutate() and paste() instead of unite?\n\nevents <- tribble(\n  ~month, ~day,\n  1     , 20,\n  1     , 21,\n  1     , 22\n)\n\nevents |>\n  unite(\"date\", month:day, sep = \"-\", remove = FALSE)\n\n\nWrite a function that turns (e.g.) a vector c(\"a\", \"b\", \"c\") into the string a, b, and c. Think carefully about what it should do if given a vector of length 0, 1, or 2."
  },
  {
    "objectID": "communicate.html",
    "href": "communicate.html",
    "title": "Communicate",
    "section": "",
    "text": "Communication is the theme of the following four chapters:\n\nIn [R Markdown], you will learn about R Markdown, a tool for integrating prose, code, and results. You can use R Markdown in notebook mode for analyst-to-analyst communication, and in report mode for analyst-to-decision-maker communication. Thanks to the power of R Markdown formats, you can even use the same document for both purposes.\nIn [Graphics for communication], you will learn how to take your exploratory graphics and turn them into expository graphics, graphics that help the newcomer to your analysis understand what’s going on as quickly and easily as possible.\nIn [R Markdown formats], you’ll learn a little about the many other varieties of outputs you can produce using R Markdown, including dashboards, websites, and books.\nWe’ll finish up with [R Markdown workflow], where you’ll learn about the “analysis notebook” and how to systematically record your successes and failures so that you can learn from them.\n\nUnfortunately, these chapters focus mostly on the technical mechanics of communication, not the really hard problems of communicating your thoughts to other humans. However, there are lot of other great books about communication, which we’ll point you to at the end of each chapter."
  },
  {
    "objectID": "rmarkdown.html",
    "href": "rmarkdown.html",
    "title": "31  R Markdown",
    "section": "",
    "text": "R Markdown provides a unified authoring framework for data science, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.\nR Markdown files are designed to be used in three ways:\n\nFor communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.\nFor collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e. the code).\nAs an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.\n\nR Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand:\n\nR Markdown Cheat Sheet: Help > Cheatsheets > R Markdown Cheat Sheet,\nR Markdown Reference Guide: Help > Cheatsheets > R Markdown Reference Guide.\n\nBoth cheatsheets are also available at https://rstudio.com/resources/cheatsheets/.\n\nYou need the rmarkdown package, but you don’t need to explicitly install it or load it, as RStudio automatically does both when needed.\n\nchunk <- \"```\"\ninline <- function(x = \"\") paste0(\"`` `r \", x, \"` ``\")\nlibrary(tidyverse)"
  },
  {
    "objectID": "rmarkdown.html#r-markdown-basics",
    "href": "rmarkdown.html#r-markdown-basics",
    "title": "31  R Markdown",
    "section": "\n31.2 R Markdown basics",
    "text": "31.2 R Markdown basics\nThis is an R Markdown file, a plain text file that has the extension .Rmd:\n\n---\ntitle: \"Diamond sizes\"\ndate: 2016-08-25\noutput: html_document\n---\n\n```{r setup, include = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsmaller <- diamonds |> \n  filter(carat <= 2.5)\n```\n\nWe have data about `r nrow(diamonds)` diamonds. Only \n`r nrow(diamonds) - nrow(smaller)` are larger than\n2.5 carats. The distribution of the remainder is shown\nbelow:\n\n```{r, echo = FALSE}\nsmaller |> \n  ggplot(aes(carat)) + \n  geom_freqpoly(binwidth = 0.01)\n```\n\nIt contains three important types of content:\n\nAn (optional) YAML header surrounded by ---s.\n\nChunks of R code surrounded by ```.\nText mixed with simple text formatting like # heading and _italics_.\n\nWhen you open an .Rmd, you get a notebook interface where code and output are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter. RStudio executes the code and displays the results inline with the code:\n\n\n\n\n\nTo produce a complete report containing all text, code, and results, click “Knit” or press Cmd/Ctrl + Shift + K. You can also do this programmatically with rmarkdown::render(\"1-example.Rmd\"). This will display the report in the viewer pane, and create a self-contained HTML file that you can share with others.\n\n\n\n\n\nWhen you knit the document, R Markdown sends the .Rmd file to knitr, http://yihui.name/knitr/, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output. The markdown file generated by knitr is then processed by pandoc, http://pandoc.org/, which is responsible for creating the finished file. The advantage of this two step workflow is that you can create a very wide range of output formats, as you’ll learn about in [R Markdown formats].\n\n\n\n\n\nTo get started with your own .Rmd file, select File > New File > R Markdown… in the menubar. RStudio will launch a wizard that you can use to pre-populate your file with useful content that reminds you how the key features of R Markdown work.\nThe following sections dive into the three components of an R Markdown document in more details: the markdown text, the code chunks, and the YAML header.\n\n31.2.1 Exercises\n\nCreate a new notebook using File > New File > R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output.\nCreate a new R Markdown document with File > New File > R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update.\nCompare and contrast the R notebook and R Markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other?\nCreate one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.)"
  },
  {
    "objectID": "rmarkdown.html#text-formatting-with-markdown",
    "href": "rmarkdown.html#text-formatting-with-markdown",
    "title": "31  R Markdown",
    "section": "\n31.3 Text formatting with Markdown",
    "text": "31.3 Text formatting with Markdown\nProse in .Rmd files is written in Markdown, a lightweight set of conventions for formatting plain text files. Markdown is designed to be easy to read and easy to write. It is also very easy to learn. The guide below shows how to use Pandoc’s Markdown, a slightly extended version of Markdown that R Markdown understands.\n\nText formatting \n------------------------------------------------------------\n\n*italic*  or _italic_\n**bold**   __bold__\n`code`\nsuperscript^2^ and subscript~2~\n\nHeadings\n------------------------------------------------------------\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n------------------------------------------------------------\n\n*   Bulleted list item 1\n\n*   Item 2\n\n    * Item 2a\n\n    * Item 2b\n\n1.  Numbered list item 1\n\n1.  Item 2. The numbers are incremented automatically in the output.\n\nLinks and images\n------------------------------------------------------------\n\n<http://example.com>\n\n[linked phrase](http://example.com)\n\n![optional caption text](path/to/img.png)\n\nTables \n------------------------------------------------------------\n\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell\n\nThe best way to learn these is simply to try them out. It will take a few days, but soon they will become second nature, and you won’t need to think about them. If you forget, you can get to a handy reference sheet with Help > Markdown Quick Reference.\n\n31.3.1 Exercises\n\nPractice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold.\n\nUsing the R Markdown quick reference, figure out how to:\n\nAdd a footnote.\nAdd a horizontal rule.\nAdd a block quote.\n\n\nCopy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/main/rmarkdown in to a local R Markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features."
  },
  {
    "objectID": "rmarkdown.html#code-chunks",
    "href": "rmarkdown.html#code-chunks",
    "title": "31  R Markdown",
    "section": "\n31.4 Code chunks",
    "text": "31.4 Code chunks\nTo run code inside an R Markdown document, you need to insert a chunk. There are three ways to do so:\n\nThe keyboard shortcut Cmd/Ctrl + Alt + I\nThe “Insert” button icon in the editor toolbar.\nBy manually typing the chunk delimiters ```{r} and ```.\n\nObviously, we’d recommend you learn the keyboard shortcut. It will save you a lot of time in the long run!\nYou can continue to run the code using the keyboard shortcut that by now (we hope!) you know and love: Cmd/Ctrl + Enter. However, chunks get a new keyboard shortcut: Cmd/Ctrl + Shift + Enter, which runs all the code in the chunk. Think of a chunk like a function. A chunk should be relatively self-contained, and focussed around a single task.\nThe following sections describe the chunk header which consists of ```{r, followed by an optional chunk name, followed by comma separated options, followed by }. Next comes your R code and the chunk end is indicated by a final ```.\n\n31.4.1 Chunk name\nChunks can be given an optional name: ```{r by-name}. This has three advantages:\n\n\nYou can more easily navigate to specific chunks using the drop-down code navigator in the bottom-left of the script editor:\n\n\n\n\n\n\nGraphics produced by the chunks will have useful names that make them easier to use elsewhere. More on that in [other important options].\nYou can set up networks of cached chunks to avoid re-performing expensive computations on every run. More on that below.\n\nThere is one chunk name that imbues special behaviour: setup. When you’re in a notebook mode, the chunk named setup will be run automatically once, before any other code is run.\n\n31.4.2 Chunk options\nChunk output can be customised with options, arguments supplied to chunk header. Knitr provides almost 60 options that you can use to customize your code chunks. Here we’ll cover the most important chunk options that you’ll use frequently. You can see the full list at http://yihui.name/knitr/options/.\nThe most important set of options controls if your code block is executed and what results are inserted in the finished report:\n\neval = FALSE prevents code from being evaluated. (And obviously if the code is not run, no results will be generated). This is useful for displaying example code, or for disabling a large block of code without commenting each line.\ninclude = FALSE runs the code, but doesn’t show the code or results in the final document. Use this for setup code that you don’t want cluttering your report.\necho = FALSE prevents code, but not the results from appearing in the finished file. Use this when writing reports aimed at people who don’t want to see the underlying R code.\nmessage = FALSE or warning = FALSE prevents messages or warnings from appearing in the finished file.\nresults = 'hide' hides printed output; fig.show = 'hide' hides plots.\nerror = TRUE causes the render to continue even if code returns an error. This is rarely something you’ll want to include in the final version of your report, but can be very useful if you need to debug exactly what is going on inside your .Rmd. It’s also useful if you’re teaching R and want to deliberately include an error. The default, error = FALSE causes knitting to fail if there is a single error in the document.\n\nThe following table summarises which types of output each option suppresses:\n\n\n\n\n\n\n\n\n\n\n\nOption\nRun code\nShow code\nOutput\nPlots\nMessages\nWarnings\n\n\n\neval = FALSE\n-\n\n-\n-\n-\n-\n\n\ninclude = FALSE\n\n-\n-\n-\n-\n-\n\n\necho = FALSE\n\n-\n\n\n\n\n\n\nresults = \"hide\"\n\n\n-\n\n\n\n\n\nfig.show = \"hide\"\n\n\n\n-\n\n\n\n\nmessage = FALSE\n\n\n\n\n-\n\n\n\nwarning = FALSE\n\n\n\n\n\n-\n\n\n\n31.4.3 Table\nBy default, R Markdown prints data frames and matrices as you’d see them in the console:\n\nmtcars[1:5, ]\n#>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\nIf you prefer that data be displayed with additional formatting you can use the knitr::kable function. The code below generates Table 31.1.\n\nknitr::kable(mtcars[1:5, ], )\n\n\n\nTable 31.1: A knitr kable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\nRead the documentation for ?knitr::kable to see the other ways in which you can customize the table. For even deeper customization, consider the xtable, stargazer, pander, tables, and ascii packages. Each provides a set of tools for returning formatted tables from R code.\nThere is also a rich set of options for controlling how figures are embedded. You’ll learn about these in [saving your plots].\n\n31.4.4 Caching\nNormally, each knit of a document starts from a completely clean slate. This is great for reproducibility, because it ensures that you’ve captured every important computation in code. However, it can be painful if you have some computations that take a long time. The solution is cache = TRUE. When set, this will save the output of the chunk to a specially named file on disk. On subsequent runs, knitr will check to see if the code has changed, and if it hasn’t, it will reuse the cached results.\nThe caching system must be used with care, because by default it is based on the code only, not its dependencies. For example, here the processed_data chunk depends on the raw_data chunk:\n```{r raw_data}\nrawdata <- readr::read_csv(\"a_very_large_file.csv\")\n```\n\n```{r processed_data, cache = TRUE}\nprocessed_data <- rawdata |> \n  filter(!is.na(import_var)) |> \n  mutate(new_variable = complicated_transformation(x, y, z))\n```\nCaching the processed_data chunk means that it will get re-run if the dplyr pipeline is changed, but it won’t get rerun if the read_csv() call changes. You can avoid that problem with the dependson chunk option:\n```{r processed_data, cache = TRUE, dependson = \"raw_data\"}\nprocessed_data <- rawdata |> \n  filter(!is.na(import_var)) |> \n  mutate(new_variable = complicated_transformation(x, y, z))\n```\ndependson should contain a character vector of every chunk that the cached chunk depends on. Knitr will update the results for the cached chunk whenever it detects that one of its dependencies have changed.\nNote that the chunks won’t update if a_very_large_file.csv changes, because knitr caching only tracks changes within the .Rmd file. If you want to also track changes to that file you can use the cache.extra option. This is an arbitrary R expression that will invalidate the cache whenever it changes. A good function to use is file.info(): it returns a bunch of information about the file including when it was last modified. Then you can write:\n```{r raw_data, cache.extra = file.info(\"a_very_large_file.csv\")}\nrawdata <- readr::read_csv(\"a_very_large_file.csv\")\n```\nAs your caching strategies get progressively more complicated, it’s a good idea to regularly clear out all your caches with knitr::clean_cache().\nWe’ve follow the advice of David Robinson to name these chunks: each chunk is named after the primary object that it creates. This makes it easier to understand the dependson specification.\n\n31.4.5 Global options\nAs you work more with knitr, you will discover that some of the default chunk options don’t fit your needs and you want to change them. You can do this by calling knitr::opts_chunk$set() in a code chunk. For example, when writing books and tutorials we set:\n\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  collapse = TRUE\n)\n\nThis uses my preferred comment formatting, and ensures that the code and output are kept closely entwined. On the other hand, if you were preparing a report, you might set:\n\nknitr::opts_chunk$set(\n  echo = FALSE\n)\n\nThat will hide the code by default, so only showing the chunks you deliberately choose to show (with echo = TRUE). You might consider setting message = FALSE and warning = FALSE, but that would make it harder to debug problems because you wouldn’t see any messages in the final document.\n\n31.4.6 Inline code\nThere is one other way to embed R code into an R Markdown document: directly into the text, with: `r `. This can be very useful if you mention properties of your data in the text. For example, the example document used at the start of the chapter had:\n\nWe have data about `r nrow(diamonds)` diamonds. Only `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats. The distribution of the remainder is shown below:\n\nWhen the report is knit, the results of these computations are inserted into the text:\n\nWe have data about 53940 diamonds. Only 126 are larger than 2.5 carats. The distribution of the remainder is shown below:\n\nWhen inserting numbers into text, format() is your friend. It allows you to set the number of digits so you don’t print to a ridiculous degree of accuracy, and a big.mark to make numbers easier to read. You might combine these into a helper function:\n\ncomma <- function(x) format(x, digits = 2, big.mark = \",\")\ncomma(3452345)\n#> [1] \"3,452,345\"\ncomma(.12358124331)\n#> [1] \"0.12\"\n\n\n31.4.7 Exercises\n\nAdd a section that explores how diamond sizes vary by cut, colour, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option.\nDownload diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/main/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes.\nModify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats.\nSet up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching."
  },
  {
    "objectID": "rmarkdown.html#troubleshooting",
    "href": "rmarkdown.html#troubleshooting",
    "title": "31  R Markdown",
    "section": "\n31.5 Troubleshooting",
    "text": "31.5 Troubleshooting\nTroubleshooting R Markdown documents can be challenging because you are no longer in an interactive R environment, and you will need to learn some new tricks. The first thing you should always try is to recreate the problem in an interactive session. Restart R, then “Run all chunks” (either from Code menu, under Run region), or with the keyboard shortcut Ctrl + Alt + R. If you’re lucky, that will recreate the problem, and you can figure out what’s going on interactively.\nIf that doesn’t help, there must be something different between your interactive environment and the R Markdown environment. You’re going to need to systematically explore the options. The most common difference is the working directory: the working directory of an R Markdown is the directory in which it lives. Check the working directory is what you expect by including getwd() in a chunk.\nNext, brainstorm all the things that might cause the bug. You’ll need to systematically check that they’re the same in your R session and your R Markdown session. The easiest way to do that is to set error = TRUE on the chunk causing the problem, then use print() and str() to check that settings are as you expect."
  },
  {
    "objectID": "rmarkdown.html#yaml-header",
    "href": "rmarkdown.html#yaml-header",
    "title": "31  R Markdown",
    "section": "\n31.6 YAML header",
    "text": "31.6 YAML header\nYou can control many other “whole document” settings by tweaking the parameters of the YAML header. You might wonder what YAML stands for: it’s “yet another markup language”, which is designed for representing hierarchical data in a way that’s easy for humans to read and write. R Markdown uses it to control many details of the output. Here we’ll discuss two: document parameters and bibliographies.\n\n31.6.1 Parameters\nR Markdown documents can include one or more parameters whose values can be set when you render the report. Parameters are useful when you want to re-render the same report with distinct values for various key inputs. For example, you might be producing sales reports per branch, exam results by student, or demographic summaries by country. To declare one or more parameters, use the params field.\nThis example uses a my_class parameter to determine which class of cars to display:\n\n---\noutput: html_document\nparams:\n  my_class: \"suv\"\n---\n\n```{r setup, include = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\n\nclass <- mpg |> filter(class == params$my_class)\n```\n\n# Fuel economy for `r params$my_class`s\n\n```{r, message = FALSE}\nggplot(class, aes(displ, hwy)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n```\n\n\nAs you can see, parameters are available within the code chunks as a read-only list named params.\nYou can write atomic vectors directly into the YAML header. You can also run arbitrary R expressions by prefacing the parameter value with !r. This is a good way to specify date/time parameters.\nparams:\n  start: !r lubridate::ymd(\"2015-01-01\")\n  snapshot: !r lubridate::ymd_hms(\"2015-01-01 12:30:00\")\nIn RStudio, you can click the “Knit with Parameters” option in the Knit dropdown menu to set parameters, render, and preview the report in a single user friendly step. You can customise the dialog by setting other options in the header. See http://rmarkdown.rstudio.com/developer_parameterized_reports.html#parameter_user_interfaces for more details.\nAlternatively, if you need to produce many such parameterised reports, you can call rmarkdown::render() with a list of params:\n\nrmarkdown::render(\"fuel-economy.Rmd\", params = list(my_class = \"suv\"))\n\nThis is particularly powerful in conjunction with purrr:pwalk(). The following example creates a report for each value of class found in mpg. First we create a data frame that has one row for each class, giving the filename of the report and the params:\n\nreports <- tibble(\n  class = unique(mpg$class),\n  filename = stringr::str_c(\"fuel-economy-\", class, \".html\"),\n  params = purrr::map(class, ~ list(my_class = .))\n)\nreports\n#> # A tibble: 7 × 3\n#>   class   filename                  params          \n#>   <chr>   <chr>                     <list>          \n#> 1 compact fuel-economy-compact.html <named list [1]>\n#> 2 midsize fuel-economy-midsize.html <named list [1]>\n#> 3 suv     fuel-economy-suv.html     <named list [1]>\n#> 4 2seater fuel-economy-2seater.html <named list [1]>\n#> 5 minivan fuel-economy-minivan.html <named list [1]>\n#> 6 pickup  fuel-economy-pickup.html  <named list [1]>\n#> # … with 1 more row\n#> # ℹ Use `print(n = ...)` to see more rows\n\nThen we match the column names to the argument names of render(), and use purrr’s parallel walk to call render() once for each row:\n\nreports |> \n  select(output_file = filename, params) |> \n  purrr::pwalk(rmarkdown::render, input = \"fuel-economy.Rmd\")\n\n\n31.6.2 Bibliographies and Citations\nPandoc can automatically generate citations and a bibliography in a number of styles. To use this feature, specify a bibliography file using the bibliography field in your file’s header. The field should contain a path from the directory that contains your .Rmd file to the file that contains the bibliography file:\nbibliography: rmarkdown.bib\nYou can use many common bibliography formats including BibLaTeX, BibTeX, endnote, medline.\nTo create a citation within your .Rmd file, use a key composed of ‘@’ + the citation identifier from the bibliography file. Then place the citation in square brackets. Here are some examples:\nSeparate multiple citations with a `;`: Blah blah [@smith04; @doe99].\n\nYou can add arbitrary comments inside the square brackets: \nBlah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].\n\nRemove the square brackets to create an in-text citation: @smith04 \nsays blah, or @smith04 [p. 33] says blah.\n\nAdd a `-` before the citation to suppress the author's name: \nSmith says blah [-@smith04].\nWhen R Markdown renders your file, it will build and append a bibliography to the end of your document. The bibliography will contain each of the cited references from your bibliography file, but it will not contain a section heading. As a result it is common practice to end your file with a section header for the bibliography, such as # References or # Bibliography.\nYou can change the style of your citations and bibliography by referencing a CSL (citation style language) file in the csl field:\nbibliography: rmarkdown.bib\ncsl: apa.csl\nAs with the bibliography field, your csl file should contain a path to the file. Here we assume that the csl file is in the same directory as the .Rmd file. A good place to find CSL style files for common bibliography styles is http://github.com/citation-style-language/styles."
  },
  {
    "objectID": "rmarkdown.html#learning-more",
    "href": "rmarkdown.html#learning-more",
    "title": "31  R Markdown",
    "section": "\n31.7 Learning more",
    "text": "31.7 Learning more\nR Markdown is still relatively young, and is still growing rapidly. The best place to stay on top of innovations is the official R Markdown website: http://rmarkdown.rstudio.com.\nThere are two important topics that we haven’t covered here: collaboration, and the details of accurately communicating your ideas to other humans. Collaboration is a vital part of modern data science, and you can make your life much easier by using version control tools, like Git and GitHub. We recommend two free resources that will teach you about Git:\n\n“Happy Git with R”: a user friendly introduction to Git and GitHub from R users, by Jenny Bryan. The book is freely available online: http://happygitwithr.com\nThe “Git and GitHub” chapter of R Packages, by Hadley. You can also read it for free online: http://r-pkgs.had.co.nz/git.html.\n\nWe have not touched on what you should actually write in order to clearly communicate the results of your analysis. To improve your writing, we highly recommend reading either Style: Lessons in Clarity and Grace by Joseph M. Williams & Joseph Bizup, or The Sense of Structure: Writing from the Reader’s Perspective by George Gopen. Both books will help you understand the structure of sentences and paragraphs, and give you the tools to make your writing more clear. (These books are rather expensive if purchased new, but they’re used by many English classes so there are plenty of cheap second-hand copies). George Gopen also has a number of short articles on writing at https://www.georgegopen.com/the-litigation-articles.html. They are aimed at lawyers, but almost everything applies to data scientists too."
  },
  {
    "objectID": "communicate-plots.html",
    "href": "communicate-plots.html",
    "title": "32  Graphics for communication",
    "section": "",
    "text": "In [exploratory data analysis], you learned how to use plots as tools for exploration. When you make exploratory plots, you know—even before looking—which variables the plot will display. You made each plot for a purpose, could quickly look at it, and then move on to the next plot. In the course of most analyses, you’ll produce tens or hundreds of plots, most of which are immediately thrown away.\nNow that you understand your data, you need to communicate your understanding to others. Your audience will likely not share your background knowledge and will not be deeply invested in the data. To help others quickly build up a good mental model of the data, you will need to invest considerable effort in making your plots as self-explanatory as possible. In this chapter, you’ll learn some of the tools that ggplot2 provides to do so.\nThis chapter focuses on the tools you need to create good graphics. We assume that you know what you want, and just need to know how to do it. For that reason, we highly recommend pairing this chapter with a good general visualisation book. We particularly like The Truthful Art, by Albert Cairo. It doesn’t teach the mechanics of creating visualisations, but instead focuses on what you need to think about in order to create effective graphics.\n\nIn this chapter, we’ll focus once again on ggplot2. We’ll also use a little dplyr for data manipulation, and a few ggplot2 extension packages, including ggrepel and viridis. Rather than loading those extensions here, we’ll refer to their functions explicitly, using the :: notation. This will help make it clear which functions are built into ggplot2, and which come from other packages. Don’t forget you’ll need to install those packages with install.packages() if you don’t already have them.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "communicate-plots.html#label",
    "href": "communicate-plots.html#label",
    "title": "32  Graphics for communication",
    "section": "\n32.2 Label",
    "text": "32.2 Label\nThe easiest place to start when turning an exploratory graphic into an expository graphic is with good labels. You add labels with the labs() function. This example adds a plot title:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Fuel efficiency generally decreases with engine size\")\n\n\n\n\nThe purpose of a plot title is to summarise the main finding. Avoid titles that just describe what the plot is, e.g. “A scatterplot of engine displacement vs. fuel economy”.\nIf you need to add more text, there are two other useful labels that you can use in ggplot2 2.2.0 and above:\n\nsubtitle adds additional detail in a smaller font beneath the title.\ncaption adds text at the bottom right of the plot, often used to describe the source of the data.\n\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Fuel efficiency generally decreases with engine size\",\n    subtitle = \"Two seaters (sports cars) are an exception because of their light weight\",\n    caption = \"Data from fueleconomy.gov\"\n  )\n\n\n\n\nYou can also use labs() to replace the axis and legend titles. It’s usually a good idea to replace short variable names with more detailed descriptions, and to include the units.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Engine displacement (L)\",\n    y = \"Highway fuel economy (mpg)\",\n    colour = \"Car type\"\n  )\n\n\n\n\nIt’s possible to use mathematical equations instead of text strings. Just switch \"\" out for quote() and read about the available options in ?plotmath:\n\ndf <- tibble(\n  x = runif(10),\n  y = runif(10)\n)\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    x = quote(sum(x[i] ^ 2, i == 1, n)),\n    y = quote(alpha + beta + frac(delta, theta))\n  )\n\n\n\n\n\n32.2.1 Exercises\n\nCreate one plot on the fuel economy data with customised title, subtitle, caption, x, y, and colour labels.\nThe geom_smooth() is somewhat misleading because the hwy for large engines is skewed upwards due to the inclusion of lightweight sports cars with big engines. Use your modelling tools to fit and display a better model. \nTake an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand."
  },
  {
    "objectID": "communicate-plots.html#annotations",
    "href": "communicate-plots.html#annotations",
    "title": "32  Graphics for communication",
    "section": "\n32.3 Annotations",
    "text": "32.3 Annotations\nIn addition to labelling major components of your plot, it’s often useful to label individual observations or groups of observations. The first tool you have at your disposal is geom_text(). geom_text() is similar to geom_point(), but it has an additional aesthetic: label. This makes it possible to add textual labels to your plots.\nThere are two possible sources of labels. First, you might have a tibble that provides labels. The plot below isn’t terribly useful, but it illustrates a useful approach: pull out the most efficient car in each class with dplyr, and then label it on the plot:\n\nbest_in_class <- mpg |>\n  group_by(class) |>\n  filter(row_number(desc(hwy)) == 1)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_text(aes(label = model), data = best_in_class)\n\n\n\n\nThis is hard to read because the labels overlap with each other, and with the points. We can make things a little better by switching to geom_label() which draws a rectangle behind the text. We also use the nudge_y parameter to move the labels slightly above the corresponding points:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)\n\n\n\n\nThat helps a bit, but if you look closely in the top-left hand corner, you’ll notice that there are two labels practically on top of each other. This happens because the highway mileage and displacement for the best cars in the compact and subcompact categories are exactly the same. There’s no way that we can fix these by applying the same transformation for every label. Instead, we can use the ggrepel package by Kamil Slowikowski. This useful package will automatically adjust labels so that they don’t overlap:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_point(size = 3, shape = 1, data = best_in_class) +\n  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)\n\n\n\n\nNote another handy technique used here: we added a second layer of large, hollow points to highlight the labelled points.\nYou can sometimes use the same idea to replace the legend with labels placed directly on the plot. It’s not wonderful for this plot, but it isn’t too bad. (theme(legend.position = \"none\") turns the legend off — we’ll talk about it more shortly.)\n\nclass_avg <- mpg |>\n  group_by(class) |>\n  summarise(\n    displ = median(displ),\n    hwy = median(hwy)\n  )\n\nggplot(mpg, aes(displ, hwy, colour = class)) +\n  ggrepel::geom_label_repel(aes(label = class),\n    data = class_avg,\n    size = 6,\n    label.size = 0,\n    segment.color = NA\n  ) +\n  geom_point() +\n  theme(legend.position = \"none\")\n\n\n\n\nAlternatively, you might just want to add a single label to the plot, but you’ll still need to create a data frame. Often, you want the label in the corner of the plot, so it’s convenient to create a new data frame using summarise() to compute the maximum values of x and y.\n\nlabel <- mpg |>\n  summarise(\n    displ = max(displ),\n    hwy = max(hwy),\n    label = \"Increasing engine size is \\nrelated to decreasing fuel economy.\"\n  )\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_text(aes(label = label), data = label, vjust = \"top\", hjust = \"right\")\n\n\n\n\nIf you want to place the text exactly on the borders of the plot, you can use +Inf and -Inf. Since we’re no longer computing the positions from mpg, we can use tibble() to create the data frame:\n\nlabel <- tibble(\n  displ = Inf,\n  hwy = Inf,\n  label = \"Increasing engine size is \\nrelated to decreasing fuel economy.\"\n)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_text(aes(label = label), data = label, vjust = \"top\", hjust = \"right\")\n\n\n\n\nIn these examples, we manually broke the label up into lines using \"\\n\". Another approach is to use stringr::str_wrap() to automatically add line breaks, given the number of characters you want per line:\n\n\"Increasing engine size is related to decreasing fuel economy.\" |>\n  stringr::str_wrap(width = 40) |>\n  writeLines()\n#> Increasing engine size is related to\n#> decreasing fuel economy.\n\nNote the use of hjust and vjust to control the alignment of the label. Figure 32.1 shows all nine possible combinations.\n\n\n\n\nFigure 32.1: All nine combinations of hjust and vjust.\n\n\n\n\nRemember, in addition to geom_text(), you have many other geoms in ggplot2 available to help annotate your plot. A few ideas:\n\nUse geom_hline() and geom_vline() to add reference lines. We often make them thick (size = 2) and white (colour = white), and draw them underneath the primary data layer. That makes them easy to see, without drawing attention away from the data.\nUse geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax.\nUse geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.\n\nThe only limit is your imagination (and your patience with positioning annotations to be aesthetically pleasing)!\n\n32.3.1 Exercises\n\nUse geom_text() with infinite positions to place text at the four corners of the plot.\nRead the documentation for annotate(). How can you use it to add a text label to a plot without having to create a tibble?\nHow do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: think about the underlying data.)\nWhat arguments to geom_label() control the appearance of the background box?\nWhat are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options."
  },
  {
    "objectID": "communicate-plots.html#scales",
    "href": "communicate-plots.html#scales",
    "title": "32  Graphics for communication",
    "section": "\n32.4 Scales",
    "text": "32.4 Scales\nThe third way you can make your plot better for communication is to adjust the scales. Scales control the mapping from data values to things that you can perceive. Normally, ggplot2 automatically adds scales for you. For example, when you type:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class))\n\nggplot2 automatically adds default scales behind the scenes:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_colour_discrete()\n\nNote the naming scheme for scales: scale_ followed by the name of the aesthetic, then _, then the name of the scale. The default scales are named according to the type of variable they align with: continuous, discrete, datetime, or date. There are lots of non-default scales which you’ll learn about below.\nThe default scales have been carefully chosen to do a good job for a wide range of inputs. Nevertheless, you might want to override the defaults for two reasons:\n\nYou might want to tweak some of the parameters of the default scale. This allows you to do things like change the breaks on the axes, or the key labels on the legend.\nYou might want to replace the scale altogether, and use a completely different algorithm. Often you can do better than the default because you know more about the data.\n\n\n32.4.1 Axis ticks and legend keys\nThere are two primary arguments that affect the appearance of the ticks on the axes and the keys on the legend: breaks and labels. Breaks controls the position of the ticks, or the values associated with the keys. Labels controls the text label associated with each tick/key. The most common use of breaks is to override the default choice:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  scale_y_continuous(breaks = seq(15, 40, by = 5))\n\n\n\n\nYou can use labels in the same way (a character vector the same length as breaks), but you can also set it to NULL to suppress the labels altogether. This is useful for maps, or for publishing plots where you can’t share the absolute numbers.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL)\n\n\n\n\nYou can also use breaks and labels to control the appearance of legends. Collectively axes and legends are called guides. Axes are used for x and y aesthetics; legends are used for everything else.\nAnother use of breaks is when you have relatively few data points and want to highlight exactly where the observations occur. For example, take this plot that shows when each US president started and ended their term.\n\npresidential |>\n  mutate(id = 33 + row_number()) |>\n  ggplot(aes(start, id)) +\n    geom_point() +\n    geom_segment(aes(xend = end, yend = id)) +\n    scale_x_date(NULL, breaks = presidential$start, date_labels = \"'%y\")\n\n\n\n\nNote that the specification of breaks and labels for date and datetime scales is a little different:\n\ndate_labels takes a format specification, in the same form as parse_datetime().\ndate_breaks (not shown here), takes a string like “2 days” or “1 month”.\n\n32.4.2 Legend layout\nYou will most often use breaks and labels to tweak the axes. While they both also work for legends, there are a few other techniques you are more likely to use.\nTo control the overall position of the legend, you need to use a theme() setting. We’ll come back to themes at the end of the chapter, but in brief, they control the non-data parts of the plot. The theme setting legend.position controls where the legend is drawn:\n\nbase <- ggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class))\n\nbase + theme(legend.position = \"left\")\nbase + theme(legend.position = \"top\")\nbase + theme(legend.position = \"bottom\")\nbase + theme(legend.position = \"right\") # the default\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also use legend.position = \"none\" to suppress the display of the legend altogether.\nTo control the display of individual legends, use guides() along with guide_legend() or guide_colourbar(). The following example shows two important settings: controlling the number of rows the legend uses with nrow, and overriding one of the aesthetics to make the points bigger. This is particularly useful if you have used a low alpha to display many points on a plot.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_smooth(se = FALSE) +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n32.4.3 Replacing a scale\nInstead of just tweaking the details a little, you can instead replace the scale altogether. There are two types of scales you’re mostly likely to want to switch out: continuous position scales and colour scales. Fortunately, the same principles apply to all the other aesthetics, so once you’ve mastered position and colour, you’ll be able to quickly pick up other scale replacements.\nIt’s very useful to plot transformations of your variable. For example, as we’ve seen in diamond prices it’s easier to see the precise relationship between carat and price if we log transform them:\n\nggplot(diamonds, aes(carat, price)) +\n  geom_bin2d()\n\nggplot(diamonds, aes(log10(carat), log10(price))) +\n  geom_bin2d()\n\n\n\n\n\n\n\n\n\n\n\nHowever, the disadvantage of this transformation is that the axes are now labelled with the transformed values, making it hard to interpret the plot. Instead of doing the transformation in the aesthetic mapping, we can instead do it with the scale. This is visually identical, except the axes are labelled on the original data scale.\n\nggplot(diamonds, aes(carat, price)) +\n  geom_bin2d() + \n  scale_x_log10() + \n  scale_y_log10()\n\n\n\n\nAnother scale that is frequently customised is colour. The default categorical scale picks colours that are evenly spaced around the colour wheel. Useful alternatives are the ColorBrewer scales which have been hand tuned to work better for people with common types of colour blindness. The two plots below look similar, but there is enough difference in the shades of red and green that the dots on the right can be distinguished even by people with red-green colour blindness.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv))\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv)) +\n  scale_colour_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\nDon’t forget simpler techniques. If there are just a few colours, you can add a redundant shape mapping. This will also help ensure your plot is interpretable in black and white.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv, shape = drv)) +\n  scale_colour_brewer(palette = \"Set1\")\n\n\n\n\nThe ColorBrewer scales are documented online at http://colorbrewer2.org/ and made available in R via the RColorBrewer package, by Erich Neuwirth. Figure 32.2 shows the complete list of all palettes. The sequential (top) and diverging (bottom) palettes are particularly useful if your categorical values are ordered, or have a “middle”. This often arises if you’ve used cut() to make a continuous variable into a categorical variable.\n\n\n\n\nFigure 32.2: All ColourBrewer scales.\n\n\n\n\nWhen you have a predefined mapping between values and colours, use scale_colour_manual(). For example, if we map presidential party to colour, we want to use the standard mapping of red for Republicans and blue for Democrats:\n\npresidential |>\n  mutate(id = 33 + row_number()) |>\n  ggplot(aes(start, id, colour = party)) +\n    geom_point() +\n    geom_segment(aes(xend = end, yend = id)) +\n    scale_colour_manual(values = c(Republican = \"red\", Democratic = \"blue\"))\n\n\n\n\nFor continuous colour, you can use the built-in scale_colour_gradient() or scale_fill_gradient(). If you have a diverging scale, you can use scale_colour_gradient2(). That allows you to give, for example, positive and negative values different colours. That’s sometimes also useful if you want to distinguish points above or below the mean.\nAnother option is scale_colour_viridis() provided by the viridis package. It’s a continuous analog of the categorical ColorBrewer scales. The designers, Nathaniel Smith and Stéfan van der Walt, carefully tailored a continuous colour scheme that has good perceptual properties. Here’s an example from the viridis vignette.\n\ndf <- tibble(\n  x = rnorm(10000),\n  y = rnorm(10000)\n)\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed()\n#> Warning: Computation failed in `stat_binhex()`:\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  viridis::scale_fill_viridis() +\n  coord_fixed()\n#> Warning: Computation failed in `stat_binhex()`:\n\n\n\n\n\n\n\n\n\n\n\nNote that all colour scales come in two variety: scale_colour_x() and scale_fill_x() for the colour and fill aesthetics respectively (the colour scales are available in both UK and US spellings).\n\n32.4.4 Exercises\n\n\nWhy doesn’t the following code override the default scale?\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  scale_colour_gradient(low = \"white\", high = \"red\") +\n  coord_fixed()\n#> Warning: Computation failed in `stat_binhex()`:\n\n\nWhat is the first argument to every scale? How does it compare to labs()?\n\nChange the display of the presidential terms by:\n\nCombining the two variants shown above.\nImproving the display of the y axis.\nLabelling each term with the name of the president.\nAdding informative plot labels.\nPlacing breaks every 4 years (this is trickier than it seems!).\n\n\n\nUse override.aes to make the legend on the following plot easier to see.\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point(aes(colour = cut), alpha = 1/20)"
  },
  {
    "objectID": "communicate-plots.html#zooming",
    "href": "communicate-plots.html#zooming",
    "title": "32  Graphics for communication",
    "section": "\n32.5 Zooming",
    "text": "32.5 Zooming\nThere are three ways to control the plot limits:\n\nAdjusting what data are plotted\nSetting the limits in each scale\nSetting xlim and ylim in coord_cartesian()\n\n\nTo zoom in on a region of the plot, it’s generally best to use coord_cartesian(). Compare the following two plots:\n\nggplot(mpg, mapping = aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth() +\n  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))\n\nmpg |>\n  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) |>\n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\nYou can also set the limits on individual scales. Reducing the limits is basically equivalent to subsetting the data. It is generally more useful if you want expand the limits, for example, to match scales across different plots. For example, if we extract two classes of cars and plot them separately, it’s difficult to compare the plots because all three scales (the x-axis, the y-axis, and the colour aesthetic) have different ranges.\n\nsuv <- mpg |> filter(class == \"suv\")\ncompact <- mpg |> filter(class == \"compact\")\n\nggplot(suv, aes(displ, hwy, colour = drv)) +\n  geom_point()\n\nggplot(compact, aes(displ, hwy, colour = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nOne way to overcome this problem is to share scales across multiple plots, training the scales with the limits of the full data.\n\nx_scale <- scale_x_continuous(limits = range(mpg$displ))\ny_scale <- scale_y_continuous(limits = range(mpg$hwy))\ncol_scale <- scale_colour_discrete(limits = unique(mpg$drv))\n\nggplot(suv, aes(displ, hwy, colour = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\nggplot(compact, aes(displ, hwy, colour = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\n\n\n\n\n\n\n\n\n\n\nIn this particular case, you could have simply used faceting, but this technique is useful more generally, if for instance, you want to spread plots over multiple pages of a report."
  },
  {
    "objectID": "communicate-plots.html#themes",
    "href": "communicate-plots.html#themes",
    "title": "32  Graphics for communication",
    "section": "\n32.6 Themes",
    "text": "32.6 Themes\nFinally, you can customize the non-data elements of your plot with a theme:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n\n\n\nggplot2 includes eight themes by default, as shown in Figure 32.3. Many more are included in add-on packages like ggthemes (https://github.com/jrnold/ggthemes), by Jeffrey Arnold.\n\n\n\n\nFigure 32.3: The eight themes built-in to ggplot2.\n\n\n\n\nMany people wonder why the default theme has a grey background. This was a deliberate choice because it puts the data forward while still making the grid lines visible. The white grid lines are visible (which is important because they significantly aid position judgements), but they have little visual impact and we can easily tune them out. The grey background gives the plot a similar typographic colour to the text, ensuring that the graphics fit in with the flow of a document without jumping out with a bright white background. Finally, the grey background creates a continuous field of colour which ensures that the plot is perceived as a single visual entity.\nIt’s also possible to control individual components of each theme, like the size and colour of the font used for the y axis. Unfortunately, this level of detail is outside the scope of this book, so you’ll need to read the ggplot2 book for the full details. You can also create your own themes, if you are trying to match a particular corporate or journal style."
  },
  {
    "objectID": "communicate-plots.html#saving-your-plots",
    "href": "communicate-plots.html#saving-your-plots",
    "title": "32  Graphics for communication",
    "section": "\n32.7 Saving your plots",
    "text": "32.7 Saving your plots\nThere are two main ways to get your plots out of R and into your final write-up: ggsave() and knitr. ggsave() will save the most recent plot to disk:\n\nggplot(mpg, aes(displ, hwy)) + geom_point()\nggsave(\"my-plot.pdf\")\n#> Saving 6 x 4 in image\n\nIf you don’t specify the width and height they will be taken from the dimensions of the current plotting device. For reproducible code, you’ll want to specify them.\nGenerally, however, we recommend that you assemble your final reports using R Markdown, so we focus on the important code chunk options that you should know about for graphics. You can learn more about ggsave() in the documentation.\n\n32.7.1 Figure sizing\n\nThe biggest challenge of graphics in R Markdown is getting your figures the right size and shape. There are five main options that control figure sizing: fig.width, fig.height, fig.asp, out.width and out.height. Image sizing is challenging because there are two sizes (the size of the figure created by R and the size at which it is inserted in the output document), and multiple ways of specifying the size (i.e., height, width, and aspect ratio: pick two of three).\n\nWe recommend three of the five options:\n\nPlots tend to be more aesthetically pleasing if they have consistent width. To enforce this, set fig.width = 6 (6”) and fig.asp = 0.618 (the golden ratio) in the defaults. Then in individual chunks, only adjust fig.asp.\nControl the output size with out.width and set it to a percentage of the line width. We suggest to out.width = \"70%\" and fig.align = \"center\". That gives plots room to breathe, without taking up too much space.\nTo put multiple plots in a single row, set the out.width to 50% for two plots, 33% for 3 plots, or 25% to 4 plots, and set fig.align = \"default\". Depending on what you’re trying to illustrate (e.g. show data or show plot variations), you might also tweak fig.width, as discussed below.\n\nIf you find that you’re having to squint to read the text in your plot, you need to tweak fig.width. If fig.width is larger than the size the figure is rendered in the final doc, the text will be too small; if fig.width is smaller, the text will be too big. You’ll often need to do a little experimentation to figure out the right ratio between the fig.width and the eventual width in your document. To illustrate the principle, the following three plots have fig.width of 4, 6, and 8 respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to make sure the font size is consistent across all your figures, whenever you set out.width, you’ll also need to adjust fig.width to maintain the same ratio with your default out.width. For example, if your default fig.width is 6 and out.width is 0.7, when you set out.width = \"50%\" you’ll need to set fig.width to 4.3 (6 * 0.5 / 0.7).\n\n32.7.2 Other important options\nWhen mingling code and text, like in this book, you can set fig.show = \"hold\" so that plots are shown after the code. This has the pleasant side effect of forcing you to break up large blocks of code with their explanations.\nTo add a caption to the plot, use fig.cap. In R Markdown this will change the figure from inline to “floating”.\nIf you’re producing PDF output, the default graphics type is PDF. This is a good default because PDFs are high quality vector graphics. However, they can produce very large and slow plots if you are displaying thousands of points. In that case, set dev = \"png\" to force the use of PNGs. They are slightly lower quality, but will be much more compact.\nIt’s a good idea to name code chunks that produce figures, even if you don’t routinely label other chunks. The chunk label is used to generate the file name of the graphic on disk, so naming your chunks makes it much easier to pick out plots and reuse in other circumstances (i.e. if you want to quickly drop a single plot into an email or a tweet)."
  },
  {
    "objectID": "communicate-plots.html#learning-more",
    "href": "communicate-plots.html#learning-more",
    "title": "32  Graphics for communication",
    "section": "\n32.8 Learning more",
    "text": "32.8 Learning more\nThe absolute best place to learn more is the ggplot2 book: ggplot2: Elegant graphics for data analysis. It goes into much more depth about the underlying theory, and has many more examples of how to combine the individual pieces to solve practical problems.\nAnother great resource is the ggplot2 extensions gallery https://exts.ggplot2.tidyverse.org/gallery/. This site lists many of the packages that extend ggplot2 with new geoms and scales. It’s a great place to start if you’re trying to do something that seems hard with ggplot2."
  },
  {
    "objectID": "rmarkdown-formats.html",
    "href": "rmarkdown-formats.html",
    "title": "33  R Markdown formats",
    "section": "",
    "text": "So far you’ve seen R Markdown used to produce HTML documents. This chapter gives a brief overview of some of the many other types of output you can produce with R Markdown. There are two ways to set the output of a document:\n\n\nPermanently, by modifying the YAML header:\ntitle: \"Viridis Demo\"\noutput: html_document\n\n\nTransiently, by calling rmarkdown::render() by hand:\n\nrmarkdown::render(\"diamond-sizes.Rmd\", output_format = \"word_document\")\n\nThis is useful if you want to programmatically produce multiple types of output.\n\n\nRStudio’s knit button renders a file to the first format listed in its output field. You can render to additional formats by clicking the dropdown menu beside the knit button."
  },
  {
    "objectID": "rmarkdown-formats.html#output-options",
    "href": "rmarkdown-formats.html#output-options",
    "title": "33  R Markdown formats",
    "section": "\n33.2 Output options",
    "text": "33.2 Output options\nEach output format is associated with an R function. You can either write foo or pkg::foo. If you omit pkg, the default is assumed to be rmarkdown. It’s important to know the name of the function that makes the output because that’s where you get help. For example, to figure out what parameters you can set with html_document, look at ?rmarkdown::html_document.\nTo override the default parameter values, you need to use an expanded output field. For example, if you wanted to render an html_document with a floating table of contents, you’d use:\noutput:\n  html_document:\n    toc: true\n    toc_float: true\nYou can even render to multiple outputs by supplying a list of formats:\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n  pdf_document: default\nNote the special syntax if you don’t want to override any of the default options."
  },
  {
    "objectID": "rmarkdown-formats.html#documents",
    "href": "rmarkdown-formats.html#documents",
    "title": "33  R Markdown formats",
    "section": "\n33.3 Documents",
    "text": "33.3 Documents\nThe previous chapter focused on the default html_document output. There are a number of basic variations on that theme, generating different types of documents:\n\npdf_document makes a PDF with LaTeX (an open source document layout system), which you’ll need to install. RStudio will prompt you if you don’t already have it.\nword_document for Microsoft Word documents (.docx).\nodt_document for OpenDocument Text documents (.odt).\nrtf_document for Rich Text Format (.rtf) documents.\nmd_document for a Markdown document. This isn’t typically useful by itself, but you might use it if, for example, your corporate CMS or lab wiki uses markdown.\ngithub_document: this is a tailored version of md_document designed for sharing on GitHub.\n\nRemember, when generating a document to share with decision makers, you can turn off the default display of code by setting global options in the setup chunk:\n\nknitr::opts_chunk$set(echo = FALSE)\n\nFor html_documents another option is to make the code chunks hidden by default, but visible with a click:\noutput:\n  html_document:\n    code_folding: hide"
  },
  {
    "objectID": "rmarkdown-formats.html#notebooks",
    "href": "rmarkdown-formats.html#notebooks",
    "title": "33  R Markdown formats",
    "section": "\n33.4 Notebooks",
    "text": "33.4 Notebooks\nA notebook, html_notebook, is a variation on a html_document. The rendered outputs are very similar, but the purpose is different. A html_document is focused on communicating with decision makers, while a notebook is focused on collaborating with other data scientists. These different purposes lead to using the HTML output in different ways. Both HTML outputs will contain the fully rendered output, but the notebook also contains the full source code. That means you can use the .nb.html generated by the notebook in two ways:\n\nYou can view it in a web browser, and see the rendered output. Unlike html_document, this rendering always includes an embedded copy of the source code that generated it.\nYou can edit it in RStudio. When you open an .nb.html file, RStudio will automatically recreate the .Rmd file that generated it. In the future, you will also be able to include supporting files (e.g. .csv data files), which will be automatically extracted when needed.\n\nEmailing .nb.html files is a simple way to share analyses with your colleagues. But things will get painful as soon as they want to make changes. If this starts to happen, it’s a good time to learn Git and GitHub. Learning Git and GitHub is definitely painful at first, but the collaboration payoff is huge. As mentioned earlier, Git and GitHub are outside the scope of the book, but there’s one tip that’s useful if you’re already using them: use both html_notebook and github_document outputs:\noutput:\n  html_notebook: default\n  github_document: default\nhtml_notebook gives you a local preview, and a file that you can share via email. github_document creates a minimal md file that you can check into git. You can easily see how the results of your analysis (not just the code) change over time, and GitHub will render it for you nicely online."
  },
  {
    "objectID": "rmarkdown-formats.html#presentations",
    "href": "rmarkdown-formats.html#presentations",
    "title": "33  R Markdown formats",
    "section": "\n33.5 Presentations",
    "text": "33.5 Presentations\nYou can also use R Markdown to produce presentations. You get less visual control than with a tool like Keynote or PowerPoint, but automatically inserting the results of your R code into a presentation can save a huge amount of time. Presentations work by dividing your content into slides, with a new slide beginning at each first (#) or second (##) level header. You can also insert a horizontal rule (***) to create a new slide without a header.\nR Markdown comes with three presentation formats built-in:\n\nioslides_presentation - HTML presentation with ioslides\nslidy_presentation - HTML presentation with W3C Slidy\nbeamer_presentation - PDF presentation with LaTeX Beamer.\n\nThree other popular formats are provided by packages:\n\nxaringan, https://slides.yihui.org/xaringan, is an R Markdown extension based on the JavaScript library remark.js to generate HTML5 presentations.\nrevealjs::revealjs_presentation - HTML presentation with reveal.js. Requires the revealjs package, https://github.com/rstudio/revealjs.\nrmdshower, https://github.com/MangoTheCat/rmdshower, provides a wrapper around the shower, https://github.com/shower/shower, presentation engine"
  },
  {
    "objectID": "rmarkdown-formats.html#dashboards",
    "href": "rmarkdown-formats.html#dashboards",
    "title": "33  R Markdown formats",
    "section": "\n33.6 Dashboards",
    "text": "33.6 Dashboards\nDashboards are a useful way to communicate large amounts of information visually and quickly. Flexdashboard makes it particularly easy to create dashboards using R Markdown and a convention for how the headers affect the layout:\n\nEach level 1 header (#) begins a new page in the dashboard.\nEach level 2 header (##) begins a new column.\nEach level 3 header (###) begins a new row.\n\nFor example, you can produce this dashboard:\n\n\n\n\n\nUsing this code:\n\n---\ntitle: \"Diamonds distribution dashboard\"\noutput: flexdashboard::flex_dashboard\n---\n\n```{r setup, include = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\nknitr::opts_chunk$set(fig.width = 5, fig.asp = 1/3)\n```\n\n## Column 1\n\n### Carat\n\n```{r}\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.1)\n```\n\n### Cut\n\n```{r}\nggplot(diamonds, aes(cut)) + geom_bar()\n```\n\n### Colour\n\n```{r}\nggplot(diamonds, aes(color)) + geom_bar()\n```\n\n## Column 2\n\n### The largest diamonds\n\n```{r}\ndiamonds |> \n  arrange(desc(carat)) |> \n  head(100) |> \n  select(carat, cut, color, price) |> \n  DT::datatable()\n```\n\nFlexdashboard also provides simple tools for creating sidebars, tabsets, value boxes, and gauges. To learn more about flexdashboard visit https://pkgs.rstudio.com/flexdashboard."
  },
  {
    "objectID": "rmarkdown-formats.html#interactivity",
    "href": "rmarkdown-formats.html#interactivity",
    "title": "33  R Markdown formats",
    "section": "\n33.7 Interactivity",
    "text": "33.7 Interactivity\nAny HTML format (document, notebook, presentation, or dashboard) can contain interactive components.\n\n33.7.1 htmlwidgets\nHTML is an interactive format, and you can take advantage of that interactivity with htmlwidgets, R functions that produce interactive HTML visualisations. For example, take the leaflet map below. If you’re viewing this page on the web, you can drag the map around, zoom in and out, etc. You obviously can’t do that in a book, so rmarkdown automatically inserts a static screenshot for you.\n\nlibrary(leaflet)\nleaflet() |>\n  setView(174.764, -36.877, zoom = 16) |> \n  addTiles() |>\n  addMarkers(174.764, -36.877, popup = \"Maungawhau\") \n\n\n\n\n\nThe great thing about htmlwidgets is that you don’t need to know anything about HTML or JavaScript to use them. All the details are wrapped inside the package, so you don’t need to worry about it.\nThere are many packages that provide htmlwidgets, including:\n\ndygraphs, http://rstudio.github.io/dygraphs, for interactive time series visualisations.\nDT, http://rstudio.github.io/DT/, for interactive tables.\nthreejs, http://bwlewis.github.io/rthreejs for interactive 3d plots.\nDiagrammeR, http://rich-iannone.github.io/DiagrammeR for diagrams (like flow charts and simple node-link diagrams).\n\nTo learn more about htmlwidgets and see a more complete list of packages that provide them visit http://www.htmlwidgets.org.\n\n33.7.2 Shiny\nhtmlwidgets provide client-side interactivity — all the interactivity happens in the browser, independently of R. On one hand, that’s great because you can distribute the HTML file without any connection to R. However, that fundamentally limits what you can do to things that have been implemented in HTML and JavaScript. An alternative approach is to use shiny, a package that allows you to create interactivity using R code, not JavaScript.\nTo call Shiny code from an R Markdown document, add runtime: shiny to the header:\ntitle: \"Shiny Web App\"\noutput: html_document\nruntime: shiny\nThen you can use the “input” functions to add interactive components to the document:\n\nlibrary(shiny)\n\ntextInput(\"name\", \"What is your name?\")\nnumericInput(\"age\", \"How old are you?\", NA, min = 0, max = 150)\n\n\n\n\n\n\nYou can then refer to the values with input$name and input$age, and the code that uses them will be automatically re-run whenever they change.\nWe can’t show you a live shiny app here because shiny interactions occur on the server-side. This means that you can write interactive apps without knowing JavaScript, but you need a server to run them on. This introduces a logistical issue: Shiny apps need a Shiny server to be run online. When you run shiny apps on your own computer, shiny automatically sets up a shiny server for you, but you need a public facing shiny server if you want to publish this sort of interactivity online. That’s the fundamental trade-off of shiny: you can do anything in a shiny document that you can do in R, but it requires someone to be running R.\nLearn more about Shiny at http://shiny.rstudio.com."
  },
  {
    "objectID": "rmarkdown-formats.html#websites",
    "href": "rmarkdown-formats.html#websites",
    "title": "33  R Markdown formats",
    "section": "\n33.8 Websites",
    "text": "33.8 Websites\nWith a little additional infrastructure you can use R Markdown to generate a complete website:\n\nPut your .Rmd files in a single directory. index.Rmd will become the home page.\n\nAdd a YAML file named _site.yml provides the navigation for the site. For example:\n\nname: \"my-website\"\nnavbar:\n  title: \"My Website\"\n  left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Viridis Colors\"\n      href: 1-example.html\n    - text: \"Terrain Colors\"\n      href: 3-inline.html\n\n\n\nExecute rmarkdown::render_site() to build _site, a directory of files ready to deploy as a standalone static website, or if you use an RStudio Project for your website directory. RStudio will add a Build tab to the IDE that you can use to build and preview your site.\nRead more at https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html."
  },
  {
    "objectID": "rmarkdown-formats.html#other-formats",
    "href": "rmarkdown-formats.html#other-formats",
    "title": "33  R Markdown formats",
    "section": "\n33.9 Other formats",
    "text": "33.9 Other formats\nOther packages provide even more output formats:\n\nThe bookdown package, https://pkgs.rstudio.com/bookdown, makes it easy to write books, like this one. To learn more, read Authoring Books with R Markdown, by Yihui Xie, which is, of course, written in bookdown. Visit http://www.bookdown.org to see other bookdown books written by the wider R community.\nThe prettydoc package, https://prettydoc.statr.me, provides lightweight document formats with a range of attractive themes.\nThe rticles package, https://pkgs.rstudio.com/rticles, compiles a selection of formats tailored for specific scientific journals.\n\nSee http://rmarkdown.rstudio.com/formats.html for a list of even more formats. You can also create your own by following the instructions at http://rmarkdown.rstudio.com/developer_custom_formats.html."
  },
  {
    "objectID": "rmarkdown-formats.html#learning-more",
    "href": "rmarkdown-formats.html#learning-more",
    "title": "33  R Markdown formats",
    "section": "\n33.10 Learning more",
    "text": "33.10 Learning more\nTo learn more about effective communication in these different formats we recommend the following resources:\n\nTo improve your presentation skills, try Presentation Patterns, by Neal Ford, Matthew McCollough, and Nathaniel Schutta. It provides a set of effective patterns (both low- and high-level) that you can apply to improve your presentations.\nIf you give academic talks, you might like the Leek group guide to giving talks.\nWe haven’t taken it outselves, but we’ve heard good things about Matt McGarrity’s online course on public speaking: https://www.coursera.org/learn/public-speaking.\nIf you are creating a lot of dashboards, make sure to read Stephen Few’s Information Dashboard Design: The Effective Visual Communication of Data. It will help you create dashboards that are truly useful, not just pretty to look at.\nEffectively communicating your ideas often benefits from some knowledge of graphic design. Robin Williams’ The Non-Designer’s Design Book is a great place to start."
  },
  {
    "objectID": "rmarkdown-workflow.html",
    "href": "rmarkdown-workflow.html",
    "title": "34  R Markdown workflow",
    "section": "",
    "text": "R Markdown is also important because it so tightly integrates prose and code. This makes it a great analysis notebook because it lets you develop code and record your thoughts. An analysis notebook shares many of the same goals as a classic lab notebook in the physical sciences. It:\n\nRecords what you did and why you did it. Regardless of how great your memory is, if you don’t record what you do, there will come a time when you have forgotten important details. Write them down so you don’t forget!\nSupports rigorous thinking. You are more likely to come up with a strong analysis if you record your thoughts as you go, and continue to reflect on them. This also saves you time when you eventually write up your analysis to share with others.\nHelps others understand your work. It is rare to do data analysis by yourself, and you’ll often be working as part of a team. A lab notebook helps you share not only what you’ve done, but why you did it with your colleagues or lab mates.\n\nMuch of the good advice about using lab notebooks effectively can also be translated to analysis notebooks. We’ve drawn on our own experiences and Colin Purrington’s advice on lab notebooks (http://colinpurrington.com/tips/lab-notebooks) to come up with the following tips:\n\nEnsure each notebook has a descriptive title, an evocative filename, and a first paragraph that briefly describes the aims of the analysis.\n\nUse the YAML header date field to record the date you started working on the notebook:\ndate: 2016-08-23\nUse ISO8601 YYYY-MM-DD format so that’s there no ambiguity. Use it even if you don’t normally write dates that way!\n\nIf you spend a lot of time on an analysis idea and it turns out to be a dead end, don’t delete it! Write up a brief note about why it failed and leave it in the notebook. That will help you avoid going down the same dead end when you come back to the analysis in the future.\nGenerally, you’re better off doing data entry outside of R. But if you do need to record a small snippet of data, clearly lay it out using tibble::tribble().\nIf you discover an error in a data file, never modify it directly, but instead write code to correct the value. Explain why you made the fix.\nBefore you finish for the day, make sure you can knit the notebook (if you’re using caching, make sure to clear the caches). That will let you fix any problems while the code is still fresh in your mind.\nIf you want your code to be reproducible in the long-run (i.e. so you can come back to run it next month or next year), you’ll need to track the versions of the packages that your code uses. A rigorous approach is to use renv, https://rstudio.github.io/renv/index.html, which stores packages in your project directory. A quick and dirty hack is to include a chunk that runs sessionInfo() — that won’t let you easily recreate your packages as they are today, but at least you’ll know what they were.\nYou are going to create many, many, many analysis notebooks over the course of your career. How are you going to organise them so you can find them again in the future? We recommend storing them in individual projects, and coming up with a good naming scheme."
  }
]